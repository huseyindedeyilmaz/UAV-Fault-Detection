{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_name(flight_name, file_name):\n",
    "    topic_name = file_name.split(flight_name)\n",
    "    topic_name =  topic_name[1].strip(\"-\") if len(topic_name)>1 else \"\"\n",
    "    topic_name = topic_name.split(\".csv\")[0]\n",
    "    return topic_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(full_path):\n",
    "    df_tmp = pd.read_csv(full_path,on_bad_lines=\"skip\")\n",
    "    df_tmp = df_tmp.rename(columns={\"%time\": \"timestamp\"})\n",
    "    \n",
    "    df_tmp[\"timestamp\"] = pd.to_datetime(df_tmp[\"timestamp\"], unit=\"ns\")\n",
    "    df_tmp.set_index(\"timestamp\", inplace=True) \n",
    "    return df_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_topic_list = [\"diagnostics\", \"emergency_responder-traj_file\",\"mavlink-from\",\"mavros-state\",\n",
    "                    \"global_position\",\n",
    "                     \"local_position\", \n",
    "                     \"mavctrl-rpy\",\n",
    "                     \"mavros-battery\", \"field_raw\",\n",
    "                     \"mavros-imu-mag\", \"mavros-mission-reached\",\n",
    "                     \"mavros-rc\",\n",
    "                      \"setpoint_raw\",\"mavros-imu-data_raw\"]\n",
    "\n",
    "unused_column_list = [\"field.header.seq\", \"field.header.stamp\", \"field.header.frame_id\", \n",
    "                      \"field.commanded\", \"field.variance\", \"%time\",\"field.x\",\"field.twist.angular.x\",\"field.twist.angular.y\",\"field.twist.angular.z\",\n",
    "                      'field.orientation_covariance0',\n",
    "       'field.orientation_covariance1',\n",
    "       'field.orientation_covariance2',\n",
    "       'field.orientation_covariance3',\n",
    "       'field.orientation_covariance4',\n",
    "       'field.orientation_covariance5',\n",
    "       'field.orientation_covariance6',\n",
    "       'field.orientation_covariance7',\n",
    "       'field.orientation_covariance8',\n",
    "\n",
    "       'field.angular_velocity_covariance0',\n",
    "       'field.angular_velocity_covariance1',\n",
    "       'field.angular_velocity_covariance2',\n",
    "       'field.angular_velocity_covariance3',\n",
    "       'field.angular_velocity_covariance4',\n",
    "       'field.angular_velocity_covariance5',\n",
    "       'field.angular_velocity_covariance6',\n",
    "       'field.angular_velocity_covariance7',\n",
    "       'field.angular_velocity_covariance8',\n",
    "\n",
    "       'field.linear_acceleration_covariance0',\n",
    "       'field.linear_acceleration_covariance1',\n",
    "       'field.linear_acceleration_covariance2',\n",
    "       'field.linear_acceleration_covariance3',\n",
    "       'field.linear_acceleration_covariance4',\n",
    "       'field.linear_acceleration_covariance5',\n",
    "       'field.linear_acceleration_covariance6',\n",
    "       'field.linear_acceleration_covariance7',\n",
    "       'field.linear_acceleration_covariance8',\n",
    "       \"field.coordinate_frame\",\"field.source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hy138\\Desktop\\UAV\n",
      "['.vscode', 'best_model.pth', 'BiLSTM.ipynb', 'BiLSTMs.ipynb', 'CNN1D.ipynb', 'CNN1Ds.ipynb', 'data', 'LSTM.ipynb', 'LSTMAutoEncoder.ipynb', 'LSTMs.ipynb', 'Yeni klasör']\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = {}\n",
    "flight_topic_list = []\n",
    "topic_list = []\n",
    "all_columns = []\n",
    "df_dict = {}\n",
    "failure_status_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_1_engine_failure\n",
      "carbonZ_2018-07-18-15-53-31_2_engine_failure\n",
      "carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-16-39-00_1_engine_failure\n",
      "carbonZ_2018-07-30-16-39-00_2_engine_failure\n",
      "carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj\n",
      "carbonZ_2018-09-11-11-56-30_engine_failure\n",
      "carbonZ_2018-09-11-14-22-07_1_engine_failure\n",
      "carbonZ_2018-09-11-14-22-07_2_engine_failure\n",
      "carbonZ_2018-09-11-14-41-51_elevator_failure\n",
      "carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\n",
      "carbonZ_2018-09-11-15-05-11_1_elevator_failure\n",
      "carbonZ_2018-09-11-15-06-34_1_rudder_right_failure\n",
      "carbonZ_2018-09-11-15-06-34_2_rudder_right_failure\n",
      "carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure\n",
      "carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure\n",
      "carbonZ_2018-09-11-17-55-30_1_right_aileron_failure\n",
      "carbonZ_2018-09-11-17-55-30_2_left_aileron_failure\n",
      "carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-14-37-22_2_right_aileron_failure\n",
      "carbonZ_2018-10-05-14-37-22_3_left_aileron_failure\n",
      "carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj\n"
     ]
    }
   ],
   "source": [
    "for i,flight in enumerate(glob.glob(data_path+\"*\")):\n",
    "    \n",
    "    flight_name = os.path.basename(flight)\n",
    "    \n",
    "    if \"no_ground_truth\" in flight_name:\n",
    "        continue\n",
    "    if \"no_failure\" in flight_name:\n",
    "            continue\n",
    "    # This folder has not path-dev csv file\n",
    "    if \"carbonZ_2018-09-11-15-06-34_3_rudder_left_failure\" in flight_name:\n",
    "        continue\n",
    "    print(flight_name)\n",
    "\n",
    "    if flight_name not in flight_topic_list:\n",
    "        flight_topic_list.append(flight_name)\n",
    "    \n",
    "\n",
    "    df_failure = None\n",
    "    failure_duration_start = pd.Timestamp('1970-01-01')\n",
    "    failure_duration_finish = pd.Timestamp('1970-01-01')\n",
    "\n",
    "    dfs = list()\n",
    "    for i , file in enumerate(glob.glob(os.path.join(data_path,flight_name,\"*.csv\"))):\n",
    "        \n",
    "        if any(x in file for x in unused_topic_list):\n",
    "            continue\n",
    "        \n",
    "\n",
    "        if \"failure_status\" in os.path.basename(file):\n",
    "            file_name = os.path.basename(file)\n",
    "            \n",
    "            df = read_data(file)\n",
    "            topic_name = extract_topic_name(flight_name,file_name)\n",
    "\n",
    "            \n",
    "            failure_duration_start = min(df.index)\n",
    "            failure_duration_end = max(df.index)\n",
    "\n",
    "            failure_status_dict[flight_name] = (failure_duration_start,failure_duration_end)\n",
    "\n",
    "            continue\n",
    "        \n",
    "        file_name = os.path.basename(file)\n",
    "\n",
    "        df = read_data(file)\n",
    "        topic_name = extract_topic_name(flight_name,file_name)\n",
    "        for col in unused_column_list:\n",
    "            if col in df.columns:\n",
    "                df.drop(col,axis=1,inplace=True)\n",
    "        new_columns = list(map(lambda x: f\"{topic_name}.{x.replace('field.', '')}\", df.columns))\n",
    "        df = df.set_axis(new_columns, axis=1)\n",
    "       \n",
    "     \n",
    "        \n",
    "        dfs.append(df)\n",
    "\n",
    "   \n",
    "    df_dict[flight_name] = dfs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dict[\"carbonZ_2018-07-18-15-53-31_1_engine_failure\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6619,\n",
       " mavctrl-path_dev.y    0\n",
       " mavctrl-path_dev.z    0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dict[\"carbonZ_2018-07-18-15-53-31_1_engine_failure\"][0]),df_dict[\"carbonZ_2018-07-18-15-53-31_1_engine_failure\"][0].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'carbonZ_2018-07-18-15-53-31_1_engine_failure': (Timestamp('2018-07-18 19:58:47.129305993'),\n",
       "  Timestamp('2018-07-18 19:59:03.134074845')),\n",
       " 'carbonZ_2018-07-18-15-53-31_2_engine_failure': (Timestamp('2018-07-18 20:01:44.729304590'),\n",
       "  Timestamp('2018-07-18 20:01:59.928010526')),\n",
       " 'carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj': (Timestamp('2018-07-18 20:32:26.878396465'),\n",
       "  Timestamp('2018-07-18 20:32:42.672828037')),\n",
       " 'carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj': (Timestamp('2018-07-18 20:46:37.888445250'),\n",
       "  Timestamp('2018-07-18 20:46:54.288954056')),\n",
       " 'carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj': (Timestamp('2018-07-21 00:35:22.170194'),\n",
       "  Timestamp('2018-07-21 00:35:41.166058768')),\n",
       " 'carbonZ_2018-07-30-16-39-00_1_engine_failure': (Timestamp('2018-07-21 01:00:21.380769776'),\n",
       "  Timestamp('2018-07-21 01:00:36.175981200')),\n",
       " 'carbonZ_2018-07-30-16-39-00_2_engine_failure': (Timestamp('2018-07-21 01:04:00.575490608'),\n",
       "  Timestamp('2018-07-21 01:04:15.194122640')),\n",
       " 'carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj': (Timestamp('2018-07-30 21:16:33.314179152'),\n",
       "  Timestamp('2018-07-30 21:16:49.124017251')),\n",
       " 'carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj': (Timestamp('2018-07-30 21:25:25.757112162'),\n",
       "  Timestamp('2018-07-30 21:25:44.754748978')),\n",
       " 'carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj': (Timestamp('2018-07-30 21:41:15.634859201'),\n",
       "  Timestamp('2018-07-30 21:41:39.036278834')),\n",
       " 'carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj': (Timestamp('2018-07-30 21:49:47.539406723'),\n",
       "  Timestamp('2018-07-30 21:50:09.732858508')),\n",
       " 'carbonZ_2018-09-11-11-56-30_engine_failure': (Timestamp('2018-09-11 16:05:02.419301184'),\n",
       "  Timestamp('2018-09-11 16:05:22.921275104')),\n",
       " 'carbonZ_2018-09-11-14-22-07_1_engine_failure': (Timestamp('2018-09-11 18:26:14.324825782'),\n",
       "  Timestamp('2018-09-11 18:26:23.326658597')),\n",
       " 'carbonZ_2018-09-11-14-22-07_2_engine_failure': (Timestamp('2018-09-11 18:28:20.325374057'),\n",
       "  Timestamp('2018-09-11 18:28:32.326442789')),\n",
       " 'carbonZ_2018-09-11-14-41-51_elevator_failure': (Timestamp('2018-09-11 18:48:33.193239864'),\n",
       "  Timestamp('2018-09-11 18:48:43.702938424')),\n",
       " 'carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure': (Timestamp('2018-09-11 18:57:29.985030040'),\n",
       "  Timestamp('2018-09-11 18:59:37.993673016')),\n",
       " 'carbonZ_2018-09-11-15-05-11_1_elevator_failure': (Timestamp('2018-09-11 19:12:08.654835256'),\n",
       "  Timestamp('2018-09-11 19:12:21.172710456')),\n",
       " 'carbonZ_2018-09-11-15-06-34_1_rudder_right_failure': (Timestamp('2018-09-11 19:17:55.038442264'),\n",
       "  Timestamp('2018-09-11 19:18:09.537019480')),\n",
       " 'carbonZ_2018-09-11-15-06-34_2_rudder_right_failure': (Timestamp('2018-09-11 19:19:58.533612056'),\n",
       "  Timestamp('2018-09-11 19:20:15.542582552')),\n",
       " 'carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure': (Timestamp('2018-09-11 21:36:15.255535472'),\n",
       "  Timestamp('2018-09-11 21:36:42.254787570')),\n",
       " 'carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure': (Timestamp('2018-09-11 21:38:23.751688384'),\n",
       "  Timestamp('2018-09-11 21:38:59.249605331')),\n",
       " 'carbonZ_2018-09-11-17-55-30_1_right_aileron_failure': (Timestamp('2018-09-11 22:04:29.915932200'),\n",
       "  Timestamp('2018-09-11 22:04:50.916117253')),\n",
       " 'carbonZ_2018-09-11-17-55-30_2_left_aileron_failure': (Timestamp('2018-09-11 22:06:18.909972946'),\n",
       "  Timestamp('2018-09-11 22:06:49.930133544')),\n",
       " 'carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj': (Timestamp('2018-10-05 18:58:10.456220968'),\n",
       "  Timestamp('2018-10-05 18:58:19.953767048')),\n",
       " 'carbonZ_2018-10-05-14-37-22_2_right_aileron_failure': (Timestamp('2018-10-05 19:41:12.176343271'),\n",
       "  Timestamp('2018-10-05 19:42:23.177067207')),\n",
       " 'carbonZ_2018-10-05-14-37-22_3_left_aileron_failure': (Timestamp('2018-10-05 19:43:36.175449735'),\n",
       "  Timestamp('2018-10-05 19:44:00.176733447')),\n",
       " 'carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj': (Timestamp('2018-10-05 20:07:04.743298528'),\n",
       "  Timestamp('2018-10-05 20:07:21.746703360')),\n",
       " 'carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj': (Timestamp('2018-10-05 19:59:59.479181792'),\n",
       "  Timestamp('2018-10-05 20:00:12.477307424')),\n",
       " 'carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj': (Timestamp('2018-10-05 20:09:30.477759008'),\n",
       "  Timestamp('2018-10-05 20:09:46.474778400')),\n",
       " 'carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj': (Timestamp('2018-10-18 15:09:31.419064456'),\n",
       "  Timestamp('2018-10-18 15:09:43.416809576')),\n",
       " 'carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj': (Timestamp('2018-10-18 15:09:27.628350888'),\n",
       "  Timestamp('2018-10-18 15:09:38.630940104')),\n",
       " 'carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj': (Timestamp('2018-10-18 15:09:39.270985672'),\n",
       "  Timestamp('2018-10-18 15:09:53.269556136')),\n",
       " 'carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj': (Timestamp('2018-10-18 15:19:45.268859464'),\n",
       "  Timestamp('2018-10-18 15:20:04.768932520')),\n",
       " 'carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj': (Timestamp('2018-10-18 15:08:37.878232904'),\n",
       "  Timestamp('2018-10-18 15:08:45.380344968')),\n",
       " 'carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj': (Timestamp('2018-10-18 15:12:17.734694408'),\n",
       "  Timestamp('2018-10-18 15:12:31.237794664'))}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failure_status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key,data in df_dict.items():\n",
    "#     df_list = []\n",
    "#     for df in data:\n",
    "#         for col in df.columns:\n",
    "#             df[f'{col}_copy'] = df[col] \n",
    "    \n",
    "#         df_list.append(df)\n",
    "#     df_dict[key] = df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_1_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-18-15-53-31_2_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-16-39-00_1_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-30-16-39-00_2_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-09-11-11-56-30_engine_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-22-07_1_engine_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-22-07_2_engine_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-41-51_elevator_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\n",
      "13\n",
      "carbonZ_2018-09-11-15-05-11_1_elevator_failure\n",
      "13\n",
      "carbonZ_2018-09-11-15-06-34_1_rudder_right_failure\n",
      "13\n",
      "carbonZ_2018-09-11-15-06-34_2_rudder_right_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-55-30_1_right_aileron_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-55-30_2_left_aileron_failure\n",
      "13\n",
      "carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-05-14-37-22_2_right_aileron_failure\n",
      "13\n",
      "carbonZ_2018-10-05-14-37-22_3_left_aileron_failure\n",
      "13\n",
      "carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for key, df in df_dict.items():\n",
    "    print(key)\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, data in df_dict.items():\n",
    "    if \"rudder_zero__left_aileron_failure\" in key:\n",
    "        \n",
    "\n",
    "        df_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\"] = df_dict['carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure'].copy()\n",
    " \n",
    " \n",
    "\n",
    "        df_dict[\"carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\"] = df_dict['carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure'].copy()\n",
    "\n",
    "   \n",
    "\n",
    "        del df_dict['carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure']\n",
    "\n",
    "\n",
    "        break\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_1_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-18-15-53-31_2_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-16-39-00_1_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-30-16-39-00_2_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-09-11-11-56-30_engine_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-22-07_1_engine_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-22-07_2_engine_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-41-51_elevator_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\n",
      "13\n",
      "carbonZ_2018-09-11-15-05-11_1_elevator_failure\n",
      "13\n",
      "carbonZ_2018-09-11-15-06-34_1_rudder_right_failure\n",
      "13\n",
      "carbonZ_2018-09-11-15-06-34_2_rudder_right_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-55-30_1_right_aileron_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-55-30_2_left_aileron_failure\n",
      "13\n",
      "carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-05-14-37-22_2_right_aileron_failure\n",
      "13\n",
      "carbonZ_2018-10-05-14-37-22_3_left_aileron_failure\n",
      "13\n",
      "carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for key, df in df_dict.items():\n",
    "    print(key)\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, data in failure_status_dict.items():\n",
    "    if \"rudder_zero__left_aileron_failure\" in key:\n",
    "        failure_status_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\"] = failure_status_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure\"]\n",
    "        failure_status_dict[\"carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\"] = failure_status_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure\"]\n",
    "        del failure_status_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure\"]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mavros-imu-atm_pressure.fluid_pressure'], dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\"][1].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mavctrl-path_dev.y</th>\n",
       "      <th>mavctrl-path_dev.z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.820860792</th>\n",
       "      <td>141.284886</td>\n",
       "      <td>14.438259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.840917944</th>\n",
       "      <td>141.284886</td>\n",
       "      <td>14.438259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.861388914</th>\n",
       "      <td>141.284886</td>\n",
       "      <td>14.438259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.880812090</th>\n",
       "      <td>141.284886</td>\n",
       "      <td>14.438259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.901712334</th>\n",
       "      <td>144.798264</td>\n",
       "      <td>14.394005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.108804965</th>\n",
       "      <td>8.337481</td>\n",
       "      <td>21.670137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.130722624</th>\n",
       "      <td>8.337481</td>\n",
       "      <td>21.670137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.149511356</th>\n",
       "      <td>8.337481</td>\n",
       "      <td>21.670137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.171009244</th>\n",
       "      <td>8.337481</td>\n",
       "      <td>21.670137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.190838957</th>\n",
       "      <td>8.337481</td>\n",
       "      <td>21.670137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6619 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mavctrl-path_dev.y  mavctrl-path_dev.z\n",
       "timestamp                                                            \n",
       "2018-07-18 19:56:50.820860792          141.284886           14.438259\n",
       "2018-07-18 19:56:50.840917944          141.284886           14.438259\n",
       "2018-07-18 19:56:50.861388914          141.284886           14.438259\n",
       "2018-07-18 19:56:50.880812090          141.284886           14.438259\n",
       "2018-07-18 19:56:50.901712334          144.798264           14.394005\n",
       "...                                           ...                 ...\n",
       "2018-07-18 19:59:03.108804965            8.337481           21.670137\n",
       "2018-07-18 19:59:03.130722624            8.337481           21.670137\n",
       "2018-07-18 19:59:03.149511356            8.337481           21.670137\n",
       "2018-07-18 19:59:03.171009244            8.337481           21.670137\n",
       "2018-07-18 19:59:03.190838957            8.337481           21.670137\n",
       "\n",
       "[6619 rows x 2 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict[\"carbonZ_2018-07-18-15-53-31_1_engine_failure\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_topic_list.append(\"carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\")\n",
    "flight_topic_list.append(\"carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\")\n",
    "flight_topic_list.remove(\"carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flight_topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2018-07-18 19:58:47.129305993'),\n",
       " Timestamp('2018-07-18 19:59:03.134074845'))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failure_status_dict[\"carbonZ_2018-07-18-15-53-31_1_engine_failure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                               mavctrl-path_dev.y  mavctrl-path_dev.z\n",
       " timestamp                                                            \n",
       " 2018-09-11 18:55:44.748852696          226.349940          -16.530594\n",
       " 2018-09-11 18:55:44.768693912          226.349940          -16.530594\n",
       " 2018-09-11 18:55:44.793884088          207.499653          -16.530594\n",
       " 2018-09-11 18:55:44.809683352          207.499366          -16.530594\n",
       " 2018-09-11 18:55:44.831660792          207.499366          -16.530594\n",
       " ...                                           ...                 ...\n",
       " 2018-09-11 18:59:38.029159576            5.033987            3.622774\n",
       " 2018-09-11 18:59:38.049874936            5.033987            3.622774\n",
       " 2018-09-11 18:59:38.073666904            5.033987            3.622774\n",
       " 2018-09-11 18:59:38.089277368            4.698973            4.732197\n",
       " 2018-09-11 18:59:38.109449304            4.698973            4.732197\n",
       " \n",
       " [11669 rows x 2 columns],\n",
       "                                mavros-imu-atm_pressure.fluid_pressure\n",
       " timestamp                                                            \n",
       " 2018-09-11 18:55:44.739465240                            97497.888184\n",
       " 2018-09-11 18:55:44.821564344                            97497.888184\n",
       " 2018-09-11 18:55:44.938280408                            97485.864258\n",
       " 2018-09-11 18:55:45.038424536                            97479.797363\n",
       " 2018-09-11 18:55:45.123441848                            97479.797363\n",
       " ...                                                               ...\n",
       " 2018-09-11 18:59:37.626786712                            97689.019775\n",
       " 2018-09-11 18:59:37.734683928                            97696.691895\n",
       " 2018-09-11 18:59:37.833203736                            97704.296875\n",
       " 2018-09-11 18:59:37.924305528                            97704.296875\n",
       " 2018-09-11 18:59:38.018054744                            97711.303711\n",
       " \n",
       " [2334 rows x 1 columns],\n",
       "                                mavros-imu-data.orientation.x  \\\n",
       " timestamp                                                      \n",
       " 2018-09-11 18:55:44.815328696                      -0.005702   \n",
       " 2018-09-11 18:55:45.302239320                      -0.027779   \n",
       " 2018-09-11 18:55:45.691413272                      -0.004409   \n",
       " 2018-09-11 18:55:46.060852152                       0.022710   \n",
       " 2018-09-11 18:55:46.443526008                       0.048855   \n",
       " ...                                                      ...   \n",
       " 2018-09-11 18:59:36.240079096                      -0.120860   \n",
       " 2018-09-11 18:59:36.683336696                      -0.126285   \n",
       " 2018-09-11 18:59:37.197629496                      -0.091700   \n",
       " 2018-09-11 18:59:37.632176952                      -0.057563   \n",
       " 2018-09-11 18:59:37.978509688                      -0.021296   \n",
       " \n",
       "                                mavros-imu-data.orientation.y  \\\n",
       " timestamp                                                      \n",
       " 2018-09-11 18:55:44.815328696                      -0.024376   \n",
       " 2018-09-11 18:55:45.302239320                      -0.232940   \n",
       " 2018-09-11 18:55:45.691413272                      -0.276876   \n",
       " 2018-09-11 18:55:46.060852152                      -0.314628   \n",
       " 2018-09-11 18:55:46.443526008                      -0.314407   \n",
       " ...                                                      ...   \n",
       " 2018-09-11 18:59:36.240079096                      -0.012206   \n",
       " 2018-09-11 18:59:36.683336696                      -0.054971   \n",
       " 2018-09-11 18:59:37.197629496                      -0.115521   \n",
       " 2018-09-11 18:59:37.632176952                      -0.169438   \n",
       " 2018-09-11 18:59:37.978509688                      -0.193174   \n",
       " \n",
       "                                mavros-imu-data.orientation.z  \\\n",
       " timestamp                                                      \n",
       " 2018-09-11 18:55:44.815328696                      -0.986004   \n",
       " 2018-09-11 18:55:45.302239320                      -0.958538   \n",
       " 2018-09-11 18:55:45.691413272                      -0.941414   \n",
       " 2018-09-11 18:55:46.060852152                      -0.920235   \n",
       " 2018-09-11 18:55:46.443526008                      -0.909142   \n",
       " ...                                                      ...   \n",
       " 2018-09-11 18:59:36.240079096                      -0.003183   \n",
       " 2018-09-11 18:59:36.683336696                       0.015267   \n",
       " 2018-09-11 18:59:37.197629496                       0.006238   \n",
       " 2018-09-11 18:59:37.632176952                       0.018558   \n",
       " 2018-09-11 18:59:37.978509688                       0.032839   \n",
       " \n",
       "                                mavros-imu-data.orientation.w  \\\n",
       " timestamp                                                      \n",
       " 2018-09-11 18:55:44.815328696                      -0.164833   \n",
       " 2018-09-11 18:55:45.302239320                      -0.161779   \n",
       " 2018-09-11 18:55:45.691413272                      -0.192508   \n",
       " 2018-09-11 18:55:46.060852152                      -0.231648   \n",
       " 2018-09-11 18:55:46.443526008                      -0.268742   \n",
       " ...                                                      ...   \n",
       " 2018-09-11 18:59:36.240079096                      -0.992589   \n",
       " 2018-09-11 18:59:36.683336696                      -0.990352   \n",
       " 2018-09-11 18:59:37.197629496                      -0.989044   \n",
       " 2018-09-11 18:59:37.632176952                      -0.983683   \n",
       " 2018-09-11 18:59:37.978509688                      -0.980384   \n",
       " \n",
       "                                mavros-imu-data.angular_velocity.x  \\\n",
       " timestamp                                                           \n",
       " 2018-09-11 18:55:44.815328696                            1.198352   \n",
       " 2018-09-11 18:55:45.302239320                            0.344504   \n",
       " 2018-09-11 18:55:45.691413272                            0.204759   \n",
       " 2018-09-11 18:55:46.060852152                           -0.013796   \n",
       " 2018-09-11 18:55:46.443526008                           -0.006801   \n",
       " ...                                                           ...   \n",
       " 2018-09-11 18:59:36.240079096                            0.044647   \n",
       " 2018-09-11 18:59:36.683336696                            0.073532   \n",
       " 2018-09-11 18:59:37.197629496                           -0.298716   \n",
       " 2018-09-11 18:59:37.632176952                           -0.146525   \n",
       " 2018-09-11 18:59:37.978509688                           -0.147029   \n",
       " \n",
       "                                mavros-imu-data.angular_velocity.y  \\\n",
       " timestamp                                                           \n",
       " 2018-09-11 18:55:44.815328696                           -0.003927   \n",
       " 2018-09-11 18:55:45.302239320                            0.093244   \n",
       " 2018-09-11 18:55:45.691413272                            0.108409   \n",
       " 2018-09-11 18:55:46.060852152                            0.087250   \n",
       " 2018-09-11 18:55:46.443526008                            0.039049   \n",
       " ...                                                           ...   \n",
       " 2018-09-11 18:59:36.240079096                            0.161116   \n",
       " 2018-09-11 18:59:36.683336696                            0.198077   \n",
       " 2018-09-11 18:59:37.197629496                            0.236909   \n",
       " 2018-09-11 18:59:37.632176952                            0.182599   \n",
       " 2018-09-11 18:59:37.978509688                            0.090268   \n",
       " \n",
       "                                mavros-imu-data.angular_velocity.z  \\\n",
       " timestamp                                                           \n",
       " 2018-09-11 18:55:44.815328696                           -0.046569   \n",
       " 2018-09-11 18:55:45.302239320                           -0.081719   \n",
       " 2018-09-11 18:55:45.691413272                           -0.277471   \n",
       " 2018-09-11 18:55:46.060852152                           -0.250878   \n",
       " 2018-09-11 18:55:46.443526008                           -0.169687   \n",
       " ...                                                           ...   \n",
       " 2018-09-11 18:59:36.240079096                           -0.134020   \n",
       " 2018-09-11 18:59:36.683336696                           -0.064672   \n",
       " 2018-09-11 18:59:37.197629496                            0.000992   \n",
       " 2018-09-11 18:59:37.632176952                           -0.147860   \n",
       " 2018-09-11 18:59:37.978509688                           -0.082429   \n",
       " \n",
       "                                mavros-imu-data.linear_acceleration.x  \\\n",
       " timestamp                                                              \n",
       " 2018-09-11 18:55:44.815328696                              -0.402073   \n",
       " 2018-09-11 18:55:45.302239320                              -1.353318   \n",
       " 2018-09-11 18:55:45.691413272                              -1.637711   \n",
       " 2018-09-11 18:55:46.060852152                              -1.980943   \n",
       " 2018-09-11 18:55:46.443526008                              -2.216303   \n",
       " ...                                                              ...   \n",
       " 2018-09-11 18:59:36.240079096                              -0.529559   \n",
       " 2018-09-11 18:59:36.683336696                              -1.108151   \n",
       " 2018-09-11 18:59:37.197629496                              -1.353318   \n",
       " 2018-09-11 18:59:37.632176952                              -1.520031   \n",
       " 2018-09-11 18:59:37.978509688                              -1.696550   \n",
       " \n",
       "                                mavros-imu-data.linear_acceleration.y  \\\n",
       " timestamp                                                              \n",
       " 2018-09-11 18:55:44.815328696                               0.127486   \n",
       " 2018-09-11 18:55:45.302239320                               0.539366   \n",
       " 2018-09-11 18:55:45.691413272                               0.480526   \n",
       " 2018-09-11 18:55:46.060852152                               0.647239   \n",
       " 2018-09-11 18:55:46.443526008                               0.931632   \n",
       " ...                                                              ...   \n",
       " 2018-09-11 18:59:36.240079096                               0.323619   \n",
       " 2018-09-11 18:59:36.683336696                               0.421686   \n",
       " 2018-09-11 18:59:37.197629496                               0.823759   \n",
       " 2018-09-11 18:59:37.632176952                               0.657046   \n",
       " 2018-09-11 18:59:37.978509688                               0.049033   \n",
       " \n",
       "                                mavros-imu-data.linear_acceleration.z  \n",
       " timestamp                                                             \n",
       " 2018-09-11 18:55:44.815328696                               8.463139  \n",
       " 2018-09-11 18:55:45.302239320                               7.874740  \n",
       " 2018-09-11 18:55:45.691413272                               4.138406  \n",
       " 2018-09-11 18:55:46.060852152                               5.864377  \n",
       " 2018-09-11 18:55:46.443526008                               7.757060  \n",
       " ...                                                              ...  \n",
       " 2018-09-11 18:59:36.240079096                               7.678607  \n",
       " 2018-09-11 18:59:36.683336696                               6.246836  \n",
       " 2018-09-11 18:59:37.197629496                               5.060231  \n",
       " 2018-09-11 18:59:37.632176952                               4.501252  \n",
       " 2018-09-11 18:59:37.978509688                               7.139241  \n",
       " \n",
       " [574 rows x 10 columns],\n",
       "                                mavros-imu-temperature.temperature\n",
       " timestamp                                                        \n",
       " 2018-09-11 18:55:44.739417624                               41.89\n",
       " 2018-09-11 18:55:44.821516824                               41.89\n",
       " 2018-09-11 18:55:44.938254904                               41.88\n",
       " 2018-09-11 18:55:45.038322712                               41.87\n",
       " 2018-09-11 18:55:45.123392344                               41.87\n",
       " ...                                                           ...\n",
       " 2018-09-11 18:59:37.626645912                               37.06\n",
       " 2018-09-11 18:59:37.734617272                               37.05\n",
       " 2018-09-11 18:59:37.833124216                               37.05\n",
       " 2018-09-11 18:59:37.924084376                               37.05\n",
       " 2018-09-11 18:59:38.018026488                               37.06\n",
       " \n",
       " [2334 rows x 1 columns],\n",
       "                                mavros-nav_info-airspeed.measured\n",
       " timestamp                                                       \n",
       " 2018-09-11 18:55:44.751156440                                0.0\n",
       " 2018-09-11 18:55:44.811619032                                0.0\n",
       " 2018-09-11 18:55:44.837812728                                0.0\n",
       " 2018-09-11 18:55:44.900119192                                0.0\n",
       " 2018-09-11 18:55:44.965182744                                0.0\n",
       " ...                                                          ...\n",
       " 2018-09-11 18:59:37.860557656                                0.0\n",
       " 2018-09-11 18:59:37.920680280                                0.0\n",
       " 2018-09-11 18:59:37.974660856                                0.0\n",
       " 2018-09-11 18:59:38.013118776                                0.0\n",
       " 2018-09-11 18:59:38.073726968                                0.0\n",
       " \n",
       " [4348 rows x 1 columns],\n",
       "                                mavros-nav_info-errors.alt_error  \\\n",
       " timestamp                                                         \n",
       " 2018-09-11 18:55:44.733756664                         -0.300000   \n",
       " 2018-09-11 18:55:44.747403736                         -0.300000   \n",
       " 2018-09-11 18:55:44.786480952                         -0.300000   \n",
       " 2018-09-11 18:55:44.836539000                         -0.430000   \n",
       " 2018-09-11 18:55:44.898553720                         -0.430000   \n",
       " ...                                                         ...   \n",
       " 2018-09-11 18:59:37.860320760                         20.289999   \n",
       " 2018-09-11 18:59:37.917356952                         20.289999   \n",
       " 2018-09-11 18:59:37.969003608                         20.859999   \n",
       " 2018-09-11 18:59:38.011773848                         20.859999   \n",
       " 2018-09-11 18:59:38.057258392                         21.469999   \n",
       " \n",
       "                                mavros-nav_info-errors.aspd_error  \\\n",
       " timestamp                                                          \n",
       " 2018-09-11 18:55:44.733756664                        -681.648438   \n",
       " 2018-09-11 18:55:44.747403736                        -681.648438   \n",
       " 2018-09-11 18:55:44.786480952                        -681.648438   \n",
       " 2018-09-11 18:55:44.836539000                        -684.812134   \n",
       " 2018-09-11 18:55:44.898553720                        -684.812134   \n",
       " ...                                                          ...   \n",
       " 2018-09-11 18:59:37.860320760                         -41.753960   \n",
       " 2018-09-11 18:59:37.917356952                         -41.753960   \n",
       " 2018-09-11 18:59:37.969003608                         -41.753960   \n",
       " 2018-09-11 18:59:38.011773848                         -41.753960   \n",
       " 2018-09-11 18:59:38.057258392                         -42.135429   \n",
       " \n",
       "                                mavros-nav_info-errors.xtrack_error  \\\n",
       " timestamp                                                            \n",
       " 2018-09-11 18:55:44.733756664                           -40.078075   \n",
       " 2018-09-11 18:55:44.747403736                           -40.078075   \n",
       " 2018-09-11 18:55:44.786480952                           -40.078075   \n",
       " 2018-09-11 18:55:44.836539000                           -37.936432   \n",
       " 2018-09-11 18:55:44.898553720                           -37.936432   \n",
       " ...                                                            ...   \n",
       " 2018-09-11 18:59:37.860320760                             5.042744   \n",
       " 2018-09-11 18:59:37.917356952                             5.042744   \n",
       " 2018-09-11 18:59:37.969003608                             4.909161   \n",
       " 2018-09-11 18:59:38.011773848                             4.909161   \n",
       " 2018-09-11 18:59:38.057258392                             4.764447   \n",
       " \n",
       "                                mavros-nav_info-errors.wp_dist  \n",
       " timestamp                                                      \n",
       " 2018-09-11 18:55:44.733756664                               4  \n",
       " 2018-09-11 18:55:44.747403736                               4  \n",
       " 2018-09-11 18:55:44.786480952                               4  \n",
       " 2018-09-11 18:55:44.836539000                               6  \n",
       " 2018-09-11 18:55:44.898553720                               6  \n",
       " ...                                                       ...  \n",
       " 2018-09-11 18:59:37.860320760                             165  \n",
       " 2018-09-11 18:59:37.917356952                             165  \n",
       " 2018-09-11 18:59:37.969003608                             165  \n",
       " 2018-09-11 18:59:38.011773848                             165  \n",
       " 2018-09-11 18:59:38.057258392                             165  \n",
       " \n",
       " [4440 rows x 4 columns],\n",
       "                                mavros-nav_info-pitch.measured\n",
       " timestamp                                                    \n",
       " 2018-09-11 18:55:44.733683000                        0.797831\n",
       " 2018-09-11 18:55:44.747360408                        0.797831\n",
       " 2018-09-11 18:55:44.786440792                        0.797831\n",
       " 2018-09-11 18:55:44.836514104                        0.183815\n",
       " 2018-09-11 18:55:44.898461464                        0.183815\n",
       " ...                                                       ...\n",
       " 2018-09-11 18:59:37.860291352                      -19.601870\n",
       " 2018-09-11 18:59:37.917281208                      -19.601870\n",
       " 2018-09-11 18:59:37.967814840                      -19.601870\n",
       " 2018-09-11 18:59:38.011744568                      -22.344082\n",
       " 2018-09-11 18:59:38.057027960                      -22.344082\n",
       " \n",
       " [4440 rows x 1 columns],\n",
       "                                mavros-nav_info-roll.measured\n",
       " timestamp                                                   \n",
       " 2018-09-11 18:55:44.733653240                     -11.961676\n",
       " 2018-09-11 18:55:44.747348664                     -11.961676\n",
       " 2018-09-11 18:55:44.785582360                     -11.961676\n",
       " 2018-09-11 18:55:44.836487064                       2.863091\n",
       " 2018-09-11 18:55:44.898428952                       2.863091\n",
       " ...                                                      ...\n",
       " 2018-09-11 18:59:37.917263992                       6.519372\n",
       " 2018-09-11 18:59:37.967593688                       6.519372\n",
       " 2018-09-11 18:59:38.011726584                       1.801064\n",
       " 2018-09-11 18:59:38.056942072                       1.801064\n",
       " 2018-09-11 18:59:38.118799704                       1.801064\n",
       " \n",
       " [4441 rows x 1 columns],\n",
       "                                mavros-nav_info-velocity.des_x  \\\n",
       " timestamp                                                       \n",
       " 2018-09-11 18:55:44.751172664                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.811629816                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.837833816                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.900179000                    1.600000e+01   \n",
       " 2018-09-11 18:55:44.964750008                    1.600000e+01   \n",
       " ...                                                       ...   \n",
       " 2018-09-11 18:59:37.860689592                    3.199951e-15   \n",
       " 2018-09-11 18:59:37.920835864                    3.169954e-15   \n",
       " 2018-09-11 18:59:37.974727832                    3.169954e-15   \n",
       " 2018-09-11 18:59:38.013186072                    3.169954e-15   \n",
       " 2018-09-11 18:59:38.073708088                    3.169954e-15   \n",
       " \n",
       "                                mavros-nav_info-velocity.des_y  \\\n",
       " timestamp                                                       \n",
       " 2018-09-11 18:55:44.751172664                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.811629816                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.837833816                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.900179000                   -3.552714e-15   \n",
       " 2018-09-11 18:55:44.964750008                   -3.552714e-15   \n",
       " ...                                                       ...   \n",
       " 2018-09-11 18:59:37.860689592                    1.580050e+01   \n",
       " 2018-09-11 18:59:37.920835864                    1.576931e+01   \n",
       " 2018-09-11 18:59:37.974727832                    1.576931e+01   \n",
       " 2018-09-11 18:59:38.013186072                    1.576931e+01   \n",
       " 2018-09-11 18:59:38.073708088                    1.576931e+01   \n",
       " \n",
       "                                mavros-nav_info-velocity.des_z  \\\n",
       " timestamp                                                       \n",
       " 2018-09-11 18:55:44.751172664                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.811629816                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.837833816                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.900179000                    1.959435e-15   \n",
       " 2018-09-11 18:55:44.964750008                    1.959435e-15   \n",
       " ...                                                       ...   \n",
       " 2018-09-11 18:59:37.860689592                    2.518796e+00   \n",
       " 2018-09-11 18:59:37.920835864                    2.707196e+00   \n",
       " 2018-09-11 18:59:37.974727832                    2.707196e+00   \n",
       " 2018-09-11 18:59:38.013186072                    2.707196e+00   \n",
       " 2018-09-11 18:59:38.073708088                    2.707196e+00   \n",
       " \n",
       "                                mavros-nav_info-velocity.meas_x  \\\n",
       " timestamp                                                        \n",
       " 2018-09-11 18:55:44.751172664                         5.448525   \n",
       " 2018-09-11 18:55:44.811629816                         5.435284   \n",
       " 2018-09-11 18:55:44.837833816                         5.446200   \n",
       " 2018-09-11 18:55:44.900179000                         5.503401   \n",
       " 2018-09-11 18:55:44.964750008                         5.601675   \n",
       " ...                                                        ...   \n",
       " 2018-09-11 18:59:37.860689592                        -1.700567   \n",
       " 2018-09-11 18:59:37.920835864                        -1.696898   \n",
       " 2018-09-11 18:59:37.974727832                        -1.686711   \n",
       " 2018-09-11 18:59:38.013186072                        -1.687362   \n",
       " 2018-09-11 18:59:38.073708088                        -1.674494   \n",
       " \n",
       "                                mavros-nav_info-velocity.meas_y  \\\n",
       " timestamp                                                        \n",
       " 2018-09-11 18:55:44.751172664                       -20.630127   \n",
       " 2018-09-11 18:55:44.811629816                       -20.569389   \n",
       " 2018-09-11 18:55:44.837833816                       -20.520662   \n",
       " 2018-09-11 18:55:44.900179000                       -20.415331   \n",
       " 2018-09-11 18:55:44.964750008                       -20.283449   \n",
       " ...                                                        ...   \n",
       " 2018-09-11 18:59:37.860689592                        16.762850   \n",
       " 2018-09-11 18:59:37.920835864                        16.808422   \n",
       " 2018-09-11 18:59:37.974727832                        16.851337   \n",
       " 2018-09-11 18:59:38.013186072                        16.912659   \n",
       " 2018-09-11 18:59:38.073708088                        17.052553   \n",
       " \n",
       "                                mavros-nav_info-velocity.meas_z  \n",
       " timestamp                                                       \n",
       " 2018-09-11 18:55:44.751172664                        -1.310496  \n",
       " 2018-09-11 18:55:44.811629816                        -1.264987  \n",
       " 2018-09-11 18:55:44.837833816                        -1.268989  \n",
       " 2018-09-11 18:55:44.900179000                        -1.315569  \n",
       " 2018-09-11 18:55:44.964750008                        -1.351536  \n",
       " ...                                                        ...  \n",
       " 2018-09-11 18:59:37.860689592                         5.994641  \n",
       " 2018-09-11 18:59:37.920835864                         6.185676  \n",
       " 2018-09-11 18:59:37.974727832                         6.284477  \n",
       " 2018-09-11 18:59:38.013186072                         6.342451  \n",
       " 2018-09-11 18:59:38.073708088                         6.416114  \n",
       " \n",
       " [4348 rows x 6 columns],\n",
       "                                mavros-nav_info-yaw.measured\n",
       " timestamp                                                  \n",
       " 2018-09-11 18:55:44.733721848                    -70.156189\n",
       " 2018-09-11 18:55:44.747371832                    -70.156189\n",
       " 2018-09-11 18:55:44.786465720                    -70.156189\n",
       " 2018-09-11 18:55:44.836527032                    -71.014297\n",
       " 2018-09-11 18:55:44.898478872                    -71.014297\n",
       " ...                                                     ...\n",
       " 2018-09-11 18:59:37.860307032                     91.034180\n",
       " 2018-09-11 18:59:37.917339416                     91.034180\n",
       " 2018-09-11 18:59:37.967848504                     91.034180\n",
       " 2018-09-11 18:59:38.011759000                     93.481194\n",
       " 2018-09-11 18:59:38.057181560                     93.481194\n",
       " \n",
       " [4440 rows x 1 columns],\n",
       "                                mavros-time_reference.time_ref\n",
       " timestamp                                                    \n",
       " 2018-09-11 18:55:44.736821976             1536692377361000000\n",
       " 2018-09-11 18:55:45.206083608             1536692377841000000\n",
       " 2018-09-11 18:55:45.815905752             1536692378440000000\n",
       " 2018-09-11 18:55:46.251725048             1536692378879000000\n",
       " 2018-09-11 18:55:46.631150328             1536692379261000000\n",
       " ...                                                       ...\n",
       " 2018-09-11 18:59:35.671488152             1536692608301000000\n",
       " 2018-09-11 18:59:36.138563064             1536692608778000000\n",
       " 2018-09-11 18:59:36.746569464             1536692609360000000\n",
       " 2018-09-11 18:59:37.272394168             1536692609917000000\n",
       " 2018-09-11 18:59:37.862205048             1536692610500000000\n",
       " \n",
       " [477 rows x 1 columns],\n",
       "                                mavros-vfr_hud.airspeed  \\\n",
       " timestamp                                                \n",
       " 2018-09-11 18:55:44.813606648                      0.0   \n",
       " 2018-09-11 18:55:45.288424728                      0.0   \n",
       " 2018-09-11 18:55:45.689541272                      0.0   \n",
       " 2018-09-11 18:55:46.059403576                      0.0   \n",
       " 2018-09-11 18:55:46.439877208                      0.0   \n",
       " ...                                                ...   \n",
       " 2018-09-11 18:59:36.236687256                      0.0   \n",
       " 2018-09-11 18:59:36.682041176                      0.0   \n",
       " 2018-09-11 18:59:37.193585176                      0.0   \n",
       " 2018-09-11 18:59:37.628674168                      0.0   \n",
       " 2018-09-11 18:59:37.976107736                      0.0   \n",
       " \n",
       "                                mavros-vfr_hud.groundspeed  \\\n",
       " timestamp                                                   \n",
       " 2018-09-11 18:55:44.813606648                   21.275387   \n",
       " 2018-09-11 18:55:45.288424728                   20.532625   \n",
       " 2018-09-11 18:55:45.689541272                   19.939848   \n",
       " 2018-09-11 18:55:46.059403576                   19.413969   \n",
       " 2018-09-11 18:55:46.439877208                   19.055178   \n",
       " ...                                                   ...   \n",
       " 2018-09-11 18:59:36.236687256                   17.511122   \n",
       " 2018-09-11 18:59:36.682041176                   17.207243   \n",
       " 2018-09-11 18:59:37.193585176                   16.966467   \n",
       " 2018-09-11 18:59:37.628674168                   16.777716   \n",
       " 2018-09-11 18:59:37.976107736                   16.935541   \n",
       " \n",
       "                                mavros-vfr_hud.heading  \\\n",
       " timestamp                                               \n",
       " 2018-09-11 18:55:44.813606648                     288   \n",
       " 2018-09-11 18:55:45.288424728                     288   \n",
       " 2018-09-11 18:55:45.689541272                     291   \n",
       " 2018-09-11 18:55:46.059403576                     294   \n",
       " 2018-09-11 18:55:46.439877208                     298   \n",
       " ...                                               ...   \n",
       " 2018-09-11 18:59:36.236687256                      89   \n",
       " 2018-09-11 18:59:36.682041176                      90   \n",
       " 2018-09-11 18:59:37.193585176                      89   \n",
       " 2018-09-11 18:59:37.628674168                      91   \n",
       " 2018-09-11 18:59:37.976107736                      93   \n",
       " \n",
       "                                mavros-vfr_hud.throttle  \\\n",
       " timestamp                                                \n",
       " 2018-09-11 18:55:44.813606648                     0.42   \n",
       " 2018-09-11 18:55:45.288424728                     0.31   \n",
       " 2018-09-11 18:55:45.689541272                     0.19   \n",
       " 2018-09-11 18:55:46.059403576                     0.13   \n",
       " 2018-09-11 18:55:46.439877208                     0.07   \n",
       " ...                                                ...   \n",
       " 2018-09-11 18:59:36.236687256                     0.28   \n",
       " 2018-09-11 18:59:36.682041176                     0.13   \n",
       " 2018-09-11 18:59:37.193585176                     0.02   \n",
       " 2018-09-11 18:59:37.628674168                     0.00   \n",
       " 2018-09-11 18:59:37.976107736                     0.00   \n",
       " \n",
       "                                mavros-vfr_hud.altitude  mavros-vfr_hud.climb  \n",
       " timestamp                                                                     \n",
       " 2018-09-11 18:55:44.813606648               410.979980              2.028334  \n",
       " 2018-09-11 18:55:45.288424728               411.619995              5.472738  \n",
       " 2018-09-11 18:55:45.689541272               411.729980              2.834411  \n",
       " 2018-09-11 18:55:46.059403576               411.119995             -0.804450  \n",
       " 2018-09-11 18:55:46.439877208               409.899994             -2.400985  \n",
       " ...                                                ...                   ...  \n",
       " 2018-09-11 18:59:36.236687256               393.539978              3.508874  \n",
       " 2018-09-11 18:59:36.682041176               393.690002              2.542262  \n",
       " 2018-09-11 18:59:37.193585176               393.039978              0.578386  \n",
       " 2018-09-11 18:59:37.628674168               391.380005             -2.801878  \n",
       " 2018-09-11 18:59:37.976107736               389.500000             -4.096410  \n",
       " \n",
       " [574 rows x 6 columns],\n",
       "                                mavros-wind_estimation.twist.linear.x  \\\n",
       " timestamp                                                              \n",
       " 2018-09-11 18:55:44.735729400                              -0.244657   \n",
       " 2018-09-11 18:55:45.204481688                              -0.246626   \n",
       " 2018-09-11 18:55:45.815334840                              -0.256024   \n",
       " 2018-09-11 18:55:46.251742712                              -0.263208   \n",
       " 2018-09-11 18:55:46.630086456                              -0.272890   \n",
       " ...                                                              ...   \n",
       " 2018-09-11 18:59:35.669880760                              -0.392364   \n",
       " 2018-09-11 18:59:36.137095000                              -0.391501   \n",
       " 2018-09-11 18:59:36.745169944                              -0.392030   \n",
       " 2018-09-11 18:59:37.272128440                              -0.391855   \n",
       " 2018-09-11 18:59:37.860959800                              -0.388418   \n",
       " \n",
       "                                mavros-wind_estimation.twist.linear.y  \\\n",
       " timestamp                                                              \n",
       " 2018-09-11 18:55:44.735729400                               1.064529   \n",
       " 2018-09-11 18:55:45.204481688                               1.066408   \n",
       " 2018-09-11 18:55:45.815334840                               1.059074   \n",
       " 2018-09-11 18:55:46.251742712                               1.054658   \n",
       " 2018-09-11 18:55:46.630086456                               1.050315   \n",
       " ...                                                              ...   \n",
       " 2018-09-11 18:59:35.669880760                               1.266036   \n",
       " 2018-09-11 18:59:36.137095000                               1.265149   \n",
       " 2018-09-11 18:59:36.745169944                               1.257739   \n",
       " 2018-09-11 18:59:37.272128440                               1.251944   \n",
       " 2018-09-11 18:59:37.860959800                               1.251336   \n",
       " \n",
       "                                mavros-wind_estimation.twist.linear.z  \n",
       " timestamp                                                             \n",
       " 2018-09-11 18:55:44.735729400                                    0.0  \n",
       " 2018-09-11 18:55:45.204481688                                    0.0  \n",
       " 2018-09-11 18:55:45.815334840                                    0.0  \n",
       " 2018-09-11 18:55:46.251742712                                    0.0  \n",
       " 2018-09-11 18:55:46.630086456                                    0.0  \n",
       " ...                                                              ...  \n",
       " 2018-09-11 18:59:35.669880760                                    0.0  \n",
       " 2018-09-11 18:59:36.137095000                                    0.0  \n",
       " 2018-09-11 18:59:36.745169944                                    0.0  \n",
       " 2018-09-11 18:59:37.272128440                                    0.0  \n",
       " 2018-09-11 18:59:37.860959800                                    0.0  \n",
       " \n",
       " [477 rows x 3 columns]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict[\"carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failure Status:\n",
    "    \n",
    "    Engine     = 1\n",
    "    Rudder     = 2\n",
    "    Aileron    = 3\n",
    "    Elevator   = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_failure(flight_name,df_failure,failure_duration_start,failure_duration_end):\n",
    "    if \"engine\" in flight_name:\n",
    "        failure_status = [\n",
    "        1 if (x > failure_duration_start and x < failure_duration_end) else 0 \n",
    "        for x in df_failure.index\n",
    "                ]\n",
    "    elif \"rudder\" in flight_name:\n",
    "        failure_status = [\n",
    "        2 if (x > failure_duration_start and x < failure_duration_end) else 0 \n",
    "        for x in df_failure.index\n",
    "                ]\n",
    "    elif \"aileron\" in flight_name:\n",
    "        failure_status = [\n",
    "        3 if (x > failure_duration_start and x < failure_duration_end) else 0 \n",
    "        for x in df_failure.index\n",
    "                ]\n",
    "    elif \"elevator\" in flight_name:\n",
    "        failure_status = [\n",
    "        4 if (x > failure_duration_start and x < failure_duration_end) else 0 \n",
    "        for x in df_failure.index\n",
    "                ]\n",
    "        \n",
    "    df_failure[\"failure_status\"] = failure_status\n",
    "        \n",
    "    return df_failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carbonZ_2018-07-18-15-53-31_1_engine_failure',\n",
       " 'carbonZ_2018-07-18-15-53-31_2_engine_failure',\n",
       " 'carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-07-30-16-39-00_1_engine_failure',\n",
       " 'carbonZ_2018-07-30-16-39-00_2_engine_failure',\n",
       " 'carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-09-11-11-56-30_engine_failure',\n",
       " 'carbonZ_2018-09-11-14-22-07_1_engine_failure',\n",
       " 'carbonZ_2018-09-11-14-22-07_2_engine_failure',\n",
       " 'carbonZ_2018-09-11-14-41-51_elevator_failure',\n",
       " 'carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure',\n",
       " 'carbonZ_2018-09-11-15-05-11_1_elevator_failure',\n",
       " 'carbonZ_2018-09-11-15-06-34_1_rudder_right_failure',\n",
       " 'carbonZ_2018-09-11-15-06-34_2_rudder_right_failure',\n",
       " 'carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure',\n",
       " 'carbonZ_2018-09-11-17-55-30_1_right_aileron_failure',\n",
       " 'carbonZ_2018-09-11-17-55-30_2_left_aileron_failure',\n",
       " 'carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-05-14-37-22_2_right_aileron_failure',\n",
       " 'carbonZ_2018-10-05-14-37-22_3_left_aileron_failure',\n",
       " 'carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure',\n",
       " 'carbonZ_2018-09-11-17-27-13_1_left_aileron_failure']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_1_engine_failure\n",
      "carbonZ_2018-07-18-15-53-31_2_engine_failure\n",
      "carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-16-39-00_1_engine_failure\n",
      "carbonZ_2018-07-30-16-39-00_2_engine_failure\n",
      "carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj\n",
      "carbonZ_2018-09-11-11-56-30_engine_failure\n",
      "carbonZ_2018-09-11-14-22-07_1_engine_failure\n",
      "carbonZ_2018-09-11-14-22-07_2_engine_failure\n",
      "carbonZ_2018-09-11-14-41-51_elevator_failure\n",
      "carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\n",
      "carbonZ_2018-09-11-15-05-11_1_elevator_failure\n",
      "carbonZ_2018-09-11-15-06-34_1_rudder_right_failure\n",
      "carbonZ_2018-09-11-15-06-34_2_rudder_right_failure\n",
      "carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure\n",
      "carbonZ_2018-09-11-17-55-30_1_right_aileron_failure\n",
      "carbonZ_2018-09-11-17-55-30_2_left_aileron_failure\n",
      "carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-14-37-22_2_right_aileron_failure\n",
      "carbonZ_2018-10-05-14-37-22_3_left_aileron_failure\n",
      "carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj\n",
      "carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\n",
      "carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\n"
     ]
    }
   ],
   "source": [
    "for flight_name in flight_topic_list:\n",
    "    print(flight_name)\n",
    "    dfs = df_dict[flight_name] \n",
    "    \n",
    "    start_time = min(min(dfs[0].index),min(dfs[1].index),min(dfs[2].index),min(dfs[3].index),min(dfs[4].index),min(dfs[5].index),min(dfs[6].index),min(dfs[7].index),min(dfs[8].index),min(dfs[9].index),min(dfs[10].index),min(dfs[11].index),min(dfs[12].index))\n",
    "    end_time = max(max(dfs[0].index),max(dfs[1].index),max(dfs[2].index),max(dfs[3].index),max(dfs[4].index),max(dfs[5].index),max(dfs[6].index),max(dfs[7].index),max(dfs[8].index),max(dfs[9].index),max(dfs[10].index),max(dfs[11].index),max(dfs[12].index))\n",
    "    \n",
    "    time_index = pd.date_range(start=start_time, end=end_time, freq=\"200ms\")  \n",
    "    data = [0] * len(time_index)\n",
    "\n",
    "    df_failure = pd.DataFrame(data, index=time_index, columns=[\"failure_status\"])\n",
    "\n",
    "    failure_duration_start = failure_status_dict[flight_name][0]\n",
    "    failure_duration_end   = failure_status_dict[flight_name][1]\n",
    "    \n",
    "    \n",
    "    df_dict[flight_name].append(add_failure(flight_name,df_failure,failure_duration_start,failure_duration_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mavros-wind_estimation.twist.linear.x</th>\n",
       "      <th>mavros-wind_estimation.twist.linear.y</th>\n",
       "      <th>mavros-wind_estimation.twist.linear.z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:19.188548395</th>\n",
       "      <td>0.360470</td>\n",
       "      <td>1.439026</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:19.566847897</th>\n",
       "      <td>0.352583</td>\n",
       "      <td>1.463511</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:19.954091484</th>\n",
       "      <td>0.345375</td>\n",
       "      <td>1.496819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:20.421941571</th>\n",
       "      <td>0.328750</td>\n",
       "      <td>1.548180</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:20.868431200</th>\n",
       "      <td>0.321009</td>\n",
       "      <td>1.588851</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:40.589048991</th>\n",
       "      <td>-0.256345</td>\n",
       "      <td>2.252147</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:40.936268682</th>\n",
       "      <td>-0.255210</td>\n",
       "      <td>2.252631</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:41.422716742</th>\n",
       "      <td>-0.253657</td>\n",
       "      <td>2.253724</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:41.785506158</th>\n",
       "      <td>-0.252744</td>\n",
       "      <td>2.256600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:42.286168071</th>\n",
       "      <td>-0.252200</td>\n",
       "      <td>2.260911</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mavros-wind_estimation.twist.linear.x  \\\n",
       "timestamp                                                              \n",
       "2018-09-11 21:34:19.188548395                               0.360470   \n",
       "2018-09-11 21:34:19.566847897                               0.352583   \n",
       "2018-09-11 21:34:19.954091484                               0.345375   \n",
       "2018-09-11 21:34:20.421941571                               0.328750   \n",
       "2018-09-11 21:34:20.868431200                               0.321009   \n",
       "...                                                              ...   \n",
       "2018-09-11 21:36:40.589048991                              -0.256345   \n",
       "2018-09-11 21:36:40.936268682                              -0.255210   \n",
       "2018-09-11 21:36:41.422716742                              -0.253657   \n",
       "2018-09-11 21:36:41.785506158                              -0.252744   \n",
       "2018-09-11 21:36:42.286168071                              -0.252200   \n",
       "\n",
       "                               mavros-wind_estimation.twist.linear.y  \\\n",
       "timestamp                                                              \n",
       "2018-09-11 21:34:19.188548395                               1.439026   \n",
       "2018-09-11 21:34:19.566847897                               1.463511   \n",
       "2018-09-11 21:34:19.954091484                               1.496819   \n",
       "2018-09-11 21:34:20.421941571                               1.548180   \n",
       "2018-09-11 21:34:20.868431200                               1.588851   \n",
       "...                                                              ...   \n",
       "2018-09-11 21:36:40.589048991                               2.252147   \n",
       "2018-09-11 21:36:40.936268682                               2.252631   \n",
       "2018-09-11 21:36:41.422716742                               2.253724   \n",
       "2018-09-11 21:36:41.785506158                               2.256600   \n",
       "2018-09-11 21:36:42.286168071                               2.260911   \n",
       "\n",
       "                               mavros-wind_estimation.twist.linear.z  \n",
       "timestamp                                                             \n",
       "2018-09-11 21:34:19.188548395                                    0.0  \n",
       "2018-09-11 21:34:19.566847897                                    0.0  \n",
       "2018-09-11 21:34:19.954091484                                    0.0  \n",
       "2018-09-11 21:34:20.421941571                                    0.0  \n",
       "2018-09-11 21:34:20.868431200                                    0.0  \n",
       "...                                                              ...  \n",
       "2018-09-11 21:36:40.589048991                                    0.0  \n",
       "2018-09-11 21:36:40.936268682                                    0.0  \n",
       "2018-09-11 21:36:41.422716742                                    0.0  \n",
       "2018-09-11 21:36:41.785506158                                    0.0  \n",
       "2018-09-11 21:36:42.286168071                                    0.0  \n",
       "\n",
       "[311 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict[\"carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\"]\n",
    "df[12].iloc[: , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mavros-wind_estimation.twist.linear.x</th>\n",
       "      <th>mavros-wind_estimation.twist.linear.y</th>\n",
       "      <th>mavros-wind_estimation.twist.linear.z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:19.188548395</th>\n",
       "      <td>0.360470</td>\n",
       "      <td>1.439026</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:19.566847897</th>\n",
       "      <td>0.352583</td>\n",
       "      <td>1.463511</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:19.954091484</th>\n",
       "      <td>0.345375</td>\n",
       "      <td>1.496819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:20.421941571</th>\n",
       "      <td>0.328750</td>\n",
       "      <td>1.548180</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:20.868431200</th>\n",
       "      <td>0.321009</td>\n",
       "      <td>1.588851</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:40.589048991</th>\n",
       "      <td>-0.256345</td>\n",
       "      <td>2.252147</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:40.936268682</th>\n",
       "      <td>-0.255210</td>\n",
       "      <td>2.252631</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:41.422716742</th>\n",
       "      <td>-0.253657</td>\n",
       "      <td>2.253724</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:41.785506158</th>\n",
       "      <td>-0.252744</td>\n",
       "      <td>2.256600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:42.286168071</th>\n",
       "      <td>-0.252200</td>\n",
       "      <td>2.260911</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mavros-wind_estimation.twist.linear.x  \\\n",
       "timestamp                                                              \n",
       "2018-09-11 21:34:19.188548395                               0.360470   \n",
       "2018-09-11 21:34:19.566847897                               0.352583   \n",
       "2018-09-11 21:34:19.954091484                               0.345375   \n",
       "2018-09-11 21:34:20.421941571                               0.328750   \n",
       "2018-09-11 21:34:20.868431200                               0.321009   \n",
       "...                                                              ...   \n",
       "2018-09-11 21:36:40.589048991                              -0.256345   \n",
       "2018-09-11 21:36:40.936268682                              -0.255210   \n",
       "2018-09-11 21:36:41.422716742                              -0.253657   \n",
       "2018-09-11 21:36:41.785506158                              -0.252744   \n",
       "2018-09-11 21:36:42.286168071                              -0.252200   \n",
       "\n",
       "                               mavros-wind_estimation.twist.linear.y  \\\n",
       "timestamp                                                              \n",
       "2018-09-11 21:34:19.188548395                               1.439026   \n",
       "2018-09-11 21:34:19.566847897                               1.463511   \n",
       "2018-09-11 21:34:19.954091484                               1.496819   \n",
       "2018-09-11 21:34:20.421941571                               1.548180   \n",
       "2018-09-11 21:34:20.868431200                               1.588851   \n",
       "...                                                              ...   \n",
       "2018-09-11 21:36:40.589048991                               2.252147   \n",
       "2018-09-11 21:36:40.936268682                               2.252631   \n",
       "2018-09-11 21:36:41.422716742                               2.253724   \n",
       "2018-09-11 21:36:41.785506158                               2.256600   \n",
       "2018-09-11 21:36:42.286168071                               2.260911   \n",
       "\n",
       "                               mavros-wind_estimation.twist.linear.z  \n",
       "timestamp                                                             \n",
       "2018-09-11 21:34:19.188548395                                    0.0  \n",
       "2018-09-11 21:34:19.566847897                                    0.0  \n",
       "2018-09-11 21:34:19.954091484                                    0.0  \n",
       "2018-09-11 21:34:20.421941571                                    0.0  \n",
       "2018-09-11 21:34:20.868431200                                    0.0  \n",
       "...                                                              ...  \n",
       "2018-09-11 21:36:40.589048991                                    0.0  \n",
       "2018-09-11 21:36:40.936268682                                    0.0  \n",
       "2018-09-11 21:36:41.422716742                                    0.0  \n",
       "2018-09-11 21:36:41.785506158                                    0.0  \n",
       "2018-09-11 21:36:42.286168071                                    0.0  \n",
       "\n",
       "[311 rows x 3 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\"]\n",
    "df[12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_1_engine_failure\n",
      "14\n",
      "carbonZ_2018-07-18-15-53-31_2_engine_failure\n",
      "14\n",
      "carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-07-30-16-39-00_1_engine_failure\n",
      "14\n",
      "carbonZ_2018-07-30-16-39-00_2_engine_failure\n",
      "14\n",
      "carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-09-11-11-56-30_engine_failure\n",
      "14\n",
      "carbonZ_2018-09-11-14-22-07_1_engine_failure\n",
      "14\n",
      "carbonZ_2018-09-11-14-22-07_2_engine_failure\n",
      "14\n",
      "carbonZ_2018-09-11-14-41-51_elevator_failure\n",
      "14\n",
      "carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\n",
      "14\n",
      "carbonZ_2018-09-11-15-05-11_1_elevator_failure\n",
      "14\n",
      "carbonZ_2018-09-11-15-06-34_1_rudder_right_failure\n",
      "14\n",
      "carbonZ_2018-09-11-15-06-34_2_rudder_right_failure\n",
      "14\n",
      "carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure\n",
      "14\n",
      "carbonZ_2018-09-11-17-55-30_1_right_aileron_failure\n",
      "14\n",
      "carbonZ_2018-09-11-17-55-30_2_left_aileron_failure\n",
      "14\n",
      "carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-05-14-37-22_2_right_aileron_failure\n",
      "14\n",
      "carbonZ_2018-10-05-14-37-22_3_left_aileron_failure\n",
      "14\n",
      "carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\n",
      "14\n",
      "carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "for key, df in df_dict.items():\n",
    "    print(key)\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Birleştirme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mavros-imu-atm_pressure.fluid_pressure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.883316762</th>\n",
       "      <td>97310.498047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.958577559</th>\n",
       "      <td>97310.498047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:51.062952770</th>\n",
       "      <td>97329.193115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:51.153503583</th>\n",
       "      <td>97341.693115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:51.245127118</th>\n",
       "      <td>97329.223633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:02.784104260</th>\n",
       "      <td>97410.662842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:02.886696084</th>\n",
       "      <td>97414.984131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:02.954867820</th>\n",
       "      <td>97414.984131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.072783613</th>\n",
       "      <td>97418.975830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.128568663</th>\n",
       "      <td>97418.975830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1323 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mavros-imu-atm_pressure.fluid_pressure\n",
       "timestamp                                                            \n",
       "2018-07-18 19:56:50.883316762                            97310.498047\n",
       "2018-07-18 19:56:50.958577559                            97310.498047\n",
       "2018-07-18 19:56:51.062952770                            97329.193115\n",
       "2018-07-18 19:56:51.153503583                            97341.693115\n",
       "2018-07-18 19:56:51.245127118                            97329.223633\n",
       "...                                                               ...\n",
       "2018-07-18 19:59:02.784104260                            97410.662842\n",
       "2018-07-18 19:59:02.886696084                            97414.984131\n",
       "2018-07-18 19:59:02.954867820                            97414.984131\n",
       "2018-07-18 19:59:03.072783613                            97418.975830\n",
       "2018-07-18 19:59:03.128568663                            97418.975830\n",
       "\n",
       "[1323 rows x 1 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict[\"carbonZ_2018-07-18-15-53-31_1_engine_failure\"][1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict = dict()\n",
    "\n",
    "# for key, dfs in df_dict.items():\n",
    "#     print(key)\n",
    "#     df_1 = dfs[1].reset_index().merge(dfs[3].reset_index(drop=True),right_index=True,left_index=True,how=\"outer\").set_index(\"timestamp\")\n",
    "#     df_2 = dfs[2].reset_index().merge(dfs[11].reset_index(drop=True),right_index=True,left_index=True,how=\"outer\").set_index(\"timestamp\")\n",
    "#     df_3 = dfs[10].reset_index().merge(dfs[12].reset_index(drop=True),right_index=True,left_index=True,how=\"outer\").set_index(\"timestamp\")\n",
    "#     df_4 = dfs[8].reset_index().merge(dfs[4].reset_index(drop=True),right_index=True,left_index=True,how=\"outer\").set_index(\"timestamp\")\n",
    "#     df_5 = dfs[5].reset_index().merge(dfs[6].reset_index(drop=True),right_index=True,left_index=True,how=\"outer\").merge(dfs[7].reset_index(drop=True),right_index=True,left_index=True,how=\"outer\").merge(dfs[9].reset_index(drop=True),right_index=True,left_index=True,how=\"outer\").set_index(\"timestamp\")\n",
    "#     df_6 = dfs[0]\n",
    "\n",
    "#     data_dict[key] = [df_1, df_2, df_3, df_4, df_5, df_6, dfs[13]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = [1323.0, 328.0, 270.0, 2471.0, 2531.0, 6619.0, 662.0]\n",
    "\n",
    "# total_length = sum(lengths)\n",
    "\n",
    "# weights = [length / total_length for length in lengths]\n",
    "\n",
    "# for i, weight in enumerate(weights):\n",
    "#     print(f\"Data {i+1} Weight: {weight:.4f}\")\n",
    "\n",
    "# for i, weight in enumerate(weights):\n",
    "#     window_size = int(weight * total_length / 10)  \n",
    "#     skip_size = int(window_size * 0.1)  \n",
    "#     print(f\"Data {i+1} - Window Size: {window_size}, Skip Size: {skip_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_1_engine_failure\n",
      "carbonZ_2018-07-18-15-53-31_2_engine_failure\n",
      "carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-16-39-00_1_engine_failure\n",
      "carbonZ_2018-07-30-16-39-00_2_engine_failure\n",
      "carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj\n",
      "carbonZ_2018-09-11-11-56-30_engine_failure\n",
      "carbonZ_2018-09-11-14-22-07_1_engine_failure\n",
      "carbonZ_2018-09-11-14-22-07_2_engine_failure\n",
      "carbonZ_2018-09-11-14-41-51_elevator_failure\n",
      "carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\n",
      "carbonZ_2018-09-11-15-05-11_1_elevator_failure\n",
      "carbonZ_2018-09-11-15-06-34_1_rudder_right_failure\n",
      "carbonZ_2018-09-11-15-06-34_2_rudder_right_failure\n",
      "carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure\n",
      "carbonZ_2018-09-11-17-55-30_1_right_aileron_failure\n",
      "carbonZ_2018-09-11-17-55-30_2_left_aileron_failure\n",
      "carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-14-37-22_2_right_aileron_failure\n",
      "carbonZ_2018-10-05-14-37-22_3_left_aileron_failure\n",
      "carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj\n",
      "carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\n",
      "carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "w_label = 66\n",
    "s_label = 6\n",
    "\n",
    "window_sizes = [661, 132, 32, 132, 247, 253, 253, 253, 247, 253, 27, 32, 27]\n",
    "step_sizes =   [66, 13, 3, 13, 24, 25, 25, 25, 24, 25, 2, 3, 2]\n",
    "\n",
    "x_arrays = [np.zeros((0, w, c)) for w, c in zip(window_sizes, [2, 1, 10, 1, 1, 4, 1, 1, 6, 1, 1, 6, 3])]\n",
    "y = []\n",
    "\n",
    "for key, data in df_dict.items():\n",
    "    print(key)\n",
    "    \n",
    "    scaled_data = [pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index).dropna() for df in data[:-1]]\n",
    "    labels_data = data[-1]\n",
    "\n",
    "    len_label = len(labels_data)\n",
    "    len_data = [len(df) for df in scaled_data]\n",
    "\n",
    "    while all(len_d - w >= 0 for len_d, w in zip(len_data, window_sizes)):\n",
    "        labels = labels_data[len_label - w_label : len_label].values\n",
    "        len_label -= s_label\n",
    "\n",
    "        slices = [df[len_d - w : len_d].values for df, len_d, w in zip(scaled_data, len_data, window_sizes)]\n",
    "        reshaped_slices = [\n",
    "            sl.reshape(-1, w, c) for sl, w, c in zip(slices, window_sizes, [2, 1, 10, 1, 1, 4, 1, 1, 6, 1, 1, 6, 3])\n",
    "        ]\n",
    "        len_data = [len_d - s for len_d, s in zip(len_data, step_sizes)]\n",
    "\n",
    "        if 2 in labels:\n",
    "            repeat_count = 4\n",
    "        elif 3 in labels:\n",
    "            repeat_count = 3\n",
    "        elif 4 in labels:\n",
    "            repeat_count = 5\n",
    "        else:\n",
    "            repeat_count = 1\n",
    "        \n",
    "        for _ in range(repeat_count):\n",
    "            for i, sl in enumerate(reshaped_slices):\n",
    "                x_arrays[i] = np.vstack((x_arrays[i], sl))\n",
    "            y.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3666, 661, 2),\n",
       " (3666, 132, 1),\n",
       " (3666, 32, 10),\n",
       " (3666, 132, 1),\n",
       " (3666, 247, 1),\n",
       " (3666, 253, 4),\n",
       " (3666, 253, 1),\n",
       " (3666, 253, 1),\n",
       " (3666, 247, 6),\n",
       " (3666, 253, 1),\n",
       " (3666, 27, 1),\n",
       " (3666, 32, 6),\n",
       " (3666, 27, 3)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes = [x.shape for x in x_arrays]\n",
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2548107])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_arrays[9][495][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3666"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3666, 1), (3666, 66, 1))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "y_ = np.array([max(m) for m in y])\n",
    "y_.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch shapes: torch.Size([32, 661, 2]) torch.Size([32, 132, 1]) torch.Size([32, 32, 10]) torch.Size([32, 132, 1]) torch.Size([32, 247, 1]) torch.Size([32, 253, 4]) torch.Size([32, 253, 1]) torch.Size([32, 253, 1]) torch.Size([32, 247, 6]) torch.Size([32, 253, 1]) torch.Size([32, 27, 1]) torch.Size([32, 32, 6]) torch.Size([32, 27, 3]) torch.Size([32])\n",
      "Test batch shapes: torch.Size([32, 661, 2]) torch.Size([32, 132, 1]) torch.Size([32, 32, 10]) torch.Size([32, 132, 1]) torch.Size([32, 247, 1]) torch.Size([32, 253, 4]) torch.Size([32, 253, 1]) torch.Size([32, 253, 1]) torch.Size([32, 247, 6]) torch.Size([32, 253, 1]) torch.Size([32, 27, 1]) torch.Size([32, 32, 6]) torch.Size([32, 27, 3]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_arrays, y):\n",
    "        self.x0 = torch.tensor(x_arrays[0], dtype=torch.float32)\n",
    "        self.x1 = torch.tensor(x_arrays[1], dtype=torch.float32)\n",
    "        self.x2 = torch.tensor(x_arrays[2], dtype=torch.float32)\n",
    "        self.x3 = torch.tensor(x_arrays[3], dtype=torch.float32)\n",
    "        self.x4 = torch.tensor(x_arrays[4], dtype=torch.float32)\n",
    "        self.x5 = torch.tensor(x_arrays[5], dtype=torch.float32)\n",
    "        self.x6 = torch.tensor(x_arrays[6], dtype=torch.float32)\n",
    "        self.x7 = torch.tensor(x_arrays[7], dtype=torch.float32)\n",
    "        self.x8 = torch.tensor(x_arrays[8], dtype=torch.float32)\n",
    "        self.x9 = torch.tensor(x_arrays[9], dtype=torch.float32)\n",
    "        self.x10 = torch.tensor(x_arrays[10], dtype=torch.float32)\n",
    "        self.x11 = torch.tensor(x_arrays[11], dtype=torch.float32)\n",
    "        self.x12 = torch.tensor(x_arrays[12], dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long).squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.x0[idx], self.x1[idx], self.x2[idx], self.x3[idx], self.x4[idx],\n",
    "            self.x5[idx], self.x6[idx], self.x7[idx], self.x8[idx], self.x9[idx],\n",
    "            self.x10[idx], self.x11[idx], self.x12[idx], self.y[idx]\n",
    "        )\n",
    "\n",
    "# Create the dataset\n",
    "dataset = CustomDataset(x_arrays, y_)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for train and test sets\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example usage\n",
    "for batch in train_dataloader:\n",
    "    (\n",
    "        x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \n",
    "        x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \n",
    "        x10_batch, x11_batch, x12_batch, y_batch\n",
    "    ) = batch\n",
    "    print(\"Train batch shapes:\", x0_batch.shape, x1_batch.shape, x2_batch.shape, x3_batch.shape, \n",
    "          x4_batch.shape, x5_batch.shape, x6_batch.shape, x7_batch.shape, x8_batch.shape, \n",
    "          x9_batch.shape, x10_batch.shape, x11_batch.shape, x12_batch.shape, y_batch.shape)\n",
    "    break\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    (\n",
    "        x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \n",
    "        x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \n",
    "        x10_batch, x11_batch, x12_batch, y_batch\n",
    "    ) = batch\n",
    "    print(\"Test batch shapes:\", x0_batch.shape, x1_batch.shape, x2_batch.shape, x3_batch.shape, \n",
    "          x4_batch.shape, x5_batch.shape, x6_batch.shape, x7_batch.shape, x8_batch.shape, \n",
    "          x9_batch.shape, x10_batch.shape, x11_batch.shape, x12_batch.shape, y_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm_layers): ModuleList(\n",
      "    (0): LSTM(2, 16, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "    (1): LSTM(1, 16, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "    (2): LSTM(10, 16, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "    (3-4): 2 x LSTM(1, 16, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "    (5): LSTM(4, 16, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "    (6-7): 2 x LSTM(1, 16, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "    (8): LSTM(6, 16, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "    (9-10): 2 x LSTM(1, 16, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "    (11): LSTM(6, 16, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "    (12): LSTM(3, 16, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=416, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=5, bias=True)\n",
      "    (3): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_shapes, hidden_sizes, num_stacked_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "\n",
    "        # Define LSTM layers for each input\n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            nn.LSTM(input_shapes[i][2], hidden_sizes[i], num_stacked_layers[i], batch_first=True, dropout=0.2, bidirectional=True)\n",
    "            for i in range(13)\n",
    "        ])\n",
    "\n",
    "        # Calculate the flattened size\n",
    "        self.flatten_size = self.get_flatten_size(input_shapes)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*self.flatten_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def get_flatten_size(self, input_shapes):\n",
    "        with torch.no_grad():\n",
    "            # Create dummy inputs\n",
    "            x = [torch.zeros(1, input_shapes[i][1], input_shapes[i][2]) for i in range(13)]\n",
    "            \n",
    "            # Pass through LSTM layers\n",
    "            outputs = [self.lstm_layers[i](x[i])[1][0][-1].view(1, -1) for i in range(13)]\n",
    "            \n",
    "            # Calculate the flattened size\n",
    "            flat_sizes = [output.shape[1] for output in outputs]\n",
    "            return sum(flat_sizes)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        # Initialize hidden and cell states for each LSTM layer\n",
    "        hidden_states = [\n",
    "            (torch.zeros(2*self.num_stacked_layers[i], inputs[i].size(0), self.hidden_sizes[i]).to(inputs[i].device),\n",
    "             torch.zeros(2*self.num_stacked_layers[i], inputs[i].size(0), self.hidden_sizes[i]).to(inputs[i].device))\n",
    "            for i in range(13)\n",
    "        ]\n",
    "\n",
    "        # Pass each input through its corresponding LSTM layer\n",
    "        lstm_outputs = [\n",
    "            self.lstm_layers[i](inputs[i], hidden_states[i])[0][:, -1, :] for i in range(13)\n",
    "        ]\n",
    "\n",
    "        # Concatenate the LSTM outputs\n",
    "        out = torch.cat(lstm_outputs, dim=1)\n",
    "\n",
    "        # Pass through the fully connected layers\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Example usage:\n",
    "hidden_sizes = [16 for i in range(13)]\n",
    "num_stacked_layers = [2 for i in range(13)]\n",
    "input_shapes = [\n",
    "    (3666, 661, 2),\n",
    "    (3666, 132, 1),\n",
    "    (3666, 32, 10),\n",
    "    (3666, 132, 1),\n",
    "    (3666, 247, 1),\n",
    "    (3666, 253, 4),\n",
    "    (3666, 253, 1),\n",
    "    (3666, 253, 1),\n",
    "    (3666, 247, 6),\n",
    "    (3666, 253, 1),\n",
    "    (3666, 27, 1),\n",
    "    (3666, 32, 6),\n",
    "    (3666, 27, 3)\n",
    "]\n",
    "num_classes = 5  \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMModel(input_shapes, hidden_sizes, num_stacked_layers, num_classes).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3382, 2.3500, 3.5941, 0.8313, 7.3320])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming y is a NumPy array or can be converted to one\n",
    "\n",
    "\n",
    "num_samples_per_class = [np.sum(y_ == i) for i in range(5)]\n",
    "\n",
    "total_samples = sum(num_samples_per_class)\n",
    "class_weights = [total_samples / (5 * num_samples) for num_samples in num_samples_per_class]\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    def __init__(self, filepath, monitor='val_loss', mode='min'):\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.best_state_dict = None\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.best_score = float('inf')\n",
    "        else:\n",
    "            self.best_score = float('-inf')\n",
    "\n",
    "    def __call__(self, score, model_state_dict):\n",
    "        if (self.mode == 'min' and score < self.best_score) or (self.mode == 'max' and score > self.best_score):\n",
    "            print(f\"Saving model with {self.monitor}: {score}\")\n",
    "            self.best_score = score\n",
    "            self.best_state_dict = model_state_dict\n",
    "            torch.save(model_state_dict, self.filepath)\n",
    "\n",
    "    def get_best_state_dict(self):\n",
    "        return self.best_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights)  # Assuming you're using CrossEntropyLoss for classification\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\",patience=30,factor=0.1)\n",
    "# # scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[150,200], gamma=0.5)\n",
    "# checkpoint = ModelCheckpoint(filepath='best_model.pth', monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights).to(device)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\",patience=5,factor=0.1)\n",
    "checkpoint = ModelCheckpoint(filepath='best_model.pth', monitor='val_loss', mode='min')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Training Loss: 1.6077211618423461, Training Accuracy: 15.0%\n",
      "Epoch 1, Batch 20, Training Loss: 1.6064767479896545, Training Accuracy: 10.15625%\n",
      "Epoch 1, Batch 30, Training Loss: 1.6028569380442301, Training Accuracy: 12.291666666666666%\n",
      "Epoch 1, Batch 40, Training Loss: 1.5996460050344468, Training Accuracy: 21.796875%\n",
      "Epoch 1, Batch 50, Training Loss: 1.5917346143722535, Training Accuracy: 31.8125%\n",
      "Epoch 1, Batch 60, Training Loss: 1.5770754357179007, Training Accuracy: 38.90625%\n",
      "Epoch 1, Batch 70, Training Loss: 1.5495747481073652, Training Accuracy: 44.86607142857143%\n",
      "Epoch 1, Batch 80, Training Loss: 1.521700131893158, Training Accuracy: 48.203125%\n",
      "Epoch 1, Batch 90, Training Loss: 1.492987589041392, Training Accuracy: 51.00694444444444%\n",
      "Epoch 1, Validation Loss: 1.225063101105068, Validation Accuracy: 77.65667574931881%\n",
      "Epoch 1: Adam lr 0.0010\n",
      "Epoch 2, Batch 10, Training Loss: 1.2580153107643128, Training Accuracy: 71.875%\n",
      "Epoch 2, Batch 20, Training Loss: 1.2135421693325044, Training Accuracy: 74.84375%\n",
      "Epoch 2, Batch 30, Training Loss: 1.1956514398256937, Training Accuracy: 75.625%\n",
      "Epoch 2, Batch 40, Training Loss: 1.1719797194004058, Training Accuracy: 76.5625%\n",
      "Epoch 2, Batch 50, Training Loss: 1.159996509552002, Training Accuracy: 77.0%\n",
      "Epoch 2, Batch 60, Training Loss: 1.150452689329783, Training Accuracy: 77.1875%\n",
      "Epoch 2, Batch 70, Training Loss: 1.1409979045391083, Training Accuracy: 77.41071428571429%\n",
      "Epoch 2, Batch 80, Training Loss: 1.133595486730337, Training Accuracy: 78.046875%\n",
      "Epoch 2, Batch 90, Training Loss: 1.1277677509519788, Training Accuracy: 78.40277777777777%\n",
      "Epoch 2, Validation Loss: 1.0625533435655676, Validation Accuracy: 81.60762942779292%\n",
      "Epoch 2: Adam lr 0.0010\n",
      "Epoch 3, Batch 10, Training Loss: 1.087596070766449, Training Accuracy: 79.0625%\n",
      "Epoch 3, Batch 20, Training Loss: 1.066153821349144, Training Accuracy: 79.0625%\n",
      "Epoch 3, Batch 30, Training Loss: 1.0528972188631693, Training Accuracy: 80.72916666666666%\n",
      "Epoch 3, Batch 40, Training Loss: 1.0538551807403564, Training Accuracy: 80.546875%\n",
      "Epoch 3, Batch 50, Training Loss: 1.047597243785858, Training Accuracy: 81.375%\n",
      "Epoch 3, Batch 60, Training Loss: 1.0445903927087783, Training Accuracy: 81.66666666666667%\n",
      "Epoch 3, Batch 70, Training Loss: 1.0367900090558189, Training Accuracy: 82.85714285714286%\n",
      "Epoch 3, Batch 80, Training Loss: 1.0355901814997197, Training Accuracy: 82.96875%\n",
      "Epoch 3, Batch 90, Training Loss: 1.0314971413877276, Training Accuracy: 83.29861111111111%\n",
      "Epoch 3, Validation Loss: 0.9909579002338907, Validation Accuracy: 88.5558583106267%\n",
      "Epoch 3: Adam lr 0.0010\n",
      "Epoch 4, Batch 10, Training Loss: 1.0014655590057373, Training Accuracy: 85.3125%\n",
      "Epoch 4, Batch 20, Training Loss: 1.0096249431371689, Training Accuracy: 85.15625%\n",
      "Epoch 4, Batch 30, Training Loss: 1.0084759751955668, Training Accuracy: 85.625%\n",
      "Epoch 4, Batch 40, Training Loss: 1.0069936648011208, Training Accuracy: 86.09375%\n",
      "Epoch 4, Batch 50, Training Loss: 1.0061269676685334, Training Accuracy: 85.6875%\n",
      "Epoch 4, Batch 60, Training Loss: 1.003423238794009, Training Accuracy: 86.19791666666666%\n",
      "Epoch 4, Batch 70, Training Loss: 1.0051543082509722, Training Accuracy: 85.98214285714286%\n",
      "Epoch 4, Batch 80, Training Loss: 1.0020444884896278, Training Accuracy: 86.2109375%\n",
      "Epoch 4, Batch 90, Training Loss: 0.9991241667005751, Training Accuracy: 86.49305555555556%\n",
      "Epoch 4, Validation Loss: 0.9826649893885073, Validation Accuracy: 90.59945504087193%\n",
      "Epoch 4: Adam lr 0.0010\n",
      "Epoch 5, Batch 10, Training Loss: 0.9854919016361237, Training Accuracy: 88.75%\n",
      "Epoch 5, Batch 20, Training Loss: 0.9711417615413666, Training Accuracy: 90.9375%\n",
      "Epoch 5, Batch 30, Training Loss: 0.9762461304664611, Training Accuracy: 90.41666666666667%\n",
      "Epoch 5, Batch 40, Training Loss: 0.9773459926247596, Training Accuracy: 90.3125%\n",
      "Epoch 5, Batch 50, Training Loss: 0.9737860715389252, Training Accuracy: 90.875%\n",
      "Epoch 5, Batch 60, Training Loss: 0.9798396279414495, Training Accuracy: 90.20833333333333%\n",
      "Epoch 5, Batch 70, Training Loss: 0.9779605593000139, Training Accuracy: 90.44642857142857%\n",
      "Epoch 5, Batch 80, Training Loss: 0.9767261117696762, Training Accuracy: 90.5078125%\n",
      "Epoch 5, Batch 90, Training Loss: 0.9754402750068241, Training Accuracy: 90.65972222222223%\n",
      "Epoch 5, Validation Loss: 0.958888494450113, Validation Accuracy: 91.68937329700273%\n",
      "Epoch 5: Adam lr 0.0010\n",
      "Epoch 6, Batch 10, Training Loss: 0.9598707795143128, Training Accuracy: 90.3125%\n",
      "Epoch 6, Batch 20, Training Loss: 0.9618840396404267, Training Accuracy: 91.25%\n",
      "Epoch 6, Batch 30, Training Loss: 0.9693987568219503, Training Accuracy: 90.9375%\n",
      "Epoch 6, Batch 40, Training Loss: 0.9709077998995781, Training Accuracy: 90.78125%\n",
      "Epoch 6, Batch 50, Training Loss: 0.9680763471126557, Training Accuracy: 90.9375%\n",
      "Epoch 6, Batch 60, Training Loss: 0.967732189098994, Training Accuracy: 91.14583333333334%\n",
      "Epoch 6, Batch 70, Training Loss: 0.9654145215238844, Training Accuracy: 91.25%\n",
      "Epoch 6, Batch 80, Training Loss: 0.96455899477005, Training Accuracy: 91.484375%\n",
      "Epoch 6, Batch 90, Training Loss: 0.9630832533041637, Training Accuracy: 91.66666666666666%\n",
      "Epoch 6, Validation Loss: 0.9564438799153203, Validation Accuracy: 92.91553133514986%\n",
      "Epoch 6: Adam lr 0.0010\n",
      "Epoch 7, Batch 10, Training Loss: 0.966194236278534, Training Accuracy: 91.25%\n",
      "Epoch 7, Batch 20, Training Loss: 0.9604551136493683, Training Accuracy: 92.34375%\n",
      "Epoch 7, Batch 30, Training Loss: 0.9680137515068055, Training Accuracy: 91.35416666666667%\n",
      "Epoch 7, Batch 40, Training Loss: 0.96517184227705, Training Accuracy: 91.953125%\n",
      "Epoch 7, Batch 50, Training Loss: 0.9664458286762238, Training Accuracy: 91.875%\n",
      "Epoch 7, Batch 60, Training Loss: 0.965232077240944, Training Accuracy: 91.97916666666667%\n",
      "Epoch 7, Batch 70, Training Loss: 0.9631505378655025, Training Accuracy: 92.27678571428571%\n",
      "Epoch 7, Batch 80, Training Loss: 0.963141668587923, Training Accuracy: 92.34375%\n",
      "Epoch 7, Batch 90, Training Loss: 0.9625440021355947, Training Accuracy: 92.46527777777777%\n",
      "Epoch 7, Validation Loss: 0.9533928425415702, Validation Accuracy: 93.73297002724796%\n",
      "Epoch 7: Adam lr 0.0010\n",
      "Epoch 8, Batch 10, Training Loss: 0.9455484986305237, Training Accuracy: 94.375%\n",
      "Epoch 8, Batch 20, Training Loss: 0.9547874450683593, Training Accuracy: 92.65625%\n",
      "Epoch 8, Batch 30, Training Loss: 0.9563751955827077, Training Accuracy: 92.5%\n",
      "Epoch 8, Batch 40, Training Loss: 0.960110193490982, Training Accuracy: 92.421875%\n",
      "Epoch 8, Batch 50, Training Loss: 0.9631023526191711, Training Accuracy: 92.0625%\n",
      "Epoch 8, Batch 60, Training Loss: 0.9619049280881882, Training Accuracy: 92.1875%\n",
      "Epoch 8, Batch 70, Training Loss: 0.9612005242279598, Training Accuracy: 92.36607142857143%\n",
      "Epoch 8, Batch 80, Training Loss: 0.9603584356606006, Training Accuracy: 92.5%\n",
      "Epoch 8, Batch 90, Training Loss: 0.9602627919779884, Training Accuracy: 92.74305555555556%\n",
      "Epoch 8, Validation Loss: 0.9517943962760593, Validation Accuracy: 94.141689373297%\n",
      "Epoch 8: Adam lr 0.0010\n",
      "Epoch 9, Batch 10, Training Loss: 0.9691077053546906, Training Accuracy: 91.875%\n",
      "Epoch 9, Batch 20, Training Loss: 0.9590470194816589, Training Accuracy: 91.875%\n",
      "Epoch 9, Batch 30, Training Loss: 0.9593669195969899, Training Accuracy: 92.29166666666667%\n",
      "Epoch 9, Batch 40, Training Loss: 0.9554483011364937, Training Accuracy: 92.5%\n",
      "Epoch 9, Batch 50, Training Loss: 0.9546170544624328, Training Accuracy: 92.5%\n",
      "Epoch 9, Batch 60, Training Loss: 0.9536379188299179, Training Accuracy: 92.8125%\n",
      "Epoch 9, Batch 70, Training Loss: 0.9532986104488372, Training Accuracy: 93.21428571428572%\n",
      "Epoch 9, Batch 80, Training Loss: 0.9537040285766125, Training Accuracy: 93.125%\n",
      "Epoch 9, Batch 90, Training Loss: 0.957768279976315, Training Accuracy: 93.02083333333333%\n",
      "Epoch 9, Validation Loss: 0.9486534828725068, Validation Accuracy: 94.00544959128065%\n",
      "Epoch 9: Adam lr 0.0010\n",
      "Epoch 10, Batch 10, Training Loss: 0.9396088361740113, Training Accuracy: 94.0625%\n",
      "Epoch 10, Batch 20, Training Loss: 0.9440892547369003, Training Accuracy: 94.21875%\n",
      "Epoch 10, Batch 30, Training Loss: 0.9478803833325704, Training Accuracy: 93.95833333333333%\n",
      "Epoch 10, Batch 40, Training Loss: 0.9503631845116616, Training Accuracy: 93.75%\n",
      "Epoch 10, Batch 50, Training Loss: 0.9540684580802917, Training Accuracy: 93.4375%\n",
      "Epoch 10, Batch 60, Training Loss: 0.9527963121732076, Training Accuracy: 93.80208333333333%\n",
      "Epoch 10, Batch 70, Training Loss: 0.9500897288322449, Training Accuracy: 94.10714285714286%\n",
      "Epoch 10, Batch 80, Training Loss: 0.9519798584282398, Training Accuracy: 93.984375%\n",
      "Epoch 10, Batch 90, Training Loss: 0.9514030310842726, Training Accuracy: 93.85416666666667%\n",
      "Epoch 10, Validation Loss: 0.9449883984482806, Validation Accuracy: 93.8692098092643%\n",
      "Epoch 10: Adam lr 0.0010\n",
      "Epoch 11, Batch 10, Training Loss: 0.9419082880020142, Training Accuracy: 95.0%\n",
      "Epoch 11, Batch 20, Training Loss: 0.9484729886054992, Training Accuracy: 93.75%\n",
      "Epoch 11, Batch 30, Training Loss: 0.954057099421819, Training Accuracy: 93.125%\n",
      "Epoch 11, Batch 40, Training Loss: 0.9489533394575119, Training Accuracy: 94.140625%\n",
      "Epoch 11, Batch 50, Training Loss: 0.9480528271198273, Training Accuracy: 94.1875%\n",
      "Epoch 11, Batch 60, Training Loss: 0.9467614899079005, Training Accuracy: 94.375%\n",
      "Epoch 11, Batch 70, Training Loss: 0.9466809519699642, Training Accuracy: 94.375%\n",
      "Epoch 11, Batch 80, Training Loss: 0.9473804205656051, Training Accuracy: 94.3359375%\n",
      "Epoch 11, Batch 90, Training Loss: 0.9466980338096619, Training Accuracy: 94.34027777777779%\n",
      "Epoch 11, Validation Loss: 0.9415135953737341, Validation Accuracy: 94.6866485013624%\n",
      "Epoch 11: Adam lr 0.0010\n",
      "Epoch 12, Batch 10, Training Loss: 0.9583728313446045, Training Accuracy: 92.5%\n",
      "Epoch 12, Batch 20, Training Loss: 0.9489592045545578, Training Accuracy: 94.375%\n",
      "Epoch 12, Batch 30, Training Loss: 0.9427825431029002, Training Accuracy: 95.0%\n",
      "Epoch 12, Batch 40, Training Loss: 0.9406354755163193, Training Accuracy: 94.921875%\n",
      "Epoch 12, Batch 50, Training Loss: 0.9380268919467926, Training Accuracy: 94.9375%\n",
      "Epoch 12, Batch 60, Training Loss: 0.9378294934829076, Training Accuracy: 95.26041666666667%\n",
      "Epoch 12, Batch 70, Training Loss: 0.9411431516919817, Training Accuracy: 94.86607142857143%\n",
      "Epoch 12, Batch 80, Training Loss: 0.9405997671186924, Training Accuracy: 95.0%\n",
      "Epoch 12, Batch 90, Training Loss: 0.9441410826312171, Training Accuracy: 94.58333333333333%\n",
      "Epoch 12, Validation Loss: 0.942568328069604, Validation Accuracy: 95.2316076294278%\n",
      "Epoch 12: Adam lr 0.0010\n",
      "Epoch 13, Batch 10, Training Loss: 0.9411696493625641, Training Accuracy: 95.3125%\n",
      "Epoch 13, Batch 20, Training Loss: 0.9487547874450684, Training Accuracy: 95.0%\n",
      "Epoch 13, Batch 30, Training Loss: 0.9521270493666331, Training Accuracy: 94.27083333333334%\n",
      "Epoch 13, Batch 40, Training Loss: 0.9485910966992378, Training Accuracy: 94.6875%\n",
      "Epoch 13, Batch 50, Training Loss: 0.9462705719470977, Training Accuracy: 94.5625%\n",
      "Epoch 13, Batch 60, Training Loss: 0.944139900803566, Training Accuracy: 94.73958333333333%\n",
      "Epoch 13, Batch 70, Training Loss: 0.9433594388621194, Training Accuracy: 94.77678571428572%\n",
      "Epoch 13, Batch 80, Training Loss: 0.9425800710916519, Training Accuracy: 94.84375%\n",
      "Epoch 13, Batch 90, Training Loss: 0.9459985978073544, Training Accuracy: 94.54861111111111%\n",
      "Epoch 13, Validation Loss: 0.9421003460884094, Validation Accuracy: 94.95912806539509%\n",
      "Epoch 13: Adam lr 0.0010\n",
      "Epoch 14, Batch 10, Training Loss: 0.9404734671115875, Training Accuracy: 96.25%\n",
      "Epoch 14, Batch 20, Training Loss: 0.9462098062038422, Training Accuracy: 95.0%\n",
      "Epoch 14, Batch 30, Training Loss: 0.9500239034493764, Training Accuracy: 94.27083333333334%\n",
      "Epoch 14, Batch 40, Training Loss: 0.9458071768283844, Training Accuracy: 94.453125%\n",
      "Epoch 14, Batch 50, Training Loss: 0.9470094978809357, Training Accuracy: 94.375%\n",
      "Epoch 14, Batch 60, Training Loss: 0.943010633190473, Training Accuracy: 94.63541666666667%\n",
      "Epoch 14, Batch 70, Training Loss: 0.9459530907017845, Training Accuracy: 94.46428571428571%\n",
      "Epoch 14, Batch 80, Training Loss: 0.9455465711653233, Training Accuracy: 94.53125%\n",
      "Epoch 14, Batch 90, Training Loss: 0.9458021183808645, Training Accuracy: 94.6875%\n",
      "Epoch 14, Validation Loss: 0.9377647094104601, Validation Accuracy: 95.50408719346049%\n",
      "Epoch 14: Adam lr 0.0010\n",
      "Epoch 15, Batch 10, Training Loss: 0.9551951229572296, Training Accuracy: 93.4375%\n",
      "Epoch 15, Batch 20, Training Loss: 0.9433266997337342, Training Accuracy: 94.53125%\n",
      "Epoch 15, Batch 30, Training Loss: 0.9376369019349416, Training Accuracy: 95.3125%\n",
      "Epoch 15, Batch 40, Training Loss: 0.9365581303834916, Training Accuracy: 95.3125%\n",
      "Epoch 15, Batch 50, Training Loss: 0.9390601110458374, Training Accuracy: 95.1875%\n",
      "Epoch 15, Batch 60, Training Loss: 0.9406824340422948, Training Accuracy: 94.89583333333333%\n",
      "Epoch 15, Batch 70, Training Loss: 0.9404329027448382, Training Accuracy: 94.73214285714285%\n",
      "Epoch 15, Batch 80, Training Loss: 0.9399555057287217, Training Accuracy: 95.078125%\n",
      "Epoch 15, Batch 90, Training Loss: 0.9411473333835602, Training Accuracy: 95.0%\n",
      "Epoch 15, Validation Loss: 0.9359422481578329, Validation Accuracy: 95.50408719346049%\n",
      "Epoch 15: Adam lr 0.0010\n",
      "Epoch 16, Batch 10, Training Loss: 0.937858122587204, Training Accuracy: 94.375%\n",
      "Epoch 16, Batch 20, Training Loss: 0.9399712979793549, Training Accuracy: 95.0%\n",
      "Epoch 16, Batch 30, Training Loss: 0.9391014496485393, Training Accuracy: 94.79166666666666%\n",
      "Epoch 16, Batch 40, Training Loss: 0.9394854485988617, Training Accuracy: 95.0%\n",
      "Epoch 16, Batch 50, Training Loss: 0.9406138217449188, Training Accuracy: 94.9375%\n",
      "Epoch 16, Batch 60, Training Loss: 0.9403945068518321, Training Accuracy: 95.0%\n",
      "Epoch 16, Batch 70, Training Loss: 0.9408837335450309, Training Accuracy: 95.08928571428571%\n",
      "Epoch 16, Batch 80, Training Loss: 0.9413653627038002, Training Accuracy: 95.1953125%\n",
      "Epoch 16, Batch 90, Training Loss: 0.9400026811493768, Training Accuracy: 95.17361111111111%\n",
      "Epoch 16, Validation Loss: 0.9367102565972701, Validation Accuracy: 95.77656675749319%\n",
      "Epoch 16: Adam lr 0.0010\n",
      "Epoch 17, Batch 10, Training Loss: 0.9372940003871918, Training Accuracy: 95.3125%\n",
      "Epoch 17, Batch 20, Training Loss: 0.9405981689691544, Training Accuracy: 95.46875%\n",
      "Epoch 17, Batch 30, Training Loss: 0.9353197793165843, Training Accuracy: 95.83333333333334%\n",
      "Epoch 17, Batch 40, Training Loss: 0.9319647327065468, Training Accuracy: 96.015625%\n",
      "Epoch 17, Batch 50, Training Loss: 0.9371945798397064, Training Accuracy: 95.5625%\n",
      "Epoch 17, Batch 60, Training Loss: 0.937999830643336, Training Accuracy: 95.41666666666667%\n",
      "Epoch 17, Batch 70, Training Loss: 0.9372898144381386, Training Accuracy: 95.40178571428571%\n",
      "Epoch 17, Batch 80, Training Loss: 0.9357023142278195, Training Accuracy: 95.703125%\n",
      "Epoch 17, Batch 90, Training Loss: 0.9358202172650232, Training Accuracy: 95.83333333333334%\n",
      "Epoch 17, Validation Loss: 0.9350027130997699, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 17: Adam lr 0.0010\n",
      "Epoch 18, Batch 10, Training Loss: 0.9379651784896851, Training Accuracy: 95.625%\n",
      "Epoch 18, Batch 20, Training Loss: 0.9319029152393341, Training Accuracy: 96.09375%\n",
      "Epoch 18, Batch 30, Training Loss: 0.9314367810885111, Training Accuracy: 95.9375%\n",
      "Epoch 18, Batch 40, Training Loss: 0.9366507813334465, Training Accuracy: 95.703125%\n",
      "Epoch 18, Batch 50, Training Loss: 0.9345351433753968, Training Accuracy: 95.9375%\n",
      "Epoch 18, Batch 60, Training Loss: 0.9335093061129253, Training Accuracy: 96.09375%\n",
      "Epoch 18, Batch 70, Training Loss: 0.933861357825143, Training Accuracy: 96.11607142857143%\n",
      "Epoch 18, Batch 80, Training Loss: 0.9334388814866543, Training Accuracy: 96.1328125%\n",
      "Epoch 18, Batch 90, Training Loss: 0.9379400948683421, Training Accuracy: 95.76388888888889%\n",
      "Epoch 18, Validation Loss: 0.9345660624296769, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 18: Adam lr 0.0010\n",
      "Epoch 19, Batch 10, Training Loss: 0.9316656172275544, Training Accuracy: 96.25%\n",
      "Epoch 19, Batch 20, Training Loss: 0.9401534020900726, Training Accuracy: 95.78125%\n",
      "Epoch 19, Batch 30, Training Loss: 0.9388306597868602, Training Accuracy: 95.9375%\n",
      "Epoch 19, Batch 40, Training Loss: 0.934199896454811, Training Accuracy: 96.25%\n",
      "Epoch 19, Batch 50, Training Loss: 0.9346363341808319, Training Accuracy: 96.3125%\n",
      "Epoch 19, Batch 60, Training Loss: 0.9354593276977539, Training Accuracy: 96.25%\n",
      "Epoch 19, Batch 70, Training Loss: 0.9367594701903207, Training Accuracy: 96.02678571428571%\n",
      "Epoch 19, Batch 80, Training Loss: 0.9360646195709705, Training Accuracy: 96.0546875%\n",
      "Epoch 19, Batch 90, Training Loss: 0.9353400707244873, Training Accuracy: 95.97222222222223%\n",
      "Epoch 19, Validation Loss: 0.931517217470252, Validation Accuracy: 96.32152588555857%\n",
      "Epoch 19: Adam lr 0.0010\n",
      "Epoch 20, Batch 10, Training Loss: 0.940869289636612, Training Accuracy: 96.25%\n",
      "Epoch 20, Batch 20, Training Loss: 0.9374496221542359, Training Accuracy: 95.9375%\n",
      "Epoch 20, Batch 30, Training Loss: 0.934662954012553, Training Accuracy: 96.45833333333333%\n",
      "Epoch 20, Batch 40, Training Loss: 0.9329785153269767, Training Accuracy: 96.5625%\n",
      "Epoch 20, Batch 50, Training Loss: 0.9333224558830261, Training Accuracy: 96.375%\n",
      "Epoch 20, Batch 60, Training Loss: 0.9313220351934433, Training Accuracy: 96.5625%\n",
      "Epoch 20, Batch 70, Training Loss: 0.9311144760676793, Training Accuracy: 96.42857142857143%\n",
      "Epoch 20, Batch 80, Training Loss: 0.9333758555352688, Training Accuracy: 96.2109375%\n",
      "Epoch 20, Batch 90, Training Loss: 0.9357399211989509, Training Accuracy: 95.76388888888889%\n",
      "Epoch 20, Validation Loss: 0.9314006774321847, Validation Accuracy: 96.32152588555857%\n",
      "Epoch 20: Adam lr 0.0010\n",
      "Epoch 21, Batch 10, Training Loss: 0.9304372370243073, Training Accuracy: 95.9375%\n",
      "Epoch 21, Batch 20, Training Loss: 0.9349233925342559, Training Accuracy: 95.78125%\n",
      "Epoch 21, Batch 30, Training Loss: 0.9393833001454671, Training Accuracy: 95.10416666666667%\n",
      "Epoch 21, Batch 40, Training Loss: 0.9360295712947846, Training Accuracy: 95.3125%\n",
      "Epoch 21, Batch 50, Training Loss: 0.9339851415157319, Training Accuracy: 95.625%\n",
      "Epoch 21, Batch 60, Training Loss: 0.9366421471039454, Training Accuracy: 95.20833333333333%\n",
      "Epoch 21, Batch 70, Training Loss: 0.9377211374895913, Training Accuracy: 95.13392857142857%\n",
      "Epoch 21, Batch 80, Training Loss: 0.9375500351190567, Training Accuracy: 95.2734375%\n",
      "Epoch 21, Batch 90, Training Loss: 0.9384244932068719, Training Accuracy: 95.20833333333333%\n",
      "Epoch 21, Validation Loss: 0.9322137625321097, Validation Accuracy: 96.32152588555857%\n",
      "Epoch 21: Adam lr 0.0010\n",
      "Epoch 22, Batch 10, Training Loss: 0.9276344656944275, Training Accuracy: 95.625%\n",
      "Epoch 22, Batch 20, Training Loss: 0.9369925439357758, Training Accuracy: 95.3125%\n",
      "Epoch 22, Batch 30, Training Loss: 0.9361139754454295, Training Accuracy: 95.83333333333334%\n",
      "Epoch 22, Batch 40, Training Loss: 0.9362983107566833, Training Accuracy: 96.09375%\n",
      "Epoch 22, Batch 50, Training Loss: 0.9381411159038544, Training Accuracy: 95.8125%\n",
      "Epoch 22, Batch 60, Training Loss: 0.9394249528646469, Training Accuracy: 95.72916666666667%\n",
      "Epoch 22, Batch 70, Training Loss: 0.9376452990940639, Training Accuracy: 95.80357142857143%\n",
      "Epoch 22, Batch 80, Training Loss: 0.9355226784944535, Training Accuracy: 95.8984375%\n",
      "Epoch 22, Batch 90, Training Loss: 0.9379323482513428, Training Accuracy: 95.79861111111111%\n",
      "Epoch 22, Validation Loss: 0.9464400021926217, Validation Accuracy: 94.27792915531336%\n",
      "Epoch 22: Adam lr 0.0010\n",
      "Epoch 23, Batch 10, Training Loss: 0.9476460158824921, Training Accuracy: 93.4375%\n",
      "Epoch 23, Batch 20, Training Loss: 0.9447189152240754, Training Accuracy: 94.0625%\n",
      "Epoch 23, Batch 30, Training Loss: 0.9449558476607005, Training Accuracy: 94.6875%\n",
      "Epoch 23, Batch 40, Training Loss: 0.9453320682048798, Training Accuracy: 94.375%\n",
      "Epoch 23, Batch 50, Training Loss: 0.9469339156150818, Training Accuracy: 94.375%\n",
      "Epoch 23, Batch 60, Training Loss: 0.9484083165725072, Training Accuracy: 94.53125%\n",
      "Epoch 23, Batch 70, Training Loss: 0.9465088060923985, Training Accuracy: 94.77678571428572%\n",
      "Epoch 23, Batch 80, Training Loss: 0.9460449285805226, Training Accuracy: 94.84375%\n",
      "Epoch 23, Batch 90, Training Loss: 0.9459993879000346, Training Accuracy: 94.82638888888889%\n",
      "Epoch 23, Validation Loss: 0.9382946102515511, Validation Accuracy: 95.77656675749319%\n",
      "Epoch 23: Adam lr 0.0010\n",
      "Epoch 24, Batch 10, Training Loss: 0.9295464992523194, Training Accuracy: 95.9375%\n",
      "Epoch 24, Batch 20, Training Loss: 0.932658776640892, Training Accuracy: 96.09375%\n",
      "Epoch 24, Batch 30, Training Loss: 0.9315935949484507, Training Accuracy: 96.04166666666667%\n",
      "Epoch 24, Batch 40, Training Loss: 0.9351978018879891, Training Accuracy: 95.703125%\n",
      "Epoch 24, Batch 50, Training Loss: 0.9360402488708496, Training Accuracy: 95.625%\n",
      "Epoch 24, Batch 60, Training Loss: 0.9379363089799881, Training Accuracy: 95.46875%\n",
      "Epoch 24, Batch 70, Training Loss: 0.9381720585482461, Training Accuracy: 95.49107142857143%\n",
      "Epoch 24, Batch 80, Training Loss: 0.9394932501018047, Training Accuracy: 95.1953125%\n",
      "Epoch 24, Batch 90, Training Loss: 0.9412870056099362, Training Accuracy: 95.10416666666667%\n",
      "Epoch 24, Validation Loss: 0.934690913428431, Validation Accuracy: 95.91280653950953%\n",
      "Epoch 24: Adam lr 0.0010\n",
      "Epoch 25, Batch 10, Training Loss: 0.9380130290985107, Training Accuracy: 94.0625%\n",
      "Epoch 25, Batch 20, Training Loss: 0.9331678330898285, Training Accuracy: 95.0%\n",
      "Epoch 25, Batch 30, Training Loss: 0.9336362063884736, Training Accuracy: 95.52083333333333%\n",
      "Epoch 25, Batch 40, Training Loss: 0.9408504515886307, Training Accuracy: 94.921875%\n",
      "Epoch 25, Batch 50, Training Loss: 0.9386468279361725, Training Accuracy: 95.25%\n",
      "Epoch 25, Batch 60, Training Loss: 0.940121145049731, Training Accuracy: 94.94791666666667%\n",
      "Epoch 25, Batch 70, Training Loss: 0.9410072846072061, Training Accuracy: 94.82142857142857%\n",
      "Epoch 25, Batch 80, Training Loss: 0.9409605242311955, Training Accuracy: 94.8828125%\n",
      "Epoch 25, Batch 90, Training Loss: 0.9398164802127414, Training Accuracy: 95.10416666666667%\n",
      "Epoch 25, Validation Loss: 0.9554456938867983, Validation Accuracy: 95.36784741144415%\n",
      "Epoch 25: Adam lr 0.0010\n",
      "Epoch 26, Batch 10, Training Loss: 0.94287468791008, Training Accuracy: 95.625%\n",
      "Epoch 26, Batch 20, Training Loss: 0.9373839676380158, Training Accuracy: 95.9375%\n",
      "Epoch 26, Batch 30, Training Loss: 0.940174917380015, Training Accuracy: 95.0%\n",
      "Epoch 26, Batch 40, Training Loss: 0.9426554813981056, Training Accuracy: 95.390625%\n",
      "Epoch 26, Batch 50, Training Loss: 0.9419101202487945, Training Accuracy: 95.3125%\n",
      "Epoch 26, Batch 60, Training Loss: 0.9410956859588623, Training Accuracy: 95.36458333333333%\n",
      "Epoch 26, Batch 70, Training Loss: 0.9395967108862741, Training Accuracy: 95.44642857142858%\n",
      "Epoch 26, Batch 80, Training Loss: 0.9422017529606819, Training Accuracy: 95.1171875%\n",
      "Epoch 26, Batch 90, Training Loss: 0.9409925758838653, Training Accuracy: 95.27777777777777%\n",
      "Epoch 26, Validation Loss: 0.9344170767328014, Validation Accuracy: 96.32152588555857%\n",
      "Epoch 26: Adam lr 0.0001\n",
      "Epoch 27, Batch 10, Training Loss: 0.9314023733139039, Training Accuracy: 96.25%\n",
      "Epoch 27, Batch 20, Training Loss: 0.9418755739927291, Training Accuracy: 95.15625%\n",
      "Epoch 27, Batch 30, Training Loss: 0.9363826890786489, Training Accuracy: 96.25%\n",
      "Epoch 27, Batch 40, Training Loss: 0.9358675450086593, Training Accuracy: 96.015625%\n",
      "Epoch 27, Batch 50, Training Loss: 0.934215031862259, Training Accuracy: 96.25%\n",
      "Epoch 27, Batch 60, Training Loss: 0.9362789879242579, Training Accuracy: 95.83333333333334%\n",
      "Epoch 27, Batch 70, Training Loss: 0.9349504683698927, Training Accuracy: 95.9375%\n",
      "Epoch 27, Batch 80, Training Loss: 0.934082044661045, Training Accuracy: 96.0546875%\n",
      "Epoch 27, Batch 90, Training Loss: 0.9354454769028557, Training Accuracy: 96.04166666666667%\n",
      "Epoch 27, Validation Loss: 0.9332156155420386, Validation Accuracy: 96.32152588555857%\n",
      "Epoch 27: Adam lr 0.0001\n",
      "Epoch 28, Batch 10, Training Loss: 0.9278114080429077, Training Accuracy: 96.875%\n",
      "Epoch 28, Batch 20, Training Loss: 0.9360820859670639, Training Accuracy: 95.78125%\n",
      "Epoch 28, Batch 30, Training Loss: 0.931290207306544, Training Accuracy: 96.35416666666666%\n",
      "Epoch 28, Batch 40, Training Loss: 0.9309313073754311, Training Accuracy: 96.171875%\n",
      "Epoch 28, Batch 50, Training Loss: 0.9307680344581604, Training Accuracy: 96.25%\n",
      "Epoch 28, Batch 60, Training Loss: 0.9301419119040172, Training Accuracy: 96.35416666666666%\n",
      "Epoch 28, Batch 70, Training Loss: 0.9319976261683873, Training Accuracy: 96.25%\n",
      "Epoch 28, Batch 80, Training Loss: 0.9326867498457432, Training Accuracy: 96.1328125%\n",
      "Epoch 28, Batch 90, Training Loss: 0.9331510676278009, Training Accuracy: 96.11111111111111%\n",
      "Epoch 28, Validation Loss: 0.9329836213070414, Validation Accuracy: 96.32152588555857%\n",
      "Epoch 28: Adam lr 0.0001\n",
      "Epoch 29, Batch 10, Training Loss: 0.9364022672176361, Training Accuracy: 96.25%\n",
      "Epoch 29, Batch 20, Training Loss: 0.9314564049243927, Training Accuracy: 96.25%\n",
      "Epoch 29, Batch 30, Training Loss: 0.9339941521485646, Training Accuracy: 96.04166666666667%\n",
      "Epoch 29, Batch 40, Training Loss: 0.9356258749961853, Training Accuracy: 96.171875%\n",
      "Epoch 29, Batch 50, Training Loss: 0.9360308730602265, Training Accuracy: 96.25%\n",
      "Epoch 29, Batch 60, Training Loss: 0.9329995999733607, Training Accuracy: 96.40625%\n",
      "Epoch 29, Batch 70, Training Loss: 0.9325406304427556, Training Accuracy: 96.42857142857143%\n",
      "Epoch 29, Batch 80, Training Loss: 0.934212227165699, Training Accuracy: 96.1328125%\n",
      "Epoch 29, Batch 90, Training Loss: 0.9338525474071503, Training Accuracy: 96.18055555555556%\n",
      "Epoch 29, Validation Loss: 0.9328912626142087, Validation Accuracy: 96.32152588555857%\n",
      "Epoch 29: Adam lr 0.0001\n",
      "Epoch 30, Batch 10, Training Loss: 0.9302169561386109, Training Accuracy: 96.875%\n",
      "Epoch 30, Batch 20, Training Loss: 0.927079153060913, Training Accuracy: 96.71875%\n",
      "Epoch 30, Batch 30, Training Loss: 0.9261952837308248, Training Accuracy: 96.77083333333333%\n",
      "Epoch 30, Batch 40, Training Loss: 0.9277835175395012, Training Accuracy: 96.796875%\n",
      "Epoch 30, Batch 50, Training Loss: 0.9289468646049499, Training Accuracy: 96.5625%\n",
      "Epoch 30, Batch 60, Training Loss: 0.929604388276736, Training Accuracy: 96.35416666666666%\n",
      "Epoch 30, Batch 70, Training Loss: 0.927709321464811, Training Accuracy: 96.51785714285714%\n",
      "Epoch 30, Batch 80, Training Loss: 0.9296302683651447, Training Accuracy: 96.25%\n",
      "Epoch 30, Batch 90, Training Loss: 0.9314683788352542, Training Accuracy: 96.21527777777777%\n",
      "Epoch 30, Validation Loss: 0.933023188425147, Validation Accuracy: 96.45776566757493%\n",
      "Epoch 30: Adam lr 0.0001\n",
      "Epoch 31, Batch 10, Training Loss: 0.936933022737503, Training Accuracy: 96.25%\n",
      "Epoch 31, Batch 20, Training Loss: 0.930548906326294, Training Accuracy: 96.40625%\n",
      "Epoch 31, Batch 30, Training Loss: 0.9269351164499918, Training Accuracy: 96.77083333333333%\n",
      "Epoch 31, Batch 40, Training Loss: 0.9274429365992546, Training Accuracy: 96.875%\n",
      "Epoch 31, Batch 50, Training Loss: 0.9285986316204071, Training Accuracy: 96.6875%\n",
      "Epoch 31, Batch 60, Training Loss: 0.9301823824644089, Training Accuracy: 96.45833333333333%\n",
      "Epoch 31, Batch 70, Training Loss: 0.9324211384568896, Training Accuracy: 96.11607142857143%\n",
      "Epoch 31, Batch 80, Training Loss: 0.9304825320839882, Training Accuracy: 96.328125%\n",
      "Epoch 31, Batch 90, Training Loss: 0.9315329796738094, Training Accuracy: 96.18055555555556%\n",
      "Epoch 31, Validation Loss: 0.9332342562468156, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 31: Adam lr 0.0001\n",
      "Epoch 32, Batch 10, Training Loss: 0.9361485838890076, Training Accuracy: 95.625%\n",
      "Epoch 32, Batch 20, Training Loss: 0.9266239762306213, Training Accuracy: 96.875%\n",
      "Epoch 32, Batch 30, Training Loss: 0.9228926499684652, Training Accuracy: 97.1875%\n",
      "Epoch 32, Batch 40, Training Loss: 0.9239228785037994, Training Accuracy: 97.03125%\n",
      "Epoch 32, Batch 50, Training Loss: 0.9243312895298004, Training Accuracy: 96.9375%\n",
      "Epoch 32, Batch 60, Training Loss: 0.9265647729237875, Training Accuracy: 96.66666666666667%\n",
      "Epoch 32, Batch 70, Training Loss: 0.9315104918820517, Training Accuracy: 96.11607142857143%\n",
      "Epoch 32, Batch 80, Training Loss: 0.9313461527228355, Training Accuracy: 96.2109375%\n",
      "Epoch 32, Batch 90, Training Loss: 0.9316528128253089, Training Accuracy: 96.11111111111111%\n",
      "Epoch 32, Validation Loss: 0.9334777334462041, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 32: Adam lr 0.0000\n",
      "Epoch 33, Batch 10, Training Loss: 0.9283008337020874, Training Accuracy: 95.9375%\n",
      "Epoch 33, Batch 20, Training Loss: 0.9348487347364426, Training Accuracy: 95.9375%\n",
      "Epoch 33, Batch 30, Training Loss: 0.935511036713918, Training Accuracy: 95.9375%\n",
      "Epoch 33, Batch 40, Training Loss: 0.9366525128483772, Training Accuracy: 95.78125%\n",
      "Epoch 33, Batch 50, Training Loss: 0.9346172189712525, Training Accuracy: 96.0625%\n",
      "Epoch 33, Batch 60, Training Loss: 0.9329615503549575, Training Accuracy: 96.14583333333333%\n",
      "Epoch 33, Batch 70, Training Loss: 0.9330596234117235, Training Accuracy: 96.07142857142857%\n",
      "Epoch 33, Batch 80, Training Loss: 0.9332399159669876, Training Accuracy: 96.09375%\n",
      "Epoch 33, Batch 90, Training Loss: 0.9327730867597792, Training Accuracy: 96.18055555555556%\n",
      "Epoch 33, Validation Loss: 0.933305385320083, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 33: Adam lr 0.0000\n",
      "Epoch 34, Batch 10, Training Loss: 0.9209266662597656, Training Accuracy: 97.1875%\n",
      "Epoch 34, Batch 20, Training Loss: 0.9222595810890197, Training Accuracy: 97.1875%\n",
      "Epoch 34, Batch 30, Training Loss: 0.9250101546446482, Training Accuracy: 96.875%\n",
      "Epoch 34, Batch 40, Training Loss: 0.9307738780975342, Training Accuracy: 96.25%\n",
      "Epoch 34, Batch 50, Training Loss: 0.9299267733097076, Training Accuracy: 96.5%\n",
      "Epoch 34, Batch 60, Training Loss: 0.9321064045031865, Training Accuracy: 96.25%\n",
      "Epoch 34, Batch 70, Training Loss: 0.9319050644125257, Training Accuracy: 96.25%\n",
      "Epoch 34, Batch 80, Training Loss: 0.9306514471769333, Training Accuracy: 96.40625%\n",
      "Epoch 34, Batch 90, Training Loss: 0.9323169820838504, Training Accuracy: 96.11111111111111%\n",
      "Epoch 34, Validation Loss: 0.9333182780638986, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 34: Adam lr 0.0000\n",
      "Epoch 35, Batch 10, Training Loss: 0.9321504592895508, Training Accuracy: 96.5625%\n",
      "Epoch 35, Batch 20, Training Loss: 0.9358451694250107, Training Accuracy: 96.5625%\n",
      "Epoch 35, Batch 30, Training Loss: 0.9335530380407969, Training Accuracy: 96.35416666666666%\n",
      "Epoch 35, Batch 40, Training Loss: 0.9357683464884758, Training Accuracy: 96.25%\n",
      "Epoch 35, Batch 50, Training Loss: 0.9344512116909027, Training Accuracy: 96.25%\n",
      "Epoch 35, Batch 60, Training Loss: 0.9336244722207387, Training Accuracy: 96.09375%\n",
      "Epoch 35, Batch 70, Training Loss: 0.9318636213030134, Training Accuracy: 96.25%\n",
      "Epoch 35, Batch 80, Training Loss: 0.930954821407795, Training Accuracy: 96.484375%\n",
      "Epoch 35, Batch 90, Training Loss: 0.930731592575709, Training Accuracy: 96.42361111111111%\n",
      "Epoch 35, Validation Loss: 0.9333737803542096, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 35: Adam lr 0.0000\n",
      "Epoch 36, Batch 10, Training Loss: 0.9372913777828217, Training Accuracy: 95.0%\n",
      "Epoch 36, Batch 20, Training Loss: 0.9311129361391067, Training Accuracy: 96.25%\n",
      "Epoch 36, Batch 30, Training Loss: 0.9345852911472321, Training Accuracy: 96.14583333333333%\n",
      "Epoch 36, Batch 40, Training Loss: 0.9351768061518669, Training Accuracy: 96.015625%\n",
      "Epoch 36, Batch 50, Training Loss: 0.9334056210517884, Training Accuracy: 96.3125%\n",
      "Epoch 36, Batch 60, Training Loss: 0.9339163392782212, Training Accuracy: 96.35416666666666%\n",
      "Epoch 36, Batch 70, Training Loss: 0.9336430430412292, Training Accuracy: 96.33928571428572%\n",
      "Epoch 36, Batch 80, Training Loss: 0.9326974429190159, Training Accuracy: 96.2890625%\n",
      "Epoch 36, Batch 90, Training Loss: 0.9318602780501047, Training Accuracy: 96.21527777777777%\n",
      "Epoch 36, Validation Loss: 0.9332240794015967, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 36: Adam lr 0.0000\n",
      "Epoch 37, Batch 10, Training Loss: 0.9378395795822143, Training Accuracy: 95.3125%\n",
      "Epoch 37, Batch 20, Training Loss: 0.9274976521730423, Training Accuracy: 96.5625%\n",
      "Epoch 37, Batch 30, Training Loss: 0.9306944330533345, Training Accuracy: 96.5625%\n",
      "Epoch 37, Batch 40, Training Loss: 0.9320862889289856, Training Accuracy: 96.40625%\n",
      "Epoch 37, Batch 50, Training Loss: 0.9334688460826874, Training Accuracy: 96.25%\n",
      "Epoch 37, Batch 60, Training Loss: 0.9313102861245474, Training Accuracy: 96.51041666666667%\n",
      "Epoch 37, Batch 70, Training Loss: 0.9322241961956024, Training Accuracy: 96.07142857142857%\n",
      "Epoch 37, Batch 80, Training Loss: 0.9307410724461078, Training Accuracy: 96.171875%\n",
      "Epoch 37, Batch 90, Training Loss: 0.9297828376293182, Training Accuracy: 96.35416666666666%\n",
      "Epoch 37, Validation Loss: 0.9331428149472112, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 37: Adam lr 0.0000\n",
      "Epoch 38, Batch 10, Training Loss: 0.9334490954875946, Training Accuracy: 96.875%\n",
      "Epoch 38, Batch 20, Training Loss: 0.9407542943954468, Training Accuracy: 96.25%\n",
      "Epoch 38, Batch 30, Training Loss: 0.9351726452509562, Training Accuracy: 96.66666666666667%\n",
      "Epoch 38, Batch 40, Training Loss: 0.9367566972970962, Training Accuracy: 96.015625%\n",
      "Epoch 38, Batch 50, Training Loss: 0.9324293446540832, Training Accuracy: 96.5%\n",
      "Epoch 38, Batch 60, Training Loss: 0.9312008241812388, Training Accuracy: 96.5625%\n",
      "Epoch 38, Batch 70, Training Loss: 0.9300760618277959, Training Accuracy: 96.60714285714286%\n",
      "Epoch 38, Batch 80, Training Loss: 0.9314154088497162, Training Accuracy: 96.484375%\n",
      "Epoch 38, Batch 90, Training Loss: 0.9320050537586212, Training Accuracy: 96.31944444444444%\n",
      "Epoch 38, Validation Loss: 0.933102750259897, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 38: Adam lr 0.0000\n",
      "Epoch 39, Batch 10, Training Loss: 0.928568160533905, Training Accuracy: 96.5625%\n",
      "Epoch 39, Batch 20, Training Loss: 0.9342944473028183, Training Accuracy: 95.78125%\n",
      "Epoch 39, Batch 30, Training Loss: 0.9301143308480581, Training Accuracy: 96.25%\n",
      "Epoch 39, Batch 40, Training Loss: 0.9262741774320602, Training Accuracy: 96.796875%\n",
      "Epoch 39, Batch 50, Training Loss: 0.9274308466911316, Training Accuracy: 96.75%\n",
      "Epoch 39, Batch 60, Training Loss: 0.9275894641876221, Training Accuracy: 96.5625%\n",
      "Epoch 39, Batch 70, Training Loss: 0.9294670019830976, Training Accuracy: 96.38392857142857%\n",
      "Epoch 39, Batch 80, Training Loss: 0.9285507440567017, Training Accuracy: 96.5234375%\n",
      "Epoch 39, Batch 90, Training Loss: 0.9301142003801134, Training Accuracy: 96.38888888888889%\n",
      "Epoch 39, Validation Loss: 0.9330887353938558, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 39: Adam lr 0.0000\n",
      "Epoch 40, Batch 10, Training Loss: 0.9230599462985992, Training Accuracy: 97.8125%\n",
      "Epoch 40, Batch 20, Training Loss: 0.9309362590312957, Training Accuracy: 97.1875%\n",
      "Epoch 40, Batch 30, Training Loss: 0.9321346362431844, Training Accuracy: 96.77083333333333%\n",
      "Epoch 40, Batch 40, Training Loss: 0.9321321845054626, Training Accuracy: 96.484375%\n",
      "Epoch 40, Batch 50, Training Loss: 0.9315564072132111, Training Accuracy: 96.5625%\n",
      "Epoch 40, Batch 60, Training Loss: 0.9299081971247991, Training Accuracy: 96.5625%\n",
      "Epoch 40, Batch 70, Training Loss: 0.9286058766501291, Training Accuracy: 96.60714285714286%\n",
      "Epoch 40, Batch 80, Training Loss: 0.9312926948070526, Training Accuracy: 96.2890625%\n",
      "Epoch 40, Batch 90, Training Loss: 0.9311038374900817, Training Accuracy: 96.28472222222221%\n",
      "Epoch 40, Validation Loss: 0.9330868021301602, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 40: Adam lr 0.0000\n",
      "Epoch 41, Batch 10, Training Loss: 0.9280714452266693, Training Accuracy: 97.1875%\n",
      "Epoch 41, Batch 20, Training Loss: 0.9236386865377426, Training Accuracy: 97.5%\n",
      "Epoch 41, Batch 30, Training Loss: 0.9255093197027843, Training Accuracy: 97.08333333333333%\n",
      "Epoch 41, Batch 40, Training Loss: 0.9281874850392342, Training Accuracy: 96.875%\n",
      "Epoch 41, Batch 50, Training Loss: 0.9305229079723358, Training Accuracy: 96.4375%\n",
      "Epoch 41, Batch 60, Training Loss: 0.9286819607019424, Training Accuracy: 96.61458333333334%\n",
      "Epoch 41, Batch 70, Training Loss: 0.9293205610343388, Training Accuracy: 96.42857142857143%\n",
      "Epoch 41, Batch 80, Training Loss: 0.9304079294204712, Training Accuracy: 96.2890625%\n",
      "Epoch 41, Batch 90, Training Loss: 0.9301719499958886, Training Accuracy: 96.31944444444444%\n",
      "Epoch 41, Validation Loss: 0.9330803855605747, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 41: Adam lr 0.0000\n",
      "Epoch 42, Batch 10, Training Loss: 0.9218296706676483, Training Accuracy: 96.875%\n",
      "Epoch 42, Batch 20, Training Loss: 0.9212111741304397, Training Accuracy: 97.5%\n",
      "Epoch 42, Batch 30, Training Loss: 0.9291053096453349, Training Accuracy: 96.14583333333333%\n",
      "Epoch 42, Batch 40, Training Loss: 0.9302318409085274, Training Accuracy: 96.171875%\n",
      "Epoch 42, Batch 50, Training Loss: 0.9309331047534942, Training Accuracy: 96.0625%\n",
      "Epoch 42, Batch 60, Training Loss: 0.931530382235845, Training Accuracy: 96.04166666666667%\n",
      "Epoch 42, Batch 70, Training Loss: 0.9311221974236624, Training Accuracy: 96.20535714285714%\n",
      "Epoch 42, Batch 80, Training Loss: 0.9302399002015591, Training Accuracy: 96.3671875%\n",
      "Epoch 42, Batch 90, Training Loss: 0.9309886852900188, Training Accuracy: 96.28472222222221%\n",
      "Epoch 42, Validation Loss: 0.9330803907435873, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 42: Adam lr 0.0000\n",
      "Epoch 43, Batch 10, Training Loss: 0.9370886564254761, Training Accuracy: 95.0%\n",
      "Epoch 43, Batch 20, Training Loss: 0.9404554605484009, Training Accuracy: 94.84375%\n",
      "Epoch 43, Batch 30, Training Loss: 0.9332858006159465, Training Accuracy: 95.83333333333334%\n",
      "Epoch 43, Batch 40, Training Loss: 0.9344673499464988, Training Accuracy: 95.703125%\n",
      "Epoch 43, Batch 50, Training Loss: 0.9332357728481293, Training Accuracy: 95.875%\n",
      "Epoch 43, Batch 60, Training Loss: 0.9316900620857874, Training Accuracy: 96.14583333333333%\n",
      "Epoch 43, Batch 70, Training Loss: 0.9307737095015389, Training Accuracy: 96.29464285714285%\n",
      "Epoch 43, Batch 80, Training Loss: 0.930639773607254, Training Accuracy: 96.2890625%\n",
      "Epoch 43, Batch 90, Training Loss: 0.9300005594889323, Training Accuracy: 96.31944444444444%\n",
      "Epoch 43, Validation Loss: 0.9330742696057195, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 43: Adam lr 0.0000\n",
      "Epoch 44, Batch 10, Training Loss: 0.937129694223404, Training Accuracy: 95.9375%\n",
      "Epoch 44, Batch 20, Training Loss: 0.9279616564512253, Training Accuracy: 96.71875%\n",
      "Epoch 44, Batch 30, Training Loss: 0.9273325761159261, Training Accuracy: 96.5625%\n",
      "Epoch 44, Batch 40, Training Loss: 0.9272999495267868, Training Accuracy: 96.640625%\n",
      "Epoch 44, Batch 50, Training Loss: 0.9277305507659912, Training Accuracy: 96.75%\n",
      "Epoch 44, Batch 60, Training Loss: 0.9266978243986765, Training Accuracy: 96.875%\n",
      "Epoch 44, Batch 70, Training Loss: 0.9274261202130999, Training Accuracy: 96.74107142857142%\n",
      "Epoch 44, Batch 80, Training Loss: 0.9306039832532406, Training Accuracy: 96.25%\n",
      "Epoch 44, Batch 90, Training Loss: 0.9304107891188728, Training Accuracy: 96.25%\n",
      "Epoch 44, Validation Loss: 0.9330684957296952, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 44: Adam lr 0.0000\n",
      "Epoch 45, Batch 10, Training Loss: 0.9417717218399048, Training Accuracy: 95.3125%\n",
      "Epoch 45, Batch 20, Training Loss: 0.9347357898950577, Training Accuracy: 95.625%\n",
      "Epoch 45, Batch 30, Training Loss: 0.9323373913764954, Training Accuracy: 96.14583333333333%\n",
      "Epoch 45, Batch 40, Training Loss: 0.9308145865797997, Training Accuracy: 96.40625%\n",
      "Epoch 45, Batch 50, Training Loss: 0.9313822090625763, Training Accuracy: 96.3125%\n",
      "Epoch 45, Batch 60, Training Loss: 0.930219570795695, Training Accuracy: 96.25%\n",
      "Epoch 45, Batch 70, Training Loss: 0.9308138234274728, Training Accuracy: 96.07142857142857%\n",
      "Epoch 45, Batch 80, Training Loss: 0.9305712014436722, Training Accuracy: 96.25%\n",
      "Epoch 45, Batch 90, Training Loss: 0.9312015175819397, Training Accuracy: 96.18055555555556%\n",
      "Epoch 45, Validation Loss: 0.9330688740896143, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 45: Adam lr 0.0000\n",
      "Epoch 46, Batch 10, Training Loss: 0.9331529796123504, Training Accuracy: 95.3125%\n",
      "Epoch 46, Batch 20, Training Loss: 0.9340150564908981, Training Accuracy: 95.78125%\n",
      "Epoch 46, Batch 30, Training Loss: 0.9334395110607148, Training Accuracy: 96.04166666666667%\n",
      "Epoch 46, Batch 40, Training Loss: 0.93024340569973, Training Accuracy: 96.40625%\n",
      "Epoch 46, Batch 50, Training Loss: 0.9296302032470704, Training Accuracy: 96.3125%\n",
      "Epoch 46, Batch 60, Training Loss: 0.9294398734966914, Training Accuracy: 96.30208333333333%\n",
      "Epoch 46, Batch 70, Training Loss: 0.9304314962455205, Training Accuracy: 96.20535714285714%\n",
      "Epoch 46, Batch 80, Training Loss: 0.93203459456563, Training Accuracy: 96.2109375%\n",
      "Epoch 46, Batch 90, Training Loss: 0.9311344332165188, Training Accuracy: 96.31944444444444%\n",
      "Epoch 46, Validation Loss: 0.9330690891846366, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 46: Adam lr 0.0000\n",
      "Epoch 47, Batch 10, Training Loss: 0.9408872663974762, Training Accuracy: 95.0%\n",
      "Epoch 47, Batch 20, Training Loss: 0.9313875168561936, Training Accuracy: 96.09375%\n",
      "Epoch 47, Batch 30, Training Loss: 0.9341448764006297, Training Accuracy: 95.9375%\n",
      "Epoch 47, Batch 40, Training Loss: 0.9304504543542862, Training Accuracy: 96.171875%\n",
      "Epoch 47, Batch 50, Training Loss: 0.9332118391990661, Training Accuracy: 96.0625%\n",
      "Epoch 47, Batch 60, Training Loss: 0.9319767187039057, Training Accuracy: 96.19791666666667%\n",
      "Epoch 47, Batch 70, Training Loss: 0.931253753389631, Training Accuracy: 96.33928571428572%\n",
      "Epoch 47, Batch 80, Training Loss: 0.9309812307357788, Training Accuracy: 96.2890625%\n",
      "Epoch 47, Batch 90, Training Loss: 0.9314007778962453, Training Accuracy: 96.31944444444444%\n",
      "Epoch 47, Validation Loss: 0.9330685449683148, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 47: Adam lr 0.0000\n",
      "Epoch 48, Batch 10, Training Loss: 0.927265077829361, Training Accuracy: 96.25%\n",
      "Epoch 48, Batch 20, Training Loss: 0.9337374597787857, Training Accuracy: 95.78125%\n",
      "Epoch 48, Batch 30, Training Loss: 0.9317633052666981, Training Accuracy: 96.25%\n",
      "Epoch 48, Batch 40, Training Loss: 0.931846471130848, Training Accuracy: 96.328125%\n",
      "Epoch 48, Batch 50, Training Loss: 0.931116555929184, Training Accuracy: 96.4375%\n",
      "Epoch 48, Batch 60, Training Loss: 0.9307534178098043, Training Accuracy: 96.40625%\n",
      "Epoch 48, Batch 70, Training Loss: 0.9315252321107047, Training Accuracy: 96.33928571428572%\n",
      "Epoch 48, Batch 80, Training Loss: 0.9330691210925579, Training Accuracy: 96.0546875%\n",
      "Epoch 48, Batch 90, Training Loss: 0.9313856773906284, Training Accuracy: 96.28472222222221%\n",
      "Epoch 48, Validation Loss: 0.9330681044122447, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 48: Adam lr 0.0000\n",
      "Epoch 49, Batch 10, Training Loss: 0.9505355715751648, Training Accuracy: 94.6875%\n",
      "Epoch 49, Batch 20, Training Loss: 0.947687828540802, Training Accuracy: 95.15625%\n",
      "Epoch 49, Batch 30, Training Loss: 0.9445394138495128, Training Accuracy: 95.3125%\n",
      "Epoch 49, Batch 40, Training Loss: 0.9410415157675743, Training Accuracy: 95.625%\n",
      "Epoch 49, Batch 50, Training Loss: 0.9373818576335907, Training Accuracy: 95.8125%\n",
      "Epoch 49, Batch 60, Training Loss: 0.9370519826809566, Training Accuracy: 95.9375%\n",
      "Epoch 49, Batch 70, Training Loss: 0.935659224646432, Training Accuracy: 95.98214285714286%\n",
      "Epoch 49, Batch 80, Training Loss: 0.9345219969749451, Training Accuracy: 96.1328125%\n",
      "Epoch 49, Batch 90, Training Loss: 0.9339255054791769, Training Accuracy: 96.18055555555556%\n",
      "Epoch 49, Validation Loss: 0.9330678763596908, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 49: Adam lr 0.0000\n",
      "Epoch 50, Batch 10, Training Loss: 0.9547787666320801, Training Accuracy: 95.9375%\n",
      "Epoch 50, Batch 20, Training Loss: 0.9458684235811233, Training Accuracy: 95.625%\n",
      "Epoch 50, Batch 30, Training Loss: 0.9381478190422058, Training Accuracy: 96.45833333333333%\n",
      "Epoch 50, Batch 40, Training Loss: 0.9418613687157631, Training Accuracy: 95.859375%\n",
      "Epoch 50, Batch 50, Training Loss: 0.9415434491634369, Training Accuracy: 95.75%\n",
      "Epoch 50, Batch 60, Training Loss: 0.937698557972908, Training Accuracy: 96.04166666666667%\n",
      "Epoch 50, Batch 70, Training Loss: 0.9349143590245929, Training Accuracy: 96.29464285714285%\n",
      "Epoch 50, Batch 80, Training Loss: 0.935175434499979, Training Accuracy: 96.2109375%\n",
      "Epoch 50, Batch 90, Training Loss: 0.9337255557378134, Training Accuracy: 96.38888888888889%\n",
      "Epoch 50, Validation Loss: 0.9330677493758823, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 50: Adam lr 0.0000\n",
      "Epoch 51, Batch 10, Training Loss: 0.9254241406917572, Training Accuracy: 96.875%\n",
      "Epoch 51, Batch 20, Training Loss: 0.9275692403316498, Training Accuracy: 96.5625%\n",
      "Epoch 51, Batch 30, Training Loss: 0.926111634572347, Training Accuracy: 96.77083333333333%\n",
      "Epoch 51, Batch 40, Training Loss: 0.9252216652035713, Training Accuracy: 96.796875%\n",
      "Epoch 51, Batch 50, Training Loss: 0.9287988364696502, Training Accuracy: 96.625%\n",
      "Epoch 51, Batch 60, Training Loss: 0.9304871002833048, Training Accuracy: 96.40625%\n",
      "Epoch 51, Batch 70, Training Loss: 0.9301105456692832, Training Accuracy: 96.42857142857143%\n",
      "Epoch 51, Batch 80, Training Loss: 0.9299716703593731, Training Accuracy: 96.40625%\n",
      "Epoch 51, Batch 90, Training Loss: 0.931058535973231, Training Accuracy: 96.35416666666666%\n",
      "Epoch 51, Validation Loss: 0.933067790839983, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 51: Adam lr 0.0000\n",
      "Epoch 52, Batch 10, Training Loss: 0.9247408807277679, Training Accuracy: 96.25%\n",
      "Epoch 52, Batch 20, Training Loss: 0.9265511780977249, Training Accuracy: 96.40625%\n",
      "Epoch 52, Batch 30, Training Loss: 0.9300855000813802, Training Accuracy: 96.25%\n",
      "Epoch 52, Batch 40, Training Loss: 0.9302546218037605, Training Accuracy: 96.328125%\n",
      "Epoch 52, Batch 50, Training Loss: 0.9309028935432434, Training Accuracy: 96.375%\n",
      "Epoch 52, Batch 60, Training Loss: 0.9308393687009812, Training Accuracy: 96.35416666666666%\n",
      "Epoch 52, Batch 70, Training Loss: 0.9314585949693407, Training Accuracy: 96.33928571428572%\n",
      "Epoch 52, Batch 80, Training Loss: 0.9316782884299755, Training Accuracy: 96.40625%\n",
      "Epoch 52, Batch 90, Training Loss: 0.9325199961662293, Training Accuracy: 96.28472222222221%\n",
      "Epoch 52, Validation Loss: 0.9330677675164264, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 52: Adam lr 0.0000\n",
      "Epoch 53, Batch 10, Training Loss: 0.9369532346725464, Training Accuracy: 95.9375%\n",
      "Epoch 53, Batch 20, Training Loss: 0.926474517583847, Training Accuracy: 96.71875%\n",
      "Epoch 53, Batch 30, Training Loss: 0.9275610009829204, Training Accuracy: 96.875%\n",
      "Epoch 53, Batch 40, Training Loss: 0.9272786095738411, Training Accuracy: 96.875%\n",
      "Epoch 53, Batch 50, Training Loss: 0.9319204044342041, Training Accuracy: 96.1875%\n",
      "Epoch 53, Batch 60, Training Loss: 0.9314950923124949, Training Accuracy: 96.35416666666666%\n",
      "Epoch 53, Batch 70, Training Loss: 0.9322996412004744, Training Accuracy: 96.11607142857143%\n",
      "Epoch 53, Batch 80, Training Loss: 0.9321531370282173, Training Accuracy: 96.09375%\n",
      "Epoch 53, Batch 90, Training Loss: 0.9310279952155219, Training Accuracy: 96.28472222222221%\n",
      "Epoch 53, Validation Loss: 0.9330677416013635, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 53: Adam lr 0.0000\n",
      "Epoch 54, Batch 10, Training Loss: 0.9240220069885254, Training Accuracy: 97.5%\n",
      "Epoch 54, Batch 20, Training Loss: 0.925721287727356, Training Accuracy: 96.875%\n",
      "Epoch 54, Batch 30, Training Loss: 0.9253409385681153, Training Accuracy: 96.97916666666667%\n",
      "Epoch 54, Batch 40, Training Loss: 0.9248899802565574, Training Accuracy: 96.953125%\n",
      "Epoch 54, Batch 50, Training Loss: 0.9260479915142059, Training Accuracy: 96.8125%\n",
      "Epoch 54, Batch 60, Training Loss: 0.9259639243284862, Training Accuracy: 96.71875%\n",
      "Epoch 54, Batch 70, Training Loss: 0.9273844191006252, Training Accuracy: 96.42857142857143%\n",
      "Epoch 54, Batch 80, Training Loss: 0.9274418734014034, Training Accuracy: 96.328125%\n",
      "Epoch 54, Batch 90, Training Loss: 0.9285615311728583, Training Accuracy: 96.28472222222221%\n",
      "Epoch 54, Validation Loss: 0.9330676975457565, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 54: Adam lr 0.0000\n",
      "Epoch 55, Batch 10, Training Loss: 0.9316325604915618, Training Accuracy: 95.9375%\n",
      "Epoch 55, Batch 20, Training Loss: 0.93055659532547, Training Accuracy: 96.09375%\n",
      "Epoch 55, Batch 30, Training Loss: 0.9280553042888642, Training Accuracy: 96.25%\n",
      "Epoch 55, Batch 40, Training Loss: 0.9296385511755944, Training Accuracy: 96.09375%\n",
      "Epoch 55, Batch 50, Training Loss: 0.932646996974945, Training Accuracy: 95.9375%\n",
      "Epoch 55, Batch 60, Training Loss: 0.9299650659163793, Training Accuracy: 96.19791666666667%\n",
      "Epoch 55, Batch 70, Training Loss: 0.9298172635691506, Training Accuracy: 96.25%\n",
      "Epoch 55, Batch 80, Training Loss: 0.9318208612501622, Training Accuracy: 96.1328125%\n",
      "Epoch 55, Batch 90, Training Loss: 0.930904487768809, Training Accuracy: 96.31944444444444%\n",
      "Epoch 55, Validation Loss: 0.9330677545588949, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 55: Adam lr 0.0000\n",
      "Epoch 56, Batch 10, Training Loss: 0.928435456752777, Training Accuracy: 96.5625%\n",
      "Epoch 56, Batch 20, Training Loss: 0.9266815215349198, Training Accuracy: 96.71875%\n",
      "Epoch 56, Batch 30, Training Loss: 0.9304663479328156, Training Accuracy: 96.77083333333333%\n",
      "Epoch 56, Batch 40, Training Loss: 0.9302183717489243, Training Accuracy: 96.640625%\n",
      "Epoch 56, Batch 50, Training Loss: 0.9309891080856323, Training Accuracy: 96.5625%\n",
      "Epoch 56, Batch 60, Training Loss: 0.9329996963342031, Training Accuracy: 96.25%\n",
      "Epoch 56, Batch 70, Training Loss: 0.9334224104881287, Training Accuracy: 96.16071428571429%\n",
      "Epoch 56, Batch 80, Training Loss: 0.9312996119260788, Training Accuracy: 96.3671875%\n",
      "Epoch 56, Batch 90, Training Loss: 0.9310052812099456, Training Accuracy: 96.31944444444444%\n",
      "Epoch 56, Validation Loss: 0.9330677001372628, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 56: Adam lr 0.0000\n",
      "Epoch 57, Batch 10, Training Loss: 0.9322258591651916, Training Accuracy: 95.0%\n",
      "Epoch 57, Batch 20, Training Loss: 0.9328343868255615, Training Accuracy: 96.09375%\n",
      "Epoch 57, Batch 30, Training Loss: 0.933670441309611, Training Accuracy: 95.9375%\n",
      "Epoch 57, Batch 40, Training Loss: 0.9352291956543922, Training Accuracy: 96.015625%\n",
      "Epoch 57, Batch 50, Training Loss: 0.9346824097633362, Training Accuracy: 95.9375%\n",
      "Epoch 57, Batch 60, Training Loss: 0.9351021985212962, Training Accuracy: 95.83333333333334%\n",
      "Epoch 57, Batch 70, Training Loss: 0.9319546444075448, Training Accuracy: 96.25%\n",
      "Epoch 57, Batch 80, Training Loss: 0.9312987379729748, Training Accuracy: 96.40625%\n",
      "Epoch 57, Batch 90, Training Loss: 0.9317546857727899, Training Accuracy: 96.31944444444444%\n",
      "Epoch 57, Validation Loss: 0.933067658673162, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 57: Adam lr 0.0000\n",
      "Epoch 58, Batch 10, Training Loss: 0.928803151845932, Training Accuracy: 96.25%\n",
      "Epoch 58, Batch 20, Training Loss: 0.9383535712957383, Training Accuracy: 95.46875%\n",
      "Epoch 58, Batch 30, Training Loss: 0.9350882530212402, Training Accuracy: 95.52083333333333%\n",
      "Epoch 58, Batch 40, Training Loss: 0.9337730899453163, Training Accuracy: 95.859375%\n",
      "Epoch 58, Batch 50, Training Loss: 0.9329026186466217, Training Accuracy: 96.0%\n",
      "Epoch 58, Batch 60, Training Loss: 0.9309676220019658, Training Accuracy: 96.25%\n",
      "Epoch 58, Batch 70, Training Loss: 0.9297689599650246, Training Accuracy: 96.38392857142857%\n",
      "Epoch 58, Batch 80, Training Loss: 0.9303574033081532, Training Accuracy: 96.328125%\n",
      "Epoch 58, Batch 90, Training Loss: 0.9309591710567474, Training Accuracy: 96.14583333333333%\n",
      "Epoch 58, Validation Loss: 0.933067684588225, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 58: Adam lr 0.0000\n",
      "Epoch 59, Batch 10, Training Loss: 0.9345131337642669, Training Accuracy: 95.9375%\n",
      "Epoch 59, Batch 20, Training Loss: 0.9304749846458436, Training Accuracy: 96.25%\n",
      "Epoch 59, Batch 30, Training Loss: 0.9331529676914215, Training Accuracy: 96.04166666666667%\n",
      "Epoch 59, Batch 40, Training Loss: 0.9327124923467636, Training Accuracy: 96.09375%\n",
      "Epoch 59, Batch 50, Training Loss: 0.932502179145813, Training Accuracy: 96.0%\n",
      "Epoch 59, Batch 60, Training Loss: 0.9303999861081441, Training Accuracy: 96.30208333333333%\n",
      "Epoch 59, Batch 70, Training Loss: 0.9304889389446803, Training Accuracy: 96.38392857142857%\n",
      "Epoch 59, Batch 80, Training Loss: 0.9298260122537613, Training Accuracy: 96.40625%\n",
      "Epoch 59, Batch 90, Training Loss: 0.9309931503401863, Training Accuracy: 96.18055555555556%\n",
      "Epoch 59, Validation Loss: 0.9330676975457565, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 59: Adam lr 0.0000\n",
      "Epoch 60, Batch 10, Training Loss: 0.9416270971298217, Training Accuracy: 96.25%\n",
      "Epoch 60, Batch 20, Training Loss: 0.939440643787384, Training Accuracy: 95.9375%\n",
      "Epoch 60, Batch 30, Training Loss: 0.9339696347713471, Training Accuracy: 96.25%\n",
      "Epoch 60, Batch 40, Training Loss: 0.9321014389395714, Training Accuracy: 96.328125%\n",
      "Epoch 60, Batch 50, Training Loss: 0.9289568674564361, Training Accuracy: 96.625%\n",
      "Epoch 60, Batch 60, Training Loss: 0.9306674857934316, Training Accuracy: 96.51041666666667%\n",
      "Epoch 60, Batch 70, Training Loss: 0.9323569910866873, Training Accuracy: 96.20535714285714%\n",
      "Epoch 60, Batch 80, Training Loss: 0.9307987123727799, Training Accuracy: 96.2890625%\n",
      "Epoch 60, Batch 90, Training Loss: 0.9313589082823859, Training Accuracy: 96.31944444444444%\n",
      "Epoch 60, Validation Loss: 0.933067702728769, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 60: Adam lr 0.0000\n",
      "Epoch 61, Batch 10, Training Loss: 0.9384011685848236, Training Accuracy: 95.3125%\n",
      "Epoch 61, Batch 20, Training Loss: 0.9293281972408295, Training Accuracy: 96.25%\n",
      "Epoch 61, Batch 30, Training Loss: 0.9289624353249868, Training Accuracy: 96.45833333333333%\n",
      "Epoch 61, Batch 40, Training Loss: 0.9334127545356751, Training Accuracy: 95.703125%\n",
      "Epoch 61, Batch 50, Training Loss: 0.9326133298873901, Training Accuracy: 95.75%\n",
      "Epoch 61, Batch 60, Training Loss: 0.9328495313723882, Training Accuracy: 95.88541666666667%\n",
      "Epoch 61, Batch 70, Training Loss: 0.930847544329507, Training Accuracy: 96.11607142857143%\n",
      "Epoch 61, Batch 80, Training Loss: 0.9307151027023792, Training Accuracy: 96.0546875%\n",
      "Epoch 61, Batch 90, Training Loss: 0.9303868446085188, Training Accuracy: 96.21527777777777%\n",
      "Epoch 61, Validation Loss: 0.9330676327580991, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 61: Adam lr 0.0000\n",
      "Epoch 62, Batch 10, Training Loss: 0.9325599253177643, Training Accuracy: 96.5625%\n",
      "Epoch 62, Batch 20, Training Loss: 0.9315024077892303, Training Accuracy: 97.03125%\n",
      "Epoch 62, Batch 30, Training Loss: 0.9363461852073669, Training Accuracy: 96.25%\n",
      "Epoch 62, Batch 40, Training Loss: 0.935478062927723, Training Accuracy: 96.40625%\n",
      "Epoch 62, Batch 50, Training Loss: 0.9336199581623077, Training Accuracy: 96.375%\n",
      "Epoch 62, Batch 60, Training Loss: 0.9337916274865469, Training Accuracy: 96.35416666666666%\n",
      "Epoch 62, Batch 70, Training Loss: 0.9327487060001918, Training Accuracy: 96.20535714285714%\n",
      "Epoch 62, Batch 80, Training Loss: 0.9315933600068093, Training Accuracy: 96.2890625%\n",
      "Epoch 62, Batch 90, Training Loss: 0.930980556541019, Training Accuracy: 96.45833333333333%\n",
      "Epoch 62, Validation Loss: 0.9330676534901494, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 62: Adam lr 0.0000\n",
      "Epoch 63, Batch 10, Training Loss: 0.9191817462444305, Training Accuracy: 97.5%\n",
      "Epoch 63, Batch 20, Training Loss: 0.9207609742879868, Training Accuracy: 97.34375%\n",
      "Epoch 63, Batch 30, Training Loss: 0.9252053956190746, Training Accuracy: 96.45833333333333%\n",
      "Epoch 63, Batch 40, Training Loss: 0.9285837441682816, Training Accuracy: 96.015625%\n",
      "Epoch 63, Batch 50, Training Loss: 0.9299275875091553, Training Accuracy: 96.1875%\n",
      "Epoch 63, Batch 60, Training Loss: 0.9293907483418783, Training Accuracy: 96.35416666666666%\n",
      "Epoch 63, Batch 70, Training Loss: 0.9302514323166439, Training Accuracy: 96.20535714285714%\n",
      "Epoch 63, Batch 80, Training Loss: 0.9294727236032486, Training Accuracy: 96.40625%\n",
      "Epoch 63, Batch 90, Training Loss: 0.9300800257258945, Training Accuracy: 96.18055555555556%\n",
      "Epoch 63, Validation Loss: 0.9330676508986432, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 63: Adam lr 0.0000\n",
      "Epoch 64, Batch 10, Training Loss: 0.9297725141048432, Training Accuracy: 96.875%\n",
      "Epoch 64, Batch 20, Training Loss: 0.9288233369588852, Training Accuracy: 96.71875%\n",
      "Epoch 64, Batch 30, Training Loss: 0.9302555600802104, Training Accuracy: 96.5625%\n",
      "Epoch 64, Batch 40, Training Loss: 0.9301134705543518, Training Accuracy: 96.484375%\n",
      "Epoch 64, Batch 50, Training Loss: 0.9277550029754639, Training Accuracy: 96.75%\n",
      "Epoch 64, Batch 60, Training Loss: 0.9306695401668549, Training Accuracy: 96.51041666666667%\n",
      "Epoch 64, Batch 70, Training Loss: 0.9297497842993055, Training Accuracy: 96.47321428571428%\n",
      "Epoch 64, Batch 80, Training Loss: 0.9296999707818031, Training Accuracy: 96.4453125%\n",
      "Epoch 64, Batch 90, Training Loss: 0.9313764274120331, Training Accuracy: 96.31944444444444%\n",
      "Epoch 64, Validation Loss: 0.9330676275750865, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 64: Adam lr 0.0000\n",
      "Epoch 65, Batch 10, Training Loss: 0.9352025866508484, Training Accuracy: 95.625%\n",
      "Epoch 65, Batch 20, Training Loss: 0.9305446922779084, Training Accuracy: 96.25%\n",
      "Epoch 65, Batch 30, Training Loss: 0.9262063662211101, Training Accuracy: 96.875%\n",
      "Epoch 65, Batch 40, Training Loss: 0.9259629130363465, Training Accuracy: 96.796875%\n",
      "Epoch 65, Batch 50, Training Loss: 0.9261313378810883, Training Accuracy: 96.625%\n",
      "Epoch 65, Batch 60, Training Loss: 0.9299702644348145, Training Accuracy: 96.40625%\n",
      "Epoch 65, Batch 70, Training Loss: 0.9298391154834202, Training Accuracy: 96.33928571428572%\n",
      "Epoch 65, Batch 80, Training Loss: 0.9300607912242412, Training Accuracy: 96.328125%\n",
      "Epoch 65, Batch 90, Training Loss: 0.9308561358186934, Training Accuracy: 96.25%\n",
      "Epoch 65, Validation Loss: 0.9330676042515299, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 65: Adam lr 0.0000\n",
      "Epoch 66, Batch 10, Training Loss: 0.9305140912532807, Training Accuracy: 96.875%\n",
      "Epoch 66, Batch 20, Training Loss: 0.9336203187704086, Training Accuracy: 96.25%\n",
      "Epoch 66, Batch 30, Training Loss: 0.9337456484635671, Training Accuracy: 96.14583333333333%\n",
      "Epoch 66, Batch 40, Training Loss: 0.936077818274498, Training Accuracy: 96.015625%\n",
      "Epoch 66, Batch 50, Training Loss: 0.9349814462661743, Training Accuracy: 96.1875%\n",
      "Epoch 66, Batch 60, Training Loss: 0.9349268823862076, Training Accuracy: 96.09375%\n",
      "Epoch 66, Batch 70, Training Loss: 0.9328069448471069, Training Accuracy: 96.42857142857143%\n",
      "Epoch 66, Batch 80, Training Loss: 0.9333913497626781, Training Accuracy: 96.171875%\n",
      "Epoch 66, Batch 90, Training Loss: 0.9329670442475213, Training Accuracy: 96.14583333333333%\n",
      "Epoch 66, Validation Loss: 0.9330675731534543, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 66: Adam lr 0.0000\n",
      "Epoch 67, Batch 10, Training Loss: 0.938132905960083, Training Accuracy: 94.375%\n",
      "Epoch 67, Batch 20, Training Loss: 0.94085313975811, Training Accuracy: 95.0%\n",
      "Epoch 67, Batch 30, Training Loss: 0.935550852616628, Training Accuracy: 95.72916666666667%\n",
      "Epoch 67, Batch 40, Training Loss: 0.9290654703974723, Training Accuracy: 96.640625%\n",
      "Epoch 67, Batch 50, Training Loss: 0.9342225682735443, Training Accuracy: 96.0%\n",
      "Epoch 67, Batch 60, Training Loss: 0.930783740679423, Training Accuracy: 96.35416666666666%\n",
      "Epoch 67, Batch 70, Training Loss: 0.9324867350714547, Training Accuracy: 96.11607142857143%\n",
      "Epoch 67, Batch 80, Training Loss: 0.933221747726202, Training Accuracy: 96.171875%\n",
      "Epoch 67, Batch 90, Training Loss: 0.9314417335722182, Training Accuracy: 96.35416666666666%\n",
      "Epoch 67, Validation Loss: 0.933067614617555, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 67: Adam lr 0.0000\n",
      "Epoch 68, Batch 10, Training Loss: 0.9315599143505097, Training Accuracy: 96.25%\n",
      "Epoch 68, Batch 20, Training Loss: 0.9252061486244202, Training Accuracy: 96.875%\n",
      "Epoch 68, Batch 30, Training Loss: 0.9235784490903218, Training Accuracy: 97.1875%\n",
      "Epoch 68, Batch 40, Training Loss: 0.9230695024132729, Training Accuracy: 97.421875%\n",
      "Epoch 68, Batch 50, Training Loss: 0.9261086320877076, Training Accuracy: 96.8125%\n",
      "Epoch 68, Batch 60, Training Loss: 0.9291469871997833, Training Accuracy: 96.35416666666666%\n",
      "Epoch 68, Batch 70, Training Loss: 0.928361667905535, Training Accuracy: 96.47321428571428%\n",
      "Epoch 68, Batch 80, Training Loss: 0.9298394948244095, Training Accuracy: 96.2890625%\n",
      "Epoch 68, Batch 90, Training Loss: 0.9312909689214495, Training Accuracy: 96.31944444444444%\n",
      "Epoch 68, Validation Loss: 0.933067570561948, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 68: Adam lr 0.0000\n",
      "Epoch 69, Batch 10, Training Loss: 0.9288209974765778, Training Accuracy: 96.875%\n",
      "Epoch 69, Batch 20, Training Loss: 0.9263214081525802, Training Accuracy: 97.34375%\n",
      "Epoch 69, Batch 30, Training Loss: 0.9293975293636322, Training Accuracy: 96.35416666666666%\n",
      "Epoch 69, Batch 40, Training Loss: 0.9312603816390037, Training Accuracy: 96.09375%\n",
      "Epoch 69, Batch 50, Training Loss: 0.9324714636802673, Training Accuracy: 96.0%\n",
      "Epoch 69, Batch 60, Training Loss: 0.9310789773861567, Training Accuracy: 96.25%\n",
      "Epoch 69, Batch 70, Training Loss: 0.9311961795602526, Training Accuracy: 96.07142857142857%\n",
      "Epoch 69, Batch 80, Training Loss: 0.9325094923377037, Training Accuracy: 95.9765625%\n",
      "Epoch 69, Batch 90, Training Loss: 0.9314746724234687, Training Accuracy: 96.18055555555556%\n",
      "Epoch 69, Validation Loss: 0.9330675368723662, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 69: Adam lr 0.0000\n",
      "Epoch 70, Batch 10, Training Loss: 0.9277546763420105, Training Accuracy: 96.25%\n",
      "Epoch 70, Batch 20, Training Loss: 0.9353720396757126, Training Accuracy: 95.78125%\n",
      "Epoch 70, Batch 30, Training Loss: 0.9313546081384023, Training Accuracy: 96.04166666666667%\n",
      "Epoch 70, Batch 40, Training Loss: 0.9316804215312005, Training Accuracy: 95.859375%\n",
      "Epoch 70, Batch 50, Training Loss: 0.9319226312637329, Training Accuracy: 96.0%\n",
      "Epoch 70, Batch 60, Training Loss: 0.9330151051282882, Training Accuracy: 96.19791666666667%\n",
      "Epoch 70, Batch 70, Training Loss: 0.9338121388639723, Training Accuracy: 96.20535714285714%\n",
      "Epoch 70, Batch 80, Training Loss: 0.9331569448113441, Training Accuracy: 96.328125%\n",
      "Epoch 70, Batch 90, Training Loss: 0.9318923897213406, Training Accuracy: 96.38888888888889%\n",
      "Epoch 70, Validation Loss: 0.9330674487611522, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 70: Adam lr 0.0000\n",
      "Epoch 71, Batch 10, Training Loss: 0.9214565932750702, Training Accuracy: 96.875%\n",
      "Epoch 71, Batch 20, Training Loss: 0.9256103426218033, Training Accuracy: 96.40625%\n",
      "Epoch 71, Batch 30, Training Loss: 0.9260760486125946, Training Accuracy: 96.77083333333333%\n",
      "Epoch 71, Batch 40, Training Loss: 0.9316601008176804, Training Accuracy: 96.71875%\n",
      "Epoch 71, Batch 50, Training Loss: 0.9325817835330963, Training Accuracy: 96.5%\n",
      "Epoch 71, Batch 60, Training Loss: 0.9326013008753459, Training Accuracy: 96.35416666666666%\n",
      "Epoch 71, Batch 70, Training Loss: 0.9324489431721824, Training Accuracy: 96.20535714285714%\n",
      "Epoch 71, Batch 80, Training Loss: 0.9310440212488175, Training Accuracy: 96.4453125%\n",
      "Epoch 71, Batch 90, Training Loss: 0.9318336089452107, Training Accuracy: 96.25%\n",
      "Epoch 71, Validation Loss: 0.9330673451009004, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 71: Adam lr 0.0000\n",
      "Epoch 72, Batch 10, Training Loss: 0.9376035869121552, Training Accuracy: 95.0%\n",
      "Epoch 72, Batch 20, Training Loss: 0.9353794276714325, Training Accuracy: 95.3125%\n",
      "Epoch 72, Batch 30, Training Loss: 0.9344273865222931, Training Accuracy: 95.625%\n",
      "Epoch 72, Batch 40, Training Loss: 0.9339309081435203, Training Accuracy: 95.78125%\n",
      "Epoch 72, Batch 50, Training Loss: 0.9302251243591309, Training Accuracy: 96.25%\n",
      "Epoch 72, Batch 60, Training Loss: 0.9307625899712245, Training Accuracy: 96.25%\n",
      "Epoch 72, Batch 70, Training Loss: 0.9301530948707035, Training Accuracy: 96.25%\n",
      "Epoch 72, Batch 80, Training Loss: 0.9306503288447857, Training Accuracy: 96.25%\n",
      "Epoch 72, Batch 90, Training Loss: 0.9315392070346409, Training Accuracy: 96.18055555555556%\n",
      "Epoch 72, Validation Loss: 0.9330672751302305, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 72: Adam lr 0.0000\n",
      "Epoch 73, Batch 10, Training Loss: 0.9398993015289306, Training Accuracy: 94.375%\n",
      "Epoch 73, Batch 20, Training Loss: 0.9329666912555694, Training Accuracy: 95.46875%\n",
      "Epoch 73, Batch 30, Training Loss: 0.9322180807590484, Training Accuracy: 96.14583333333333%\n",
      "Epoch 73, Batch 40, Training Loss: 0.9343721985816955, Training Accuracy: 96.171875%\n",
      "Epoch 73, Batch 50, Training Loss: 0.9329289388656616, Training Accuracy: 96.4375%\n",
      "Epoch 73, Batch 60, Training Loss: 0.9302897046009699, Training Accuracy: 96.71875%\n",
      "Epoch 73, Batch 70, Training Loss: 0.9319586864539555, Training Accuracy: 96.38392857142857%\n",
      "Epoch 73, Batch 80, Training Loss: 0.932050060480833, Training Accuracy: 96.2890625%\n",
      "Epoch 73, Batch 90, Training Loss: 0.9321531746122572, Training Accuracy: 96.14583333333333%\n",
      "Epoch 73, Validation Loss: 0.9330672751302305, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 73: Adam lr 0.0000\n",
      "Epoch 74, Batch 10, Training Loss: 0.929602861404419, Training Accuracy: 95.9375%\n",
      "Epoch 74, Batch 20, Training Loss: 0.9264617472887039, Training Accuracy: 96.71875%\n",
      "Epoch 74, Batch 30, Training Loss: 0.9274881521860758, Training Accuracy: 96.66666666666667%\n",
      "Epoch 74, Batch 40, Training Loss: 0.9298001259565354, Training Accuracy: 96.640625%\n",
      "Epoch 74, Batch 50, Training Loss: 0.9307434785366059, Training Accuracy: 96.25%\n",
      "Epoch 74, Batch 60, Training Loss: 0.9307614147663117, Training Accuracy: 96.19791666666667%\n",
      "Epoch 74, Batch 70, Training Loss: 0.9286495072501046, Training Accuracy: 96.42857142857143%\n",
      "Epoch 74, Batch 80, Training Loss: 0.929232818633318, Training Accuracy: 96.40625%\n",
      "Epoch 74, Batch 90, Training Loss: 0.9307256115807427, Training Accuracy: 96.25%\n",
      "Epoch 74, Validation Loss: 0.9330672258916108, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 74: Adam lr 0.0000\n",
      "Epoch 75, Batch 10, Training Loss: 0.9221791505813599, Training Accuracy: 97.1875%\n",
      "Epoch 75, Batch 20, Training Loss: 0.9252510398626328, Training Accuracy: 97.03125%\n",
      "Epoch 75, Batch 30, Training Loss: 0.9279938677946726, Training Accuracy: 96.35416666666666%\n",
      "Epoch 75, Batch 40, Training Loss: 0.9304303973913193, Training Accuracy: 96.09375%\n",
      "Epoch 75, Batch 50, Training Loss: 0.9304102909564972, Training Accuracy: 96.0625%\n",
      "Epoch 75, Batch 60, Training Loss: 0.9318243980407714, Training Accuracy: 95.88541666666667%\n",
      "Epoch 75, Batch 70, Training Loss: 0.9324446243899209, Training Accuracy: 95.89285714285715%\n",
      "Epoch 75, Batch 80, Training Loss: 0.9309383146464825, Training Accuracy: 96.1328125%\n",
      "Epoch 75, Batch 90, Training Loss: 0.9305112573835584, Training Accuracy: 96.25%\n",
      "Epoch 75, Validation Loss: 0.9330671662869661, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 75: Adam lr 0.0000\n",
      "Epoch 76, Batch 10, Training Loss: 0.9213754296302795, Training Accuracy: 97.8125%\n",
      "Epoch 76, Batch 20, Training Loss: 0.9235371738672257, Training Accuracy: 97.34375%\n",
      "Epoch 76, Batch 30, Training Loss: 0.9312890887260437, Training Accuracy: 96.45833333333333%\n",
      "Epoch 76, Batch 40, Training Loss: 0.9331621065735817, Training Accuracy: 96.171875%\n",
      "Epoch 76, Batch 50, Training Loss: 0.9334824275970459, Training Accuracy: 95.9375%\n",
      "Epoch 76, Batch 60, Training Loss: 0.9321872572104136, Training Accuracy: 96.14583333333333%\n",
      "Epoch 76, Batch 70, Training Loss: 0.931183774130685, Training Accuracy: 96.11607142857143%\n",
      "Epoch 76, Batch 80, Training Loss: 0.9317104957997799, Training Accuracy: 96.09375%\n",
      "Epoch 76, Batch 90, Training Loss: 0.9313012619813283, Training Accuracy: 96.11111111111111%\n",
      "Epoch 76, Validation Loss: 0.9330671818360038, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 76: Adam lr 0.0000\n",
      "Epoch 77, Batch 10, Training Loss: 0.9306753516197205, Training Accuracy: 96.25%\n",
      "Epoch 77, Batch 20, Training Loss: 0.9277858167886734, Training Accuracy: 97.03125%\n",
      "Epoch 77, Batch 30, Training Loss: 0.9262870132923127, Training Accuracy: 96.875%\n",
      "Epoch 77, Batch 40, Training Loss: 0.9301669269800186, Training Accuracy: 96.40625%\n",
      "Epoch 77, Batch 50, Training Loss: 0.9277426016330719, Training Accuracy: 96.6875%\n",
      "Epoch 77, Batch 60, Training Loss: 0.9277320603529612, Training Accuracy: 96.61458333333334%\n",
      "Epoch 77, Batch 70, Training Loss: 0.9278886369296483, Training Accuracy: 96.51785714285714%\n",
      "Epoch 77, Batch 80, Training Loss: 0.928922425955534, Training Accuracy: 96.4453125%\n",
      "Epoch 77, Batch 90, Training Loss: 0.9298640496200985, Training Accuracy: 96.28472222222221%\n",
      "Epoch 77, Validation Loss: 0.9330671611039535, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 77: Adam lr 0.0000\n",
      "Epoch 78, Batch 10, Training Loss: 0.9325223624706268, Training Accuracy: 95.9375%\n",
      "Epoch 78, Batch 20, Training Loss: 0.9367036670446396, Training Accuracy: 95.0%\n",
      "Epoch 78, Batch 30, Training Loss: 0.935369215408961, Training Accuracy: 95.72916666666667%\n",
      "Epoch 78, Batch 40, Training Loss: 0.9361441537737847, Training Accuracy: 95.546875%\n",
      "Epoch 78, Batch 50, Training Loss: 0.9356420505046844, Training Accuracy: 95.5625%\n",
      "Epoch 78, Batch 60, Training Loss: 0.9332433859507243, Training Accuracy: 95.9375%\n",
      "Epoch 78, Batch 70, Training Loss: 0.9315804217542921, Training Accuracy: 96.20535714285714%\n",
      "Epoch 78, Batch 80, Training Loss: 0.9305465415120124, Training Accuracy: 96.25%\n",
      "Epoch 78, Batch 90, Training Loss: 0.9295242773161994, Training Accuracy: 96.28472222222221%\n",
      "Epoch 78, Validation Loss: 0.9330671144568402, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 78: Adam lr 0.0000\n",
      "Epoch 79, Batch 10, Training Loss: 0.9303244173526763, Training Accuracy: 95.9375%\n",
      "Epoch 79, Batch 20, Training Loss: 0.9256145387887955, Training Accuracy: 96.71875%\n",
      "Epoch 79, Batch 30, Training Loss: 0.9308695991834005, Training Accuracy: 96.04166666666667%\n",
      "Epoch 79, Batch 40, Training Loss: 0.9300432905554772, Training Accuracy: 96.328125%\n",
      "Epoch 79, Batch 50, Training Loss: 0.9300240886211395, Training Accuracy: 96.375%\n",
      "Epoch 79, Batch 60, Training Loss: 0.9283810138702393, Training Accuracy: 96.51041666666667%\n",
      "Epoch 79, Batch 70, Training Loss: 0.9284574253218515, Training Accuracy: 96.47321428571428%\n",
      "Epoch 79, Batch 80, Training Loss: 0.9301633007824421, Training Accuracy: 96.25%\n",
      "Epoch 79, Batch 90, Training Loss: 0.9305148601531983, Training Accuracy: 96.21527777777777%\n",
      "Epoch 79, Validation Loss: 0.9330670704012332, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 79: Adam lr 0.0000\n",
      "Epoch 80, Batch 10, Training Loss: 0.9372441709041596, Training Accuracy: 95.9375%\n",
      "Epoch 80, Batch 20, Training Loss: 0.9330098092556, Training Accuracy: 96.40625%\n",
      "Epoch 80, Batch 30, Training Loss: 0.938230554262797, Training Accuracy: 95.83333333333334%\n",
      "Epoch 80, Batch 40, Training Loss: 0.935546089708805, Training Accuracy: 96.09375%\n",
      "Epoch 80, Batch 50, Training Loss: 0.9357420361042023, Training Accuracy: 96.0%\n",
      "Epoch 80, Batch 60, Training Loss: 0.9331552257140477, Training Accuracy: 96.30208333333333%\n",
      "Epoch 80, Batch 70, Training Loss: 0.9329933745520456, Training Accuracy: 96.29464285714285%\n",
      "Epoch 80, Batch 80, Training Loss: 0.9325582049787045, Training Accuracy: 96.2890625%\n",
      "Epoch 80, Batch 90, Training Loss: 0.93176256219546, Training Accuracy: 96.31944444444444%\n",
      "Epoch 80, Validation Loss: 0.9330670574437017, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 80: Adam lr 0.0000\n",
      "Epoch 81, Batch 10, Training Loss: 0.9283250212669373, Training Accuracy: 97.5%\n",
      "Epoch 81, Batch 20, Training Loss: 0.9341444075107574, Training Accuracy: 96.25%\n",
      "Epoch 81, Batch 30, Training Loss: 0.9343252917130788, Training Accuracy: 96.04166666666667%\n",
      "Epoch 81, Batch 40, Training Loss: 0.936948861181736, Training Accuracy: 95.78125%\n",
      "Epoch 81, Batch 50, Training Loss: 0.9351051080226899, Training Accuracy: 96.125%\n",
      "Epoch 81, Batch 60, Training Loss: 0.933669971426328, Training Accuracy: 96.25%\n",
      "Epoch 81, Batch 70, Training Loss: 0.9354022698743003, Training Accuracy: 96.07142857142857%\n",
      "Epoch 81, Batch 80, Training Loss: 0.9334363706409932, Training Accuracy: 96.25%\n",
      "Epoch 81, Batch 90, Training Loss: 0.9332578466998206, Training Accuracy: 96.25%\n",
      "Epoch 81, Validation Loss: 0.9330670004305632, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 81: Adam lr 0.0000\n",
      "Epoch 82, Batch 10, Training Loss: 0.929975175857544, Training Accuracy: 96.25%\n",
      "Epoch 82, Batch 20, Training Loss: 0.9295199513435364, Training Accuracy: 96.40625%\n",
      "Epoch 82, Batch 30, Training Loss: 0.9276303033034007, Training Accuracy: 96.45833333333333%\n",
      "Epoch 82, Batch 40, Training Loss: 0.9293217241764069, Training Accuracy: 96.171875%\n",
      "Epoch 82, Batch 50, Training Loss: 0.9316431164741517, Training Accuracy: 96.0%\n",
      "Epoch 82, Batch 60, Training Loss: 0.9333272765080134, Training Accuracy: 95.83333333333334%\n",
      "Epoch 82, Batch 70, Training Loss: 0.9334313452243805, Training Accuracy: 95.89285714285715%\n",
      "Epoch 82, Batch 80, Training Loss: 0.9308546349406243, Training Accuracy: 96.25%\n",
      "Epoch 82, Batch 90, Training Loss: 0.9323666618929969, Training Accuracy: 96.14583333333333%\n",
      "Epoch 82, Validation Loss: 0.9330670211626135, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 82: Adam lr 0.0000\n",
      "Epoch 83, Batch 10, Training Loss: 0.9204686343669891, Training Accuracy: 96.875%\n",
      "Epoch 83, Batch 20, Training Loss: 0.9281426250934601, Training Accuracy: 97.03125%\n",
      "Epoch 83, Batch 30, Training Loss: 0.9287430942058563, Training Accuracy: 96.875%\n",
      "Epoch 83, Batch 40, Training Loss: 0.9312328442931175, Training Accuracy: 96.328125%\n",
      "Epoch 83, Batch 50, Training Loss: 0.9305742406845092, Training Accuracy: 96.5%\n",
      "Epoch 83, Batch 60, Training Loss: 0.9302355329195658, Training Accuracy: 96.51041666666667%\n",
      "Epoch 83, Batch 70, Training Loss: 0.9299723199435643, Training Accuracy: 96.60714285714286%\n",
      "Epoch 83, Batch 80, Training Loss: 0.9291872106492519, Training Accuracy: 96.6015625%\n",
      "Epoch 83, Batch 90, Training Loss: 0.9315045197804769, Training Accuracy: 96.28472222222221%\n",
      "Epoch 83, Validation Loss: 0.9330670082050821, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 83: Adam lr 0.0000\n",
      "Epoch 84, Batch 10, Training Loss: 0.949316930770874, Training Accuracy: 94.375%\n",
      "Epoch 84, Batch 20, Training Loss: 0.941838663816452, Training Accuracy: 94.84375%\n",
      "Epoch 84, Batch 30, Training Loss: 0.9348937074343363, Training Accuracy: 95.83333333333334%\n",
      "Epoch 84, Batch 40, Training Loss: 0.9326381668448448, Training Accuracy: 96.015625%\n",
      "Epoch 84, Batch 50, Training Loss: 0.9309196209907532, Training Accuracy: 96.25%\n",
      "Epoch 84, Batch 60, Training Loss: 0.9311586221059164, Training Accuracy: 96.25%\n",
      "Epoch 84, Batch 70, Training Loss: 0.9293101549148559, Training Accuracy: 96.5625%\n",
      "Epoch 84, Batch 80, Training Loss: 0.9296618707478046, Training Accuracy: 96.484375%\n",
      "Epoch 84, Batch 90, Training Loss: 0.9307112640804714, Training Accuracy: 96.35416666666666%\n",
      "Epoch 84, Validation Loss: 0.9330670211626135, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 84: Adam lr 0.0000\n",
      "Epoch 85, Batch 10, Training Loss: 0.9258462727069855, Training Accuracy: 97.5%\n",
      "Epoch 85, Batch 20, Training Loss: 0.9257097184658051, Training Accuracy: 97.1875%\n",
      "Epoch 85, Batch 30, Training Loss: 0.9238369643688202, Training Accuracy: 97.39583333333334%\n",
      "Epoch 85, Batch 40, Training Loss: 0.9298906996846199, Training Accuracy: 96.5625%\n",
      "Epoch 85, Batch 50, Training Loss: 0.9277947747707367, Training Accuracy: 96.875%\n",
      "Epoch 85, Batch 60, Training Loss: 0.927446848154068, Training Accuracy: 96.875%\n",
      "Epoch 85, Batch 70, Training Loss: 0.9279562030519758, Training Accuracy: 96.69642857142857%\n",
      "Epoch 85, Batch 80, Training Loss: 0.9310322605073452, Training Accuracy: 96.3671875%\n",
      "Epoch 85, Batch 90, Training Loss: 0.9321747574541304, Training Accuracy: 96.28472222222221%\n",
      "Epoch 85, Validation Loss: 0.9330669926560443, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 85: Adam lr 0.0000\n",
      "Epoch 86, Batch 10, Training Loss: 0.9389314591884613, Training Accuracy: 95.0%\n",
      "Epoch 86, Batch 20, Training Loss: 0.9341584950685501, Training Accuracy: 95.78125%\n",
      "Epoch 86, Batch 30, Training Loss: 0.9278748671213786, Training Accuracy: 96.45833333333333%\n",
      "Epoch 86, Batch 40, Training Loss: 0.9260183870792389, Training Accuracy: 96.5625%\n",
      "Epoch 86, Batch 50, Training Loss: 0.9294591403007507, Training Accuracy: 96.25%\n",
      "Epoch 86, Batch 60, Training Loss: 0.9311157455046971, Training Accuracy: 96.14583333333333%\n",
      "Epoch 86, Batch 70, Training Loss: 0.930974509034838, Training Accuracy: 96.11607142857143%\n",
      "Epoch 86, Batch 80, Training Loss: 0.9302537277340889, Training Accuracy: 96.25%\n",
      "Epoch 86, Batch 90, Training Loss: 0.9297439184453752, Training Accuracy: 96.31944444444444%\n",
      "Epoch 86, Validation Loss: 0.9330669745155002, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 86: Adam lr 0.0000\n",
      "Epoch 87, Batch 10, Training Loss: 0.9389092683792114, Training Accuracy: 95.3125%\n",
      "Epoch 87, Batch 20, Training Loss: 0.9303629606962204, Training Accuracy: 96.25%\n",
      "Epoch 87, Batch 30, Training Loss: 0.9366267025470734, Training Accuracy: 95.83333333333334%\n",
      "Epoch 87, Batch 40, Training Loss: 0.9338551238179207, Training Accuracy: 95.9375%\n",
      "Epoch 87, Batch 50, Training Loss: 0.9314346635341644, Training Accuracy: 96.25%\n",
      "Epoch 87, Batch 60, Training Loss: 0.9330706894397736, Training Accuracy: 96.19791666666667%\n",
      "Epoch 87, Batch 70, Training Loss: 0.9308519959449768, Training Accuracy: 96.42857142857143%\n",
      "Epoch 87, Batch 80, Training Loss: 0.9321806125342846, Training Accuracy: 96.25%\n",
      "Epoch 87, Batch 90, Training Loss: 0.9318260702821943, Training Accuracy: 96.28472222222221%\n",
      "Epoch 87, Validation Loss: 0.9330669589664625, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 87: Adam lr 0.0000\n",
      "Epoch 88, Batch 10, Training Loss: 0.9379934787750244, Training Accuracy: 96.25%\n",
      "Epoch 88, Batch 20, Training Loss: 0.9380943328142166, Training Accuracy: 95.46875%\n",
      "Epoch 88, Batch 30, Training Loss: 0.9344092408816019, Training Accuracy: 95.625%\n",
      "Epoch 88, Batch 40, Training Loss: 0.9328124701976777, Training Accuracy: 95.625%\n",
      "Epoch 88, Batch 50, Training Loss: 0.9343528532981873, Training Accuracy: 95.75%\n",
      "Epoch 88, Batch 60, Training Loss: 0.9312471548716227, Training Accuracy: 96.19791666666667%\n",
      "Epoch 88, Batch 70, Training Loss: 0.9308962064129965, Training Accuracy: 96.20535714285714%\n",
      "Epoch 88, Batch 80, Training Loss: 0.9307035259902477, Training Accuracy: 96.25%\n",
      "Epoch 88, Batch 90, Training Loss: 0.9303090790907542, Training Accuracy: 96.21527777777777%\n",
      "Epoch 88, Validation Loss: 0.9330669071363367, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 88: Adam lr 0.0000\n",
      "Epoch 89, Batch 10, Training Loss: 0.9293463826179504, Training Accuracy: 96.5625%\n",
      "Epoch 89, Batch 20, Training Loss: 0.925938805937767, Training Accuracy: 96.875%\n",
      "Epoch 89, Batch 30, Training Loss: 0.9295596480369568, Training Accuracy: 96.45833333333333%\n",
      "Epoch 89, Batch 40, Training Loss: 0.9295891970396042, Training Accuracy: 96.484375%\n",
      "Epoch 89, Batch 50, Training Loss: 0.9288623380661011, Training Accuracy: 96.4375%\n",
      "Epoch 89, Batch 60, Training Loss: 0.9285934547583262, Training Accuracy: 96.40625%\n",
      "Epoch 89, Batch 70, Training Loss: 0.9311723138604845, Training Accuracy: 96.29464285714285%\n",
      "Epoch 89, Batch 80, Training Loss: 0.9313725546002388, Training Accuracy: 96.25%\n",
      "Epoch 89, Batch 90, Training Loss: 0.9314892053604126, Training Accuracy: 96.28472222222221%\n",
      "Epoch 89, Validation Loss: 0.9330668993618177, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 89: Adam lr 0.0000\n",
      "Epoch 90, Batch 10, Training Loss: 0.9280915439128876, Training Accuracy: 95.625%\n",
      "Epoch 90, Batch 20, Training Loss: 0.9320320606231689, Training Accuracy: 95.15625%\n",
      "Epoch 90, Batch 30, Training Loss: 0.9282687306404114, Training Accuracy: 95.9375%\n",
      "Epoch 90, Batch 40, Training Loss: 0.9272354215383529, Training Accuracy: 96.484375%\n",
      "Epoch 90, Batch 50, Training Loss: 0.929925035238266, Training Accuracy: 96.125%\n",
      "Epoch 90, Batch 60, Training Loss: 0.9302089631557464, Training Accuracy: 96.14583333333333%\n",
      "Epoch 90, Batch 70, Training Loss: 0.9313545959336417, Training Accuracy: 96.16071428571429%\n",
      "Epoch 90, Batch 80, Training Loss: 0.9313500344753265, Training Accuracy: 96.2890625%\n",
      "Epoch 90, Batch 90, Training Loss: 0.9306778411070505, Training Accuracy: 96.35416666666666%\n",
      "Epoch 90, Validation Loss: 0.9330669175023618, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 90: Adam lr 0.0000\n",
      "Epoch 91, Batch 10, Training Loss: 0.9360229909420014, Training Accuracy: 95.9375%\n",
      "Epoch 91, Batch 20, Training Loss: 0.9352310568094253, Training Accuracy: 95.78125%\n",
      "Epoch 91, Batch 30, Training Loss: 0.9309936503569285, Training Accuracy: 96.25%\n",
      "Epoch 91, Batch 40, Training Loss: 0.9279716715216637, Training Accuracy: 96.640625%\n",
      "Epoch 91, Batch 50, Training Loss: 0.9286683833599091, Training Accuracy: 96.4375%\n",
      "Epoch 91, Batch 60, Training Loss: 0.9291404594977697, Training Accuracy: 96.5625%\n",
      "Epoch 91, Batch 70, Training Loss: 0.9307717817170279, Training Accuracy: 96.29464285714285%\n",
      "Epoch 91, Batch 80, Training Loss: 0.9295852929353714, Training Accuracy: 96.4453125%\n",
      "Epoch 91, Batch 90, Training Loss: 0.9308062334855397, Training Accuracy: 96.28472222222221%\n",
      "Epoch 91, Validation Loss: 0.9330669304598933, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 91: Adam lr 0.0000\n",
      "Epoch 92, Batch 10, Training Loss: 0.9253747701644898, Training Accuracy: 96.875%\n",
      "Epoch 92, Batch 20, Training Loss: 0.932416045665741, Training Accuracy: 95.46875%\n",
      "Epoch 92, Batch 30, Training Loss: 0.9319143831729889, Training Accuracy: 95.72916666666667%\n",
      "Epoch 92, Batch 40, Training Loss: 0.932414884865284, Training Accuracy: 96.015625%\n",
      "Epoch 92, Batch 50, Training Loss: 0.932895268201828, Training Accuracy: 96.1875%\n",
      "Epoch 92, Batch 60, Training Loss: 0.9321248690287273, Training Accuracy: 96.30208333333333%\n",
      "Epoch 92, Batch 70, Training Loss: 0.9316881903580256, Training Accuracy: 96.25%\n",
      "Epoch 92, Batch 80, Training Loss: 0.9313553318381309, Training Accuracy: 96.25%\n",
      "Epoch 92, Batch 90, Training Loss: 0.9307073487175835, Training Accuracy: 96.25%\n",
      "Epoch 92, Validation Loss: 0.9330668889957926, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 92: Adam lr 0.0000\n",
      "Epoch 93, Batch 10, Training Loss: 0.9281469583511353, Training Accuracy: 96.875%\n",
      "Epoch 93, Batch 20, Training Loss: 0.9350393623113632, Training Accuracy: 95.9375%\n",
      "Epoch 93, Batch 30, Training Loss: 0.9304038921991984, Training Accuracy: 96.45833333333333%\n",
      "Epoch 93, Batch 40, Training Loss: 0.928429989516735, Training Accuracy: 96.640625%\n",
      "Epoch 93, Batch 50, Training Loss: 0.9296223795413971, Training Accuracy: 96.5625%\n",
      "Epoch 93, Batch 60, Training Loss: 0.9296326180299123, Training Accuracy: 96.61458333333334%\n",
      "Epoch 93, Batch 70, Training Loss: 0.9295053328786578, Training Accuracy: 96.65178571428571%\n",
      "Epoch 93, Batch 80, Training Loss: 0.9287908338010311, Training Accuracy: 96.6796875%\n",
      "Epoch 93, Batch 90, Training Loss: 0.9312467787000868, Training Accuracy: 96.25%\n",
      "Epoch 93, Validation Loss: 0.9330669641494751, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 93: Adam lr 0.0000\n",
      "Epoch 94, Batch 10, Training Loss: 0.9176705539226532, Training Accuracy: 97.8125%\n",
      "Epoch 94, Batch 20, Training Loss: 0.9182085037231446, Training Accuracy: 97.65625%\n",
      "Epoch 94, Batch 30, Training Loss: 0.924085118373235, Training Accuracy: 96.97916666666667%\n",
      "Epoch 94, Batch 40, Training Loss: 0.9319214209914207, Training Accuracy: 96.09375%\n",
      "Epoch 94, Batch 50, Training Loss: 0.93027552485466, Training Accuracy: 96.125%\n",
      "Epoch 94, Batch 60, Training Loss: 0.9311153362194697, Training Accuracy: 96.30208333333333%\n",
      "Epoch 94, Batch 70, Training Loss: 0.9306069229330335, Training Accuracy: 96.38392857142857%\n",
      "Epoch 94, Batch 80, Training Loss: 0.931400216370821, Training Accuracy: 96.2890625%\n",
      "Epoch 94, Batch 90, Training Loss: 0.9313933299647437, Training Accuracy: 96.31944444444444%\n",
      "Epoch 94, Validation Loss: 0.9330669667409814, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 94: Adam lr 0.0000\n",
      "Epoch 95, Batch 10, Training Loss: 0.9470267951488495, Training Accuracy: 95.9375%\n",
      "Epoch 95, Batch 20, Training Loss: 0.9340722143650055, Training Accuracy: 96.71875%\n",
      "Epoch 95, Batch 30, Training Loss: 0.9346183836460114, Training Accuracy: 96.25%\n",
      "Epoch 95, Batch 40, Training Loss: 0.9323978647589684, Training Accuracy: 96.40625%\n",
      "Epoch 95, Batch 50, Training Loss: 0.9328541457653046, Training Accuracy: 96.3125%\n",
      "Epoch 95, Batch 60, Training Loss: 0.9309374163548152, Training Accuracy: 96.51041666666667%\n",
      "Epoch 95, Batch 70, Training Loss: 0.9312173247337341, Training Accuracy: 96.42857142857143%\n",
      "Epoch 95, Batch 80, Training Loss: 0.930226993560791, Training Accuracy: 96.4453125%\n",
      "Epoch 95, Batch 90, Training Loss: 0.9306843280792236, Training Accuracy: 96.31944444444444%\n",
      "Epoch 95, Validation Loss: 0.9330669822900192, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 95: Adam lr 0.0000\n",
      "Epoch 96, Batch 10, Training Loss: 0.9271941542625427, Training Accuracy: 95.9375%\n",
      "Epoch 96, Batch 20, Training Loss: 0.9300842136144638, Training Accuracy: 96.40625%\n",
      "Epoch 96, Batch 30, Training Loss: 0.932367871205012, Training Accuracy: 95.72916666666667%\n",
      "Epoch 96, Batch 40, Training Loss: 0.9320979088544845, Training Accuracy: 96.09375%\n",
      "Epoch 96, Batch 50, Training Loss: 0.9325693941116333, Training Accuracy: 95.9375%\n",
      "Epoch 96, Batch 60, Training Loss: 0.929994582136472, Training Accuracy: 96.25%\n",
      "Epoch 96, Batch 70, Training Loss: 0.9310185432434082, Training Accuracy: 96.16071428571429%\n",
      "Epoch 96, Batch 80, Training Loss: 0.9299349561333656, Training Accuracy: 96.3671875%\n",
      "Epoch 96, Batch 90, Training Loss: 0.9303511811627282, Training Accuracy: 96.25%\n",
      "Epoch 96, Validation Loss: 0.933066946008931, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 96: Adam lr 0.0000\n",
      "Epoch 97, Batch 10, Training Loss: 0.9340501964092255, Training Accuracy: 94.6875%\n",
      "Epoch 97, Batch 20, Training Loss: 0.928855100274086, Training Accuracy: 96.25%\n",
      "Epoch 97, Batch 30, Training Loss: 0.9257564226786296, Training Accuracy: 96.77083333333333%\n",
      "Epoch 97, Batch 40, Training Loss: 0.926775187253952, Training Accuracy: 96.796875%\n",
      "Epoch 97, Batch 50, Training Loss: 0.9283701658248902, Training Accuracy: 96.4375%\n",
      "Epoch 97, Batch 60, Training Loss: 0.9319827179114024, Training Accuracy: 96.19791666666667%\n",
      "Epoch 97, Batch 70, Training Loss: 0.9316195334706988, Training Accuracy: 96.20535714285714%\n",
      "Epoch 97, Batch 80, Training Loss: 0.9315840750932693, Training Accuracy: 96.25%\n",
      "Epoch 97, Batch 90, Training Loss: 0.930556552277671, Training Accuracy: 96.28472222222221%\n",
      "Epoch 97, Validation Loss: 0.9330669589664625, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 97: Adam lr 0.0000\n",
      "Epoch 98, Batch 10, Training Loss: 0.9322308897972107, Training Accuracy: 95.0%\n",
      "Epoch 98, Batch 20, Training Loss: 0.9348727524280548, Training Accuracy: 95.3125%\n",
      "Epoch 98, Batch 30, Training Loss: 0.9335578401883443, Training Accuracy: 95.625%\n",
      "Epoch 98, Batch 40, Training Loss: 0.9319306641817093, Training Accuracy: 96.015625%\n",
      "Epoch 98, Batch 50, Training Loss: 0.931597044467926, Training Accuracy: 96.1875%\n",
      "Epoch 98, Batch 60, Training Loss: 0.9304066141446431, Training Accuracy: 96.35416666666666%\n",
      "Epoch 98, Batch 70, Training Loss: 0.9322681767599923, Training Accuracy: 96.16071428571429%\n",
      "Epoch 98, Batch 80, Training Loss: 0.932851929962635, Training Accuracy: 96.015625%\n",
      "Epoch 98, Batch 90, Training Loss: 0.9317037887043423, Training Accuracy: 96.21527777777777%\n",
      "Epoch 98, Validation Loss: 0.9330669252768807, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 98: Adam lr 0.0000\n",
      "Epoch 99, Batch 10, Training Loss: 0.9280557811260224, Training Accuracy: 97.5%\n",
      "Epoch 99, Batch 20, Training Loss: 0.9358296722173691, Training Accuracy: 96.875%\n",
      "Epoch 99, Batch 30, Training Loss: 0.9359069923559825, Training Accuracy: 96.875%\n",
      "Epoch 99, Batch 40, Training Loss: 0.9361102268099785, Training Accuracy: 96.328125%\n",
      "Epoch 99, Batch 50, Training Loss: 0.9357948863506317, Training Accuracy: 96.125%\n",
      "Epoch 99, Batch 60, Training Loss: 0.9331154733896255, Training Accuracy: 96.40625%\n",
      "Epoch 99, Batch 70, Training Loss: 0.9337662816047668, Training Accuracy: 96.29464285714285%\n",
      "Epoch 99, Batch 80, Training Loss: 0.9334163881838322, Training Accuracy: 96.2109375%\n",
      "Epoch 99, Batch 90, Training Loss: 0.9317116651270124, Training Accuracy: 96.31944444444444%\n",
      "Epoch 99, Validation Loss: 0.9330669226853744, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 99: Adam lr 0.0000\n",
      "Epoch 100, Batch 10, Training Loss: 0.9425758123397827, Training Accuracy: 95.3125%\n",
      "Epoch 100, Batch 20, Training Loss: 0.9354258835315704, Training Accuracy: 96.25%\n",
      "Epoch 100, Batch 30, Training Loss: 0.9341393371423086, Training Accuracy: 95.9375%\n",
      "Epoch 100, Batch 40, Training Loss: 0.9318904384970665, Training Accuracy: 96.171875%\n",
      "Epoch 100, Batch 50, Training Loss: 0.9297944402694702, Training Accuracy: 96.5%\n",
      "Epoch 100, Batch 60, Training Loss: 0.9291843871275584, Training Accuracy: 96.5625%\n",
      "Epoch 100, Batch 70, Training Loss: 0.9274379823889051, Training Accuracy: 96.78571428571429%\n",
      "Epoch 100, Batch 80, Training Loss: 0.9297142907977104, Training Accuracy: 96.40625%\n",
      "Epoch 100, Batch 90, Training Loss: 0.9320953256554074, Training Accuracy: 96.14583333333333%\n",
      "Epoch 100, Validation Loss: 0.9330669511919436, Validation Accuracy: 96.18528610354224%\n",
      "Epoch 100: Adam lr 0.0000\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for i, (x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \n",
    "        x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \n",
    "        x10_batch, x11_batch, x12_batch, y_batch) in enumerate(train_dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device\n",
    "        x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \\\n",
    "        x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \\\n",
    "        x10_batch, x11_batch, x12_batch, y_batch = (\n",
    "            x0_batch.to(device),\n",
    "            x1_batch.to(device),\n",
    "            x2_batch.to(device),\n",
    "            x3_batch.to(device),\n",
    "            x4_batch.to(device),\n",
    "            x5_batch.to(device),\n",
    "            x6_batch.to(device),\n",
    "            x7_batch.to(device),\n",
    "            x8_batch.to(device),\n",
    "            x9_batch.to(device),\n",
    "            x10_batch.to(device),\n",
    "            x11_batch.to(device),\n",
    "            x12_batch.to(device),\n",
    "            y_batch.to(device)\n",
    "        )\n",
    "        \n",
    "        outputs = model(x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \n",
    "                         x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \n",
    "                         x10_batch, x11_batch, x12_batch)\n",
    "        \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        correct_train += (predicted == y_batch).sum().item()\n",
    "\n",
    "        if i % 10 == 9:  \n",
    "            print(f\"Epoch {epoch + 1}, Batch {i + 1}, Training Loss: {running_loss / (i + 1)}, Training Accuracy: {(correct_train / total_train) * 100}%\")\n",
    "            \n",
    "    train_epoch_loss = running_loss / len(train_dataloader)\n",
    "    train_epoch_accuracy = (correct_train / total_train) * 100\n",
    "\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    train_accuracies.append(train_epoch_accuracy)\n",
    "\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x0_val, x1_val, x2_val, x3_val, x4_val, x5_val, x6_val, \\\n",
    "            x7_val, x8_val, x9_val, x10_val, x11_val, x12_val, y_val in test_dataloader:\n",
    "            \n",
    "            x0_val, x1_val, x2_val, x3_val, x4_val, \\\n",
    "            x5_val, x6_val, x7_val, x8_val, x9_val, \\\n",
    "            x10_val, x11_val, x12_val, y_val = (\n",
    "                x0_val.to(device),\n",
    "                x1_val.to(device),\n",
    "                x2_val.to(device),\n",
    "                x3_val.to(device),\n",
    "                x4_val.to(device),\n",
    "                x5_val.to(device),\n",
    "                x6_val.to(device),\n",
    "                x7_val.to(device),\n",
    "                x8_val.to(device),\n",
    "                x9_val.to(device),\n",
    "                x10_val.to(device),\n",
    "                x11_val.to(device),\n",
    "                x12_val.to(device),\n",
    "                y_val.to(device)\n",
    "            )\n",
    "            \n",
    "            outputs = model(x0_val, x1_val, x2_val, x3_val, x4_val, \n",
    "                             x5_val, x6_val, x7_val, x8_val, x9_val, \n",
    "                             x10_val, x11_val, x12_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += y_val.size(0)\n",
    "            correct_val += (predicted == y_val).sum().item()\n",
    "\n",
    "    scheduler.step(val_running_loss)\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(test_dataloader)\n",
    "    val_epoch_accuracy = (correct_val / total_val) * 100\n",
    "\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {val_epoch_loss}, Validation Accuracy: {val_epoch_accuracy}%\")\n",
    "    print(f\"Epoch {epoch + 1}: Adam lr {optimizer.param_groups[0]['lr']:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABMU0lEQVR4nO3deXhU9d3+8feZNfsKJAQSFgVZhBhBKGIFBAvRUhVarVLFKlosoEhtlQcX0EexVatWrdb6CD9alWovwb0IiCK4gRCEgiiCJELCnpVkMpk5vz8mGRKBmISZOSTcr+uaK5kzZ2a+cwjk5vPdDNM0TURERETaCJvVDRAREREJJYUbERERaVMUbkRERKRNUbgRERGRNkXhRkRERNoUhRsRERFpUxRuREREpE1xWN2ASPP7/ezevZv4+HgMw7C6OSIiItIEpmlSVlZGRkYGNlvjtZlTLtzs3r2bzMxMq5shIiIiLVBQUEDnzp0bPeeUCzfx8fFA4OIkJCRY3BoRERFpitLSUjIzM4O/xxtzyoWbuq6ohIQEhRsREZFWpilDSjSgWERERNoUhRsRERFpUywNNytXrmTs2LFkZGRgGAaLFy9u9Pz3338fwzCOuhUVFUWmwSIiInLSs3TMTUVFBdnZ2Vx33XWMGzeuyc/bunVrg/EyHTp0CEfzRETkOHw+H16v1+pmSBvjcrl+cJp3U1gabnJzc8nNzW328zp06EBSUlLoGyQiIo0yTZOioiKKi4utboq0QTabjW7duuFyuU7odVrlbKmzzjoLj8fDmWeeyezZsxk6dOhxz/V4PHg8nuD90tLSSDRRRKRNqgs2HTp0ICYmRouhSsjULbJbWFhIVlbWCf1stapw07FjR5555hkGDhyIx+PhueeeY/jw4Xz66aecffbZx3zO3LlzmTNnToRbKiLS9vh8vmCwSU1Ntbo50ga1b9+e3bt3U1NTg9PpbPHrGKZpmiFsV4sZhsGiRYu49NJLm/W8YcOGkZWVxT/+8Y9jPn6syk1mZiYlJSVa50ZEpBmqqqrYsWMHXbt2JTo62urmSBtUWVnJt99+S7du3YiKimrwWGlpKYmJiU36/d2qKjfHMmjQIFatWnXcx91uN263O4ItEhFp29QVJeESqp+tVr/OTV5eHh07drS6GSIiInKSsLRyU15ezrZt24L3d+zYQV5eHikpKWRlZTFz5kx27drFggULAHjsscfo1q0bffv2paqqiueee4733nuPd99916qPICIiIicZSys3a9euJScnh5ycHABmzJhBTk4Od999NwCFhYXk5+cHz6+uruZ3v/sd/fr1Y9iwYWzYsIFly5YxcuRIS9ovIiKnpq5du/LYY481+fy6RWg1hT4yTpoBxZHSnAFJzeHzm+wprcLnN8lMiQnZ64qInCzqBhQfa7DnyeqHxnDcc889zJ49u9mvu2/fPmJjY4mJadq/99XV1Rw8eJC0tLSwjll6//33GTFiBIcOHWqV68E19jN2Sg0oPlnsK/Nw7oPv4bAZbHvgIqubIyIiBHoA6vzrX//i7rvvZuvWrcFjcXFxwe9N08Tn8+Fw/PCvxvbt2zerHS6Xi/T09GY9R1qu1Q8oPlk47YEkXuM3OcWKYSJyCjNNk8PVNRG/NfXf2fT09OAtMTERwzCC97/88kvi4+N55513GDBgAG63m1WrVvHNN99wySWXkJaWRlxcHOeccw7Lli1r8Lrf75YyDIPnnnuOyy67jJiYGHr06MHrr78efPz73VLz588nKSmJJUuW0Lt3b+Li4hgzZkyDMFZTU8PNN99MUlISqamp3H777UycOLHZS6bUd+jQIa655hqSk5OJiYkhNzeXr7/+Ovj4zp07GTt2LMnJycTGxtK3b1/efvvt4HMnTJhA+/btiY6OpkePHsybN6/FbQknVW5CxGE/khO9PhOXQ1MlRaTtq/T66HP3koi/7+Z7RxPjCs2vsDvuuIOHH36Y7t27k5ycTEFBARdddBH3338/brebBQsWMHbsWLZu3UpWVtZxX2fOnDn86U9/4qGHHuKJJ55gwoQJ7Ny5k5SUlGOef/jwYR5++GH+8Y9/YLPZ+NWvfsVtt93GCy+8AMAf//hHXnjhBebNm0fv3r15/PHHWbx4MSNGjGjxZ7322mv5+uuvef3110lISOD222/noosuYvPmzTidTqZMmUJ1dTUrV64kNjaWzZs3B6tbd911F5s3b+add96hXbt2bNu2jcrKyha3JZwUbkKkrnIDUOP341JRTESkVbj33nu58MILg/dTUlLIzs4O3r/vvvtYtGgRr7/+OlOnTj3u61x77bVceeWVADzwwAP85S9/4bPPPmPMmDHHPN/r9fLMM89w2mmnATB16lTuvffe4ONPPPEEM2fO5LLLLgPgySefDFZRWqIu1KxevZpzzz0XgBdeeIHMzEwWL17ML37xC/Lz8xk/fjz9+vUDoHv37sHn5+fnk5OTw8CBA4FA9epkpXATIs76lZsaE05szy8RkVYh2mln872jLXnfUKn7ZV2nvLyc2bNn89Zbb1FYWEhNTQ2VlZUNZu8eS//+/YPfx8bGkpCQwN69e497fkxMTDDYQGCLobrzS0pK2LNnD4MGDQo+brfbGTBgAH6/v1mfr86WLVtwOBwMHjw4eCw1NZUzzjiDLVu2AHDzzTdz00038e677zJq1CjGjx8f/Fw33XQT48ePZ926dfzkJz/h0ksvDYakk43KCyHisB2p3Hhb+IMnItLaGIZBjMsR8VsoZxzFxsY2uH/bbbexaNEiHnjgAT788EPy8vLo168f1dXVjb7O9/dCMgyj0SByrPOtHrM5adIktm/fztVXX83GjRsZOHAgTzzxBAC5ubns3LmTW2+9ld27dzNy5Ehuu+02S9t7PAo3IWIYRjDg1Pg0oFhEpLVavXo11157LZdddhn9+vUjPT2db7/9NqJtSExMJC0tjTVr1gSP+Xw+1q1b1+LX7N27NzU1NXz66afBYwcOHGDr1q306dMneCwzM5PJkyfz6quv8rvf/Y6///3vwcfat2/PxIkT+ec//8ljjz3Gs88+2+L2hJO6pULIabdR4/fh9alyIyLSWvXo0YNXX32VsWPHYhgGd911V4u7gk7EtGnTmDt3Lqeffjq9evXiiSee4NChQ02qWm3cuJH4+PjgfcMwyM7O5pJLLuGGG27gb3/7G/Hx8dxxxx106tSJSy65BIDp06eTm5tLz549OXToECtWrKB3794A3H333QwYMIC+ffvi8Xh48803g4+dbBRuQshhN8CLwo2ISCv25z//meuuu45zzz2Xdu3acfvtt1NaWhrxdtx+++0UFRVxzTXXYLfbufHGGxk9ejR2+w+PNzr//PMb3Lfb7dTU1DBv3jxuueUWfvrTn1JdXc3555/P22+/Hewi8/l8TJkyhe+++46EhATGjBnDo48+CgTW6pk5cybffvst0dHR/PjHP2bhwoWh/+AhoBWKQ+js+5ZysKKad289n55p8T/8BBGRVqQ1rlDclvj9fnr37s3ll1/OfffdZ3VzwkIrFJ+E6qaDV9eociMiIidm586dvPvuuwwbNgyPx8OTTz7Jjh07uOqqq6xu2klPA4pDyGELXM4a/ylVDBMRkTCw2WzMnz+fc845h6FDh7Jx40aWLVt20o5zOZmochNCLkdtuNGYGxEROUGZmZmsXr3a6ma0SqrchFDdVPBqhRsRERHLKNyEUN3+UlrnRkRExDoKNyHkCu4MrsqNiIiIVRRuQqiuclNdo8qNiIiIVRRuQii4/YIqNyIiIpZRuAmhutlSWqFYRKRtGT58ONOnTw/e79q1K4899lijzzEMg8WLF5/we4fqdU4lCjchVFe58WpAsYjISWHs2LGMGTPmmI99+OGHGIbBF1980ezXXbNmDTfeeOOJNq+B2bNnc9ZZZx11vLCwkNzc3JC+1/fNnz+fpKSksL5HJCnchJBmS4mInFyuv/56li5dynfffXfUY/PmzWPgwIH079+/2a/bvn17YmJiQtHEH5Seno7b7Y7Ie7UVCjch5LKrW0pE5GTy05/+lPbt2zN//vwGx8vLy3nllVe4/vrrOXDgAFdeeSWdOnUiJiaGfv368dJLLzX6ut/vlvr66685//zziYqKok+fPixduvSo59x+++307NmTmJgYunfvzl133YXX6wUClZM5c+awYcMGDMPAMIxgm7/fLbVx40YuuOACoqOjSU1N5cYbb6S8vDz4+LXXXsull17Kww8/TMeOHUlNTWXKlCnB92qJ/Px8LrnkEuLi4khISODyyy9nz549wcc3bNjAiBEjiI+PJyEhgQEDBrB27VogsI3E2LFjSU5OJjY2lr59+/L222+3uC1NoRWKQ8hhr+uWUrgRkVOEaYL3cOTf1xkDhvGDpzkcDq655hrmz5/PrFmzMGqf88orr+Dz+bjyyispLy9nwIAB3H777SQkJPDWW29x9dVXc9pppzFo0KAffA+/38+4ceNIS0vj008/paSkpMH4nDrx8fHMnz+fjIwMNm7cyA033EB8fDx/+MMfuOKKK9i0aRP/+c9/WLZsGQCJiYlHvUZFRQWjR49myJAhrFmzhr179zJp0iSmTp3aIMCtWLGCjh07smLFCrZt28YVV1zBWWedxQ033PCDn+dYn68u2HzwwQfU1NQwZcoUrrjiCt5//30AJkyYQE5ODk8//TR2u528vLzgTuNTpkyhurqalStXEhsby+bNm4mLi2t2O5pD4SaEtLeUiJxyvIfhgYzIv+//7AZXbJNOve6663jooYf44IMPGD58OBDokho/fjyJiYkkJiZy2223Bc+fNm0aS5Ys4eWXX25SuFm2bBlffvklS5YsISMjcC0eeOCBo8bJ3HnnncHvu3btym233cbChQv5wx/+QHR0NHFxcTgcDtLT04/7Xi+++CJVVVUsWLCA2NjA53/yyScZO3Ysf/zjH0lLSwMgOTmZJ598ErvdTq9evbj44otZvnx5i8LN8uXL2bhxIzt27CAzMxOABQsW0LdvX9asWcM555xDfn4+v//97+nVqxcAPXr0CD4/Pz+f8ePH069fPwC6d+/e7DY0l7qlQsjlqK3caFdwEZGTRq9evTj33HN5/vnnAdi2bRsffvgh119/PQA+n4/77ruPfv36kZKSQlxcHEuWLCE/P79Jr79lyxYyMzODwQZgyJAhR533r3/9i6FDh5Kenk5cXBx33nlnk9+j/ntlZ2cHgw3A0KFD8fv9bN26NXisb9++2O324P2OHTuyd+/eZr1X/ffMzMwMBhuAPn36kJSUxJYtWwCYMWMGkyZNYtSoUTz44IN88803wXNvvvlm/vd//5ehQ4dyzz33tGgAd3OpchNCdZUbryo3InKqcMYEqihWvG8zXH/99UybNo2nnnqKefPmcdpppzFs2DAAHnroIR5//HEee+wx+vXrR2xsLNOnT6e6ujpkzf3444+ZMGECc+bMYfTo0SQmJrJw4UIeeeSRkL1HfXVdQnUMw8AfxjXYZs+ezVVXXcVbb73FO++8wz333MPChQu57LLLmDRpEqNHj+att97i3XffZe7cuTzyyCNMmzYtbO1R5SaE6sbcaFdwETllGEageyjStyaMt6nv8ssvx2az8eKLL7JgwQKuu+664Pib1atXc8kll/CrX/2K7OxsunfvzldffdXk1+7duzcFBQUUFhYGj33yyScNzvnoo4/o0qULs2bNYuDAgfTo0YOdO3c2OMflcuHz+X7wvTZs2EBFRUXw2OrVq7HZbJxxxhlNbnNz1H2+goKC4LHNmzdTXFxMnz59gsd69uzJrbfeyrvvvsu4ceOYN29e8LHMzEwmT57Mq6++yu9+9zv+/ve/h6WtdRRuQkizpURETk5xcXFcccUVzJw5k8LCQq699trgYz169GDp0qV89NFHbNmyhd/85jcNZgL9kFGjRtGzZ08mTpzIhg0b+PDDD5k1a1aDc3r06EF+fj4LFy7km2++4S9/+QuLFi1qcE7Xrl3ZsWMHeXl57N+/H4/Hc9R7TZgwgaioKCZOnMimTZtYsWIF06ZN4+qrrw6Ot2kpn89HXl5eg9uWLVsYNWoU/fr1Y8KECaxbt47PPvuMa665hmHDhjFw4EAqKyuZOnUq77//Pjt37mT16tWsWbOG3r17AzB9+nSWLFnCjh07WLduHStWrAg+Fi4KNyF0ZLaUuqVERE42119/PYcOHWL06NENxsfceeednH322YwePZrhw4eTnp7OpZde2uTXtdlsLFq0iMrKSgYNGsSkSZO4//77G5zzs5/9jFtvvZWpU6dy1lln8dFHH3HXXXc1OGf8+PGMGTOGESNG0L59+2NOR4+JiWHJkiUcPHiQc845h5///OeMHDmSJ598snkX4xjKy8vJyclpcBs7diyGYfDaa6+RnJzM+eefz6hRo+jevTv/+te/ALDb7Rw4cIBrrrmGnj17cvnll5Obm8ucOXOAQGiaMmUKvXv3ZsyYMfTs2ZO//vWvJ9zexhimaZ5Sv4lLS0tJTEykpKSEhISEkL72o0u/4vHlX/OrH2Xxv5f2C+lri4hYraqqih07dtCtWzeioqKsbo60QY39jDXn97cqNyEU3FtKu4KLiIhYRuEmhIJ7S2lXcBEREcso3ISQ9pYSERGxnsJNCLm0/YKIiIjlFG5CyBGcCq7KjYi0XafYPBSJoFD9bCnchJCzrltKY25EpA2qW/X28GELNsqUU0LdqtD1t45oCW2/EEJOdUuJSBtmt9tJSkoK7lEUExMTXOVX5ET5/X727dtHTEwMDseJxROFmxAK7i2lbikRaaPqdqxu6SaMIo2x2WxkZWWdcGhWuAkhVW5EpK0zDIOOHTvSoUMHvF6v1c2RNsblcmGznfiIGYWbEHJqKriInCLsdvsJj4sQCRcNKA4hhyo3IiIillO4CSGndgUXERGxnMJNCNWNuanxq1tKRETEKgo3IVQ3W0pjbkRERKyjcBNCdd1S1eqWEhERsYzCTQgFu6UUbkRERCyjcBNC2hVcRETEego3IVRXuVG3lIiIiHUUbkLoyMaZqtyIiIhYReEmhBy2QOXG5zfxK+CIiIhYwtJws3LlSsaOHUtGRgaGYbB48eImP3f16tU4HA7OOuussLWvuZyOI5fT61fXlIiIiBUsDTcVFRVkZ2fz1FNPNet5xcXFXHPNNYwcOTJMLWsZZ73NvjSoWERExBqWbpyZm5tLbm5us583efJkrrrqKux2+w9WezweDx6PJ3i/tLS02e/XVHV7S4HCjYiIiFVa3ZibefPmsX37du65554mnT937lwSExODt8zMzLC1rW7MDWjGlIiIiFVaVbj5+uuvueOOO/jnP/+Jw9G0otPMmTMpKSkJ3goKCsLWPsMw6u0vpXAjIiJiBUu7pZrD5/Nx1VVXMWfOHHr27Nnk57ndbtxudxhb1pDDZsPr86lbSkRExCKtJtyUlZWxdu1a1q9fz9SpUwHw+/2YponD4eDdd9/lggsusLiVgYX8Kr3qlhIREbFKqwk3CQkJbNy4scGxv/71r7z33nv8+9//plu3bha1rCGntmAQERGxlKXhpry8nG3btgXv79ixg7y8PFJSUsjKymLmzJns2rWLBQsWYLPZOPPMMxs8v0OHDkRFRR113Ep14caryo2IiIglLA03a9euZcSIEcH7M2bMAGDixInMnz+fwsJC8vPzrWpei9RNB1e4ERERsYZhmuYp1X9SWlpKYmIiJSUlJCQkhPz1Rzz8Pjv2V/DK5CGc0zUl5K8vIiJyKmrO7+9WNRW8NaibCu6tUeVGRETECgo3Ieao3YLBq40zRURELKFwE2LBRfw05kZERMQSCjchptlSIiIi1lK4CbEjs6XULSUiImIFhZsQCy7ip72lRERELKFwE2LBbqkaVW5ERESsoHATYg5bbbeUKjciIiKWULgJMe0tJSIiYi2FmxBzavsFERERSynchJgjOBVclRsRERErKNyEmBbxExERsZbCTYhpET8RERFrKdyEmPaWEhERsZbCTYipW0pERMRaCjch5tSAYhEREUsp3ISYQ1PBRURELKVwE2JaxE9ERMRaCjchpkX8RERErKVwE2KaLSUiImIthZsQczrqdgVX5UZERMQKCjch5qzdFbxGu4KLiIhYQuEmxLS3lIiIiLUUbkJMA4pFRESspXATYpoKLiIiYi2FmxBz1I658WrMjYiIiCUUbkIsOFtK3VIiIiKWULgJMadN3VIiIiJWUrgJMe0tJSIiYi2FmxDTruAiIiLWUrgJsbqp4DWq3IiIiFhC4SbEtLeUiIiItRRuQszl0JgbERERKynchJhDs6VEREQspXATYpotJSIiYi2FmxBz2bWIn4iIiJUUbkKsbldwvwl+DSoWERGJOIWbEKvrlgLtLyUiImIFhZsQq+uWAi3kJyIiYgWFmxCr2xUctJCfiIiIFRRuQsxeL9xUK9yIiIhEnMJNiBmGEeya0lo3IiIikadwEwaO4P5SCjciIiKRpnATBnU7g6tbSkREJPIUbsIguDO4poKLiIhEnMJNGGh/KREREeso3ISBs3ZncHVLiYiIRJ7CTRg4VbkRERGxjMJNGByZLaXKjYiISKRZGm5WrlzJ2LFjycjIwDAMFi9e3Oj5q1atYujQoaSmphIdHU2vXr149NFHI9PYZtBsKREREes4rHzziooKsrOzue666xg3btwPnh8bG8vUqVPp378/sbGxrFq1it/85jfExsZy4403RqDFTePQIn4iIiKWsTTc5Obmkpub2+Tzc3JyyMnJCd7v2rUrr776Kh9++OFxw43H48Hj8QTvl5aWtrzBTeS0aSq4iIiIVVr1mJv169fz0UcfMWzYsOOeM3fuXBITE4O3zMzMsLfrSLeUKjciIiKR1irDTefOnXG73QwcOJApU6YwadKk4547c+ZMSkpKgreCgoKwt08DikVERKxjabdUS3344YeUl5fzySefcMcdd3D66adz5ZVXHvNct9uN2+2OaPucGnMjIiJimVYZbrp16wZAv3792LNnD7Nnzz5uuLFC3fYLmi0lIiISea2yW6o+v9/fYMDwyeDIbCmFGxERkUiztHJTXl7Otm3bgvd37NhBXl4eKSkpZGVlMXPmTHbt2sWCBQsAeOqpp8jKyqJXr15AYJ2chx9+mJtvvtmS9h/PkdlS6pYSERGJNEvDzdq1axkxYkTw/owZMwCYOHEi8+fPp7CwkPz8/ODjfr+fmTNnsmPHDhwOB6eddhp//OMf+c1vfhPxtjdGi/iJiIhYx9JwM3z4cEzz+NWN+fPnN7g/bdo0pk2bFuZWnTgt4iciImKdVj/m5mTk1FRwERERyyjchIEW8RMREbGOwk0YaBE/ERER6yjchIHTFrisXoUbERGRiFO4CYO6bimvpoKLiIhEnMJNGKhbSkRExDoKN2FQN1vKqwHFIiIiEadwEwbBbilVbkRERCJO4SYMtIifiIiIdRRuwsAV7JZS5UZERCTSFG7CwGHTbCkRERGrKNyEgWZLiYiIWEfhJgxcGlAsIiJiGYWbMHAEw426pURERCJN4SYMgt1SflVuREREIk3hJgyC3VI1qtyIiIhEmsJNGDhstVPBVbkRERGJOIWbMNAifiIiItZRuAkDzZYSERGxjsJNGDi0caaIiIhlFG7CwKnZUiIiIpZRuAmD4K7gNQo3IiIikaZwEwbBRfy0t5SIiEjEKdyEgdOmXcFFRESsonATBnXdUqYJPlVvREREIkrhJgzqZkuBqjciIiKRpnATBnWVG1C4ERERiTSFmzCoH260SrGIiEhkKdyEgd1mYNT2TGl/KRERkchSuAkTp61uCwZVbkRERCJJ4SZMgqsUa8yNiIhIRCnchElwIT9VbkRERCJK4SZMnNoZXERExBIKN2FypFtKlRsREZFIUrgJk7qF/DRbSkREJLIUbsJEO4OLiIhYw2F1A9qMqlLIexF81TD05uBU8BrtLSUiIhJRLarcFBQU8N133wXvf/bZZ0yfPp1nn302ZA1rdaor4D+3w7LZYJpHuqU0oFhERCSiWhRurrrqKlasWAFAUVERF154IZ999hmzZs3i3nvvDWkDWw1XbOCr6YMaT73ZUqrciIiIRFKLws2mTZsYNGgQAC+//DJnnnkmH330ES+88ALz588PZftaj7pwA1BdoUX8RERELNKicOP1enG73QAsW7aMn/3sZwD06tWLwsLC0LWuNbHZwREd+L66HEfd9gsacyMiIhJRLQo3ffv25ZlnnuHDDz9k6dKljBkzBoDdu3eTmpoa0ga2KnXVm+oKnA7NlhIREbFCi8LNH//4R/72t78xfPhwrrzySrKzswF4/fXXg91Vp6T64cZW2y2ldW5EREQiqkVTwYcPH87+/fspLS0lOTk5ePzGG28kJiYmZI1rdVxxga/V5Tjs8YAGFIuIiERaiyo3lZWVeDyeYLDZuXMnjz32GFu3bqVDhw4hbWCrUr9yo72lRERELNGicHPJJZewYMECAIqLixk8eDCPPPIIl156KU8//XRIG9iquGqrVvXCjfaWEhERiawWhZt169bx4x//GIB///vfpKWlsXPnThYsWMBf/vKXkDawVanfLVU75qZalRsREZGIalG4OXz4MPHxgTEl7777LuPGjcNms/GjH/2InTt3hrSBrUpdt5T3cHC2lCo3IiIikdWicHP66aezePFiCgoKWLJkCT/5yU8A2Lt3LwkJCSFtYKui2VIiIiKWa1G4ufvuu7ntttvo2rUrgwYNYsiQIUCgipOTk9Pk11m5ciVjx44lIyMDwzBYvHhxo+e/+uqrXHjhhbRv356EhASGDBnCkiVLWvIRwiMYbspx1I65UbeUiIhIZLUo3Pz85z8nPz+ftWvXNggXI0eO5NFHH23y61RUVJCdnc1TTz3VpPNXrlzJhRdeyNtvv83nn3/OiBEjGDt2LOvXr2/2ZwiL4JgbDSgWERGxSovWuQFIT08nPT09uDt4586dm72AX25uLrm5uU0+/7HHHmtw/4EHHuC1117jjTfeaFbFKGzqd0vFaW8pERERK7SocuP3+7n33ntJTEykS5cudOnShaSkJO677z78ERxj4vf7KSsrIyUl5bjneDweSktLG9zCpl64qdtbqlqVGxERkYhqUeVm1qxZ/N///R8PPvggQ4cOBWDVqlXMnj2bqqoq7r///pA28ngefvhhysvLufzyy497zty5c5kzZ05E2lN/KrjTocqNiIiIFVoUbv7f//t/PPfcc8HdwAH69+9Pp06d+O1vfxuRcPPiiy8yZ84cXnvttUZXRZ45cyYzZswI3i8tLSUzMzM8jWowW6p2zI12BRcREYmoFoWbgwcP0qtXr6OO9+rVi4MHD55wo37IwoULmTRpEq+88gqjRo1q9Fy3243b7Q57m4Dvbb+gRfxERESs0KIxN9nZ2Tz55JNHHX/yySfp37//CTeqMS+99BK//vWveemll7j44ovD+l7N5jx6Kri6pURERCKrRZWbP/3pT1x88cUsW7YsuMbNxx9/TEFBAW+//XaTX6e8vJxt27YF7+/YsYO8vDxSUlLIyspi5syZ7Nq1K7iP1YsvvsjEiRN5/PHHGTx4MEVFRQBER0eTmJjYko8SWseo3GgquIiISGS1qHIzbNgwvvrqKy677DKKi4spLi5m3Lhx/Pe//+Uf//hHk19n7dq15OTkBKdxz5gxg5ycHO6++24ACgsLyc/PD57/7LPPUlNTw5QpU+jYsWPwdsstt7TkY4ReMNwcDq5zo24pERGRyDJM0wxZaWHDhg2cffbZ+Hy+UL1kyJWWlpKYmEhJSUnot4qoOAAPdQfg9Us3cfPCLxjSPZWXbvxRaN9HRETkFNOc398tqtzIcdRVboBofxWgvaVEREQiTeEmlBxuMOwAuM1KQIv4iYiIRJrCTSgZRnAhP7c/EG40W0pERCSymjVbaty4cY0+XlxcfCJtaRtcseApqRduVLkRERGJpGaFmx+abp2YmMg111xzQg1q9WrH3dSFG68qNyIiIhHVrHAzb968cLWj7agNN05/JeDGqwHFIiIiEaUxN6FWO+bG5TsMgLdG3VIiIiKRpHATasHKTSDcaCq4iIhIZCnchJorBgBHTd2YG1VuREREIknhJtRqKzcOnwYUi4iIWEHhJtRqx9w4amq7pVS5ERERiSiFm1CrrdzYayoA8Pr9hHD7LhEREfkBCjehFgw3gcqNaYLPr3AjIiISKQo3oVbbLWWrDTcANQo3IiIiEaNwE2q1lRtbdUXwkAYVi4iIRI7CTajVhRtv/XCjyo2IiEikKNyEWm23lOGtwGYEDmlncBERkchRuAm12soN1RU47IHL69WYGxERkYhRuAm1euHGVRdualS5ERERiRSFm1Bz1q/cBPqltL+UiIhI5CjchFr9bikjEG40oFhERCRyFG5CrS7cmD7i7DWApoKLiIhEksJNqNWFGyDeXg2ociMiIhJJCjehZrODIxqAOFsVoKngIiIikaRwEw611ZsEmyo3IiIikaZwEw614SbOCFRuvJotJSIiEjEKN+FQu0pxrOEBtM6NiIhIJCnchENt5Sa+NtxoV3AREZHIUbgJh9pwE1PXLaUBxSIiIhGjcBMOdeGGunCjyo2IiEikKNyEQ224iTU0FVxERCTSFG7CoTbcRJu1A4o15kZERCRiFG7CoS7c1HVLabaUiIhIxCjchEPtVPBosxLQruAiIiKRpHATDrWVmyhTA4pFREQiTeEmHILhJlC50VRwERGRyFG4CYfabqm6yk2NKjciIiIRo3ATDrWVG7dflRsREZFIU7gJh+91S5VWea1sjYiIyClF4SYcgt1SgXBzoLzaytaIiIicUhRuwqG2cuP0BcLNwQqFGxERkUhRuAmHYLg5DCjciIiIRJLCTTg4A+HG5vNgw88BhRsREZGIUbgJh9rKDQR2Bi+p9GrGlIiISIQo3ISDww2GHYBYI7B55iFVb0RERCJC4SYcDCM4Y6pjtA9AXVMiIiIRonATLrVdU+m14UaDikVERCJD4SZc6sKNuwZQ5UZERCRSFG7CpTbctI8KrE58sNxjZWtEREROGZaGm5UrVzJ27FgyMjIwDIPFixc3en5hYSFXXXUVPXv2xGazMX369Ii0s0Vqx9ykOmvDjSo3IiIiEWFpuKmoqCA7O5unnnqqSed7PB7at2/PnXfeSXZ2dphbd4JqKzfJTnVLiYiIRJLDyjfPzc0lNze3yed37dqVxx9/HIDnn38+XM0Kjdpwk2QPdEepciMiIhIZloabSPB4PHg8R8a7lJaWRuaNa8NNgj0QalS5ERERiYw2P6B47ty5JCYmBm+ZmZmReePacBNvU+VGREQkktp8uJk5cyYlJSXBW0FBQWTeuDbcxKBwIyIiEkltvlvK7Xbjdrsj/8a14SaaSgAOHa7G5zex24zIt0VEROQU0uYrN5apnQru9lcBYJpQfFjVGxERkXCztHJTXl7Otm3bgvd37NhBXl4eKSkpZGVlMXPmTHbt2sWCBQuC5+Tl5QWfu2/fPvLy8nC5XPTp0yfSzW9cbeXG5q0gMdpJSaWXgxXVpMZZUEUSERE5hVgabtauXcuIESOC92fMmAHAxIkTmT9/PoWFheTn5zd4Tk5OTvD7zz//nBdffJEuXbrw7bffRqTNTVYbbqiuIDXWRUmllwMV1fSwtlUiIiJtnqXhZvjw4ZimedzH58+ff9Sxxs4/qdR2S1FdTkqsi+37KzSoWEREJAI05iZc6lVuUmJdgNa6ERERiQSFm3Cp3y0VVxtutHmmiIhI2CnchEuwW+pI5UbdUiIiIuGncBMuwcpNOakx6pYSERGJFIWbcHHGBL6aftrXfnuwXOFGREQk3BRuwqWucgO0c3kBdUuJiIhEgsJNuNjs4IgGILU23KhbSkREJPwUbsKptnqT4giEm0OHq/H7W8k6PSIiIq2Uwk041YabBHtgCrjPb1Ja5bWyRSIiIm2ewk041U4Hd/kqiXcHFoNW15SIiEh4KdyEU/1ViuO01o2IiEgkKNyE07G2YNB0cBERkbBSuAmn+gv5aZViERGRiFC4CadjbsGg/aVERETCSeEmnBp0S7kBDSgWEREJN4WbcHLXVm48peqWEhERiRCFm3CKSwt8LSvSzuAiIiIRonATTvEdA1/LCoNTwTVbSkREJLwUbsIpISPwtXS3uqVEREQiROEmnIKVm6JguDlQ4cE0tb+UiIhIuCjchFNduPF5SLWVA+D1mZR5aixslIiISNumcBNODhfEtAMgqnIPMS47AAc17kZERCRsFG7CLaG2elNaeGQLBo27ERERCRuFm3CLrx1UXKZBxSIiIpGgcBNux6jcaAsGERGR8FG4Cbd6lRttwSAiIhJ+CjfhVq9yk1q7kJ8GFIuIiISPwk24BSs3hdqCQUREJAIUbsItWLnZpdlSIiIiEaBwE251C/lVHqK92w+ociMiIhJOCjfhFp0MjigAOhiHAIUbERGRcFK4CTfDCFZv2vkPAIH9pURERCQ8FG4iIaETAIk1+wGo8vo5XK39pURERMJB4SYSagcVuyv3EB/lAGDH/gorWyQiItJmKdxEQm23lFFWSHbnJADyCoqta4+IiEgbpnATCQm1a92U7iYnKwmA9fnFljVHRESkLVO4iYS66eBlhfXCzSHr2iMiItKGKdxEQrByU8hZmckAfLOvgpLDXgsbJSIi0jYp3ERCvcpNSrSDLqkxAGz4rti6NomIiLRRCjeREJ8OGOD3wuED5GQmARp3IyIiEg4KN5Fgd0Js+8D3ZbvJyQp0Ta0v0LgbERGRUFO4iZTgBpqFDWZMmaZpXZtERETaIIWbSImvHVRctpte6Qm4HTZKKr1azE9ERCTEFG4ipV7lxuWwcWanRECL+YmIiISawk2k1KvcABpULCIiEiYKN5FSr3IDaFCxiIhImCjcREq9tW6A4KDiLYVlVFb7LGqUiIhI26NwEyn19pcC6JgYRVqCG5/fZOOuEgsbJiIi0rYo3ERKXeWmqhi8lRiGwVm1427y1DUlIiISMgo3kRKVCM7Atgt11ZvguBsNKhYREQkZS8PNypUrGTt2LBkZGRiGweLFi3/wOe+//z5nn302breb008/nfnz54e9nSFhGEePu9GMKRERkZCzNNxUVFSQnZ3NU0891aTzd+zYwcUXX8yIESPIy8tj+vTpTJo0iSVLloS5pSFSb3dwgH6dE7HbDIpKqygsqbSwYSIiIm2Hw8o3z83NJTc3t8nnP/PMM3Tr1o1HHnkEgN69e7Nq1SoeffRRRo8efczneDwePB5P8H5paemJNfpEBCs3gW6pGJeDXunx/Hd3Kevzi+nYL9q6tomIiLQRrWrMzccff8yoUaMaHBs9ejQff/zxcZ8zd+5cEhMTg7fMzMxwN/P4vrfWDRAcVLxupwYVi4iIhEKrCjdFRUWkpaU1OJaWlkZpaSmVlcfu1pk5cyYlJSXBW0FBQSSaemzfW6UYYFC3FADe27pXm2iKiIiEgKXdUpHgdrtxu91WNyPgGJWbC3p1wOWwsX1fBVv3lNErPcGixomIiLQNrapyk56ezp49exoc27NnDwkJCURHt4LxKsHKzZFwEx/lZFjP9gC8/UXhsZ4lIiIizdCqws2QIUNYvnx5g2NLly5lyJAhFrWomRLqTQX3+4OHf9o/cPzNjYXqmhIRETlBloab8vJy8vLyyMvLAwJTvfPy8sjPzwcC42Wuueaa4PmTJ09m+/bt/OEPf+DLL7/kr3/9Ky+//DK33nqrFc1vvrh0sDnAXwMbXw4eHtk7rUHXlIiIiLScpeFm7dq15OTkkJOTA8CMGTPIycnh7rvvBqCwsDAYdAC6devGW2+9xdKlS8nOzuaRRx7hueeeO+408JOO3QE/+m3g+9emwNfLAIhzO4JdU2+pa0pEROSEGOYp1g9SWlpKYmIiJSUlJCRYMHjX74dXb4BN/w5sxzDxDeg8kNfydnHLwjy6t49l+YxhGIYR+baJiIicpJrz+7tVjblpE2w2uPRpOG0keA/DCz+HfVvVNSUiIhIiCjdWcLjg8gXQaQBUHoJ/XEZcVRHD1TUlIiJywhRurOKOg6tegdQeULoLlt7NxbWzpt7SrCkREZEWU7ixUmwqXPZM4Put/2Fkj8Rg19SXReqaEhERaQmFG6t1GgAJncBbQdx3Hwa7pt7eqK4pERGRllC4sZphQK+fBr7f8qa6pkRERE6Qws3JoHdtuNn6NiPPSA12Ta3VTuEiIiLNpnBzMsg6F6JToPIgcUWfMf7sTgD871tb8PtVvREREWkOhZuTgd0BZ1wU+H7Lm9x6YU9iXXY2FBTz+obd1rZNRESklVG4OVnUdU19+SYd4tz8dsTpAPzxP19SWe2zsGEiIiKti8LNyaL7CHDGBta82b2O68/rRqekaApLqnh25XarWyciItJqKNycLJxR0OPCwPdb3iTKaWfmRb0AeOaDbygqqbKwcSIiIq2Hws3JpPfYwNctbwBwcb+ODOySTKXXx5+WfGlhw0RERFoPhZuTSY+fgN0FB76GfVsxDIO7ftoHgFfX7WLR+u/4ZPsB1n57kLyCYr7dX6G1cERERL7HYXUDpJ6oBOg+HL5+N1C9aX8G2ZlJjDu7E6+u28Wt/9pw1FPOSIvn0pxOXHJWBhlJ0ZFvs4iIyEnGME+x//qXlpaSmJhISUkJCQkJVjfnaJ//P3jjZuh4FvzmAwD2llXxu5c3sK/MQ43fpMbnp8ZvsrfUQ7XPDwQWOh7cLYXRfdMZcloqPTvEY7MZFn4QERGR0GnO72+Fm5NN+T54pCeYfjj9wsA4nF4XQ2y7o04tqfTyzsZCFq3fxac7DjZ4LDnGyeBuqQzt0Y5fDOhMlNMeqU8gIiIScgo3jTjpww3Aoptgw4tH7hs26DIU+l8OZ44HV+xRT9lVXMmbG3azatt+1n57iErvkbVxsjOTePbqAaQlREWi9SIiIiGncNOIVhFuAPZ9BVteC4y9Kaw31sadAP2vgIG/hrS+x3yq1+fni+9K+GT7Af7+4XaKD3vpEO/mb1cPICcrOUIfoJVYfi988Qpc+wYkd7W6NSIichwKN41oNeGmvkPfwn8XBcbjHNpx5HinAdDl3MDXTgMgMTMw+KaenQcquGHBWr7aU47LYWPuZf0YP6BzZNt/sqoqgYd6gM8DQ6fDhXOsbpGIiByHwk0jWmW4qeP3w44PYO3zsPVt8Nc0fDy2fWA6+cDrofOA4OFyTw23/iuPpZv3ADAupxMX9evIkNNSiXWfwhPm1i2A16cFvk/oBNM3gU2rI4iInIwUbhrRqsNNfWVFsG0Z7FoHuz6HPZsahp2MHDhnUmCMjjMav9/k0WVf8cR724KnOO0G53RNYfgZ7fnloCwSopwWfBALzbsIdq4+cn/iG9DtfOvaIyIix6Vw04g2E26+z1sZCDnr/gH/fRV81YHjUUkweDIM/g3EpPDp9gO8tbGQ97fuI//g4eDTu6bG8PSvBtC7Yxu6Jo05tBMe7w8YcPoo2LYUcn4FlzxldctEROQYFG4a0WbDTX0V+2H9P2DN81CSHzjmiodBk+BHUyCuPQDf7q/gg6/28ezK7ewqriTKaeOBy/ox7uyjx+RU1/gprfJSWumlrKqG0iovKbEu+mYkRvKTHV9lMXz8JPT7BbQ/A4DlW/awdU8Z157blRjX97rfVj4E7/1voFIz7A6Yf1FgsPZtX4FTiyGKiJxsFG4acUqEmzp+H2x5HVY+HOi2AnBEw4BrYchvISkLgEMV1dzyrzxWfrUPgAmDs/jdT85gw3fFfLL9AJ9sP8imXSX4/Ef/qOSemc6dP+1DJ6tXR35taiDQxbSD65bwdmEsU15ch2lCt3ax/Pny7CMzxUwTnhwIB7bBJX+F7CsDVZySAvjFfOh7maUfRUREjqZw04hTKtzUMU346j/wwZ9g97rAMcMOZ46Dc2+Gjv3x+U3+svxrnn9vA1nsJQoPZcRQasZQSiyHcQMG8W4HCdFO4twOtu0rx+c3iXbamTbydCad1x2Xw4IBufu+gr8ODix8CHhiMxhVPIsCXzJRThtVXj92m8GUEacz7YLTcRauh+cuCAS9338N7nhYNgdW/RnOuAiufCnyn0FERBqlcNOIUzLc1DFN2L4CVj8O298/cjxrSKDKc3A7HN5/7Kfa3Zj9fo5t5N0Qnw7Al0Wl3L34v3z2bWB15O7tY/n10G4M7JJMz7R47JHa/uFfvwqsB9R9OJ4D+bhLtrPNn8HT3Z/izl+cx5w3/svivN0AZHdO5OG4F+jx7Ysc6H4JpRc9TUKUA8eBr0icdx6mzUHFtM3EJadFpu0iItIkCjeNOKXDTX278+CjvwTWz6mteNQxY9rhc8bh8JYF1oKpPwvLGQs/ngFDpoIzCtM0WZy3i/vf+pL95Z7gafFRDs7OSmbIaan88pxMkmJc4fkc330eqMIYNgqvfI8bXv6SZ73/Q4ZxEH/HHGzXvgHueN7YsJtZizZSWVXFJ+4ppBplTKy+nQ/82cGXetP1P5xp+5Y7vb/mq6wreOyKs7QZqYjISULhphEKN99z6Fv4emlg76qU7pDcLbA7eR3TBO9hKPwC3r0Tdq0NHE/KghGzAvtfxaZSWuXlHx/v5ONvDrAu/xCHq49s/xDvdnDded247rxuJEY3nG5eVuWlqKSKbu1icdh/uEurstrHu5uLWLx+F7sPVfJw5V30827gk4Qx3Ob9Dd8dquTCDiX8zXsntsoDkHUujH0M2p9BYUkl7702nwnb7+CQkcRlUc+x/7Cfck8gvF1vf4u7nC+w1t+Tn1fPJjnGyWO/zGFYz/YnepVFROQEKdw0QuHmBPj9sOnfsPQeKNt95Hj73tBlSKB7CwNf6W4OFeVTtq+A3QdLWX04k3VmT3a4ejLhx33omBjFuvxi1ucfYuueMkwzUOkZelo7zuvRjh/3aEfn5BgqvT4OV9dQWe2j4GAlr+Xt4p1NRcEwcp5tI/90zcVjOrjA8wi7aE/n5GhevelcOpRvgfljoboMMAIbkP54Bqx6FDa/Fpg1NuaB2o9lYgJm6W7sj5+JYfq5Puk5lhfFYBgw7YIe3DKyR+S62URE5CgKN41QuAmB6gr46MnAejr7vmzy02pMG1vMLHaaaZSZMZQRQ7kZTbEtibe9A9hP06aVZ6ZEM+6sDCZ9eT3xBzfxVberWdl9Bn7T5JKzOh3ZIHTPZlhxP3z5Zr1nG4AJv/kQOvY/+sUXXArbV1Bzzo08VHExf1tXDhj8qHsKP+mTTmZKDJkp0WQmx5zaqzuLiESYwk0jFG5CrOIA5H8MOz8KdFnZXRDfMTDoOL4jYMJ3azALPsMo3XXcl/HbnHyTMpwX/SP5554s6jY1NwyIcdqJj3Iyold7xp3dmYFdkjE2L4ZXrgVXHNyyIdCtdjx7t8Cqx2DjK2D6oENfuGn1UftwAZD3EiyeHLxb7Uzgv9XpbPd3oNJ048GFByce04nN4cTpdOJ2OXE7ncS4nWQlu+me4iY5yoZh+gKLKdZ4AjefB5/Xg83nwajxQE1V4Li/JnDd7E6wO6kxHNgMGw0KRaYZGBtl+gPX1PQHdou3u8DmCDzX5gjMgrPZAo8Z9tqvBhg2Kqp97C2rpn18FHHfX436WNfiKE0456jXCV21q8rrY+fBw3x7oILC4ir8pondZmAzDOw2gxiXg+7tYzmtfSyx31/XSKj2+fGbEHWCMxpNAj+OzS1kev3+wJ9Vk37WwstvQmlVYM0uh83AabfhtBs47AbRTju2ZrTRNMFnmjhU2W3IHQfn/z6kL6lw0wiFGwuVfAffrYXyPVBVCp4S8JRB0cbA6sq1/CmnUZ15Hg6nC7vdgWF3BP4FKd8DpbsDt5LvwO+F4TNh+B1Ne/+6DUh7joEOvY99jrcK3rgFCj6F4p1HDbYWEZEfVuZsR/ysb0L6mgo3jVC4OUkVbYS18+CLl2vHyTRBcleYvCqwTk04eKvg4DewbysU59dWWgLVlmrPYTweD9VeL15vDd4aL5UeL3vKa9hb4cPjt+HDTg322mqPg2rTSTUOqupVfzy4MDFw4MNJDU6jBhdHZqeZ9SofPmz4MTAxME0Dm+HHiQ8HNTjxYceHHROnzSQjwUmnRBd7SiopLKnEwMSGSVK0g5JKb/BVDQNinXYqqms41j8ENgPi3A5cdhvVNT48NX6qfccOfN//f6sNE4xALg2VlFgXXVJiyEyJweWw4TdN/H4Tn2myv6yab/aVs7fM88MvVE+My067ODepcS7axbmJctr47mAl+QcPc6CiusVtddgMfLXjuUIpIcpBx8Royj01lFRWU+7x/fCT2hi3w0as20Gsy05slIM4lyMw98Hvp8Zn4vX5OVzt49Dh6gaTG+pz2Azioxz4/CY1fhOvz6TG5w/Jn5fDZhDjslPmqTnmz79BYJyhw26j5LAXXzP+kjjtBnabDczasYIExg3WHGOR1ea2OdbtwDRrr4Xfj9fX8tc03fHceNezJ9Sm71O4aYTCzUnOUx5YVbm4INBdU3fDhNgOkJAR2ME7oSMkdAb7ydf9UOGp4dMdB1j51X7KPTV0TY2ha7tYuqbG0ikpmrKqGgpLKikqraKwpIrqGj/piVFkJEaTnhhFWoKbGp9JcaWXktrboYpqDlRUc6Dcw4Hyag5UePD5TRx15XSbjQpPDesLijn4vV/ITrvB2P4Z/HpoN/p1TqSopIo3v9jN6xt288V3JcHzXHYb6YlRtI93c6Dcw67iykb/cYt22omLchAf5SDe7cBmMyg+7OVAuYfSqprjPi/O7SAhKrAYZHyUg/goJ7FuB3FuO7EuBzEuOxgGpmlimuA3TdISorigVwcyU2J+8PoXHDzMG1/s5v2t+0iIcnB6h3hO7xBHjw5xRLvsbCgoZn1BMevzi9laVEpjvxNsBpydlUyn5MCfW1ltV0ZFdQ2xLgdxbgdxUQ5i3Q48Xj+7iwNh8tBhb4PXSYx2khzjJMblaNBzZ5qB7qLKah+eGh+V1T4cdht9OibQr3Mi/Tol0is9ns2Fpbxduyecp+bocJka6yI+yoGnxk+VNxBCTRP6ZCQwoEsyZ2clc3aXJGJdDnbsrwjevjt0GJfDRqwr8Bli3Q6inDacNht2W6CbxmGzEeO2kxDlJKH2z8tmg0MVgT/rfbU/k5W171tdewPISIoKjFNLjqFzSuAabi0qZWtROVuLStmxvyL4M15aVdNgFfRop51Yt4Nol43Kaj9lVd5jfvamyEyJ5oy0BHp3jKdXeuBrl9TYoyYJ+P0mu0sq+XpvOd/sLWfb3nJ2l1SREuMkLSHwdyMtIYqU2usdHxX4GTZNav/O7+ODr/axp/RIwI522unaLpbOydHsK/Pwzb5yyr739yPWZadnejxnpMUT5bRTWe3jsDfw81BW5WVfmYe9ZZ7gZIrGxLgCf1YxbjvRTjtRTjtRThtuhz3wGU0TvwmmaVJS6WV3cVWDZTyOJT7KQfs4N+3i3YGvtf8RaBfvrv3Zc1LpraHC46PCU0O5pwaXw8Y1Q7o28U+oaRRuGqFwI22ZaZp8e+Awa789SF5BMe3j3Vw5KOvIIOvvKaitTmQkRdEu1o2t3j/2fr/J3jIP3x06TJmnhsRoJ4nRTpKinSREO3E2MnXf6/Nz6HA1pklwPEPgq+2kmnV2uLqGLYWlbNpVysZdJWzaVcL+cg+Du6UysncHhp/RgZTY5q/RdLi6hv1l1cS47SRFO5u0zEFTVHhqWLF1L18WlpGVEsNpHWI5rX1c+NaRiiDTNKmo9uE3TWJdjmP+nHhqfJRX1XCgoro2SFZRWFzJnlIPdrtBjNNOjDsQkJNjnPRMi6dnWnxEB/+bpslXe8o5dLiarqmxpCW4MeolWtM02V8eqDJWeGromRZPp6ToBn/3jqfCU8PeMg9enx+bAYZhBMcxxUcFgnZjfy+Px1Pjo6ikiqKSKhx2G7G1/9GIdtmJczuIctqb/ZrhoHDTCIUbERGR1qc5v78t2AhIREREJHwUbkRERKRNUbgRERGRNkXhRkRERNoUhRsRERFpUxRuREREpE1RuBEREZE2ReFGRERE2hSFGxEREWlTFG5ERESkTVG4ERERkTZF4UZERETaFIUbERERaVMUbkRERKRNcVjdgEgzTRMIbJ0uIiIirUPd7+263+ONOeXCTVlZGQCZmZkWt0RERESaq6ysjMTExEbPMcymRKA2xO/3s3v3buLj4zEMI6SvXVpaSmZmJgUFBSQkJIT0taUhXevI0bWOHF3ryNG1jpxQXWvTNCkrKyMjIwObrfFRNadc5cZms9G5c+ewvkdCQoL+skSIrnXk6FpHjq515OhaR04orvUPVWzqaECxiIiItCkKNyIiItKmKNyEkNvt5p577sHtdlvdlDZP1zpydK0jR9c6cnStI8eKa33KDSgWERGRtk2VGxEREWlTFG5ERESkTVG4ERERkTZF4UZERETaFIWbEHnqqafo2rUrUVFRDB48mM8++8zqJrV6c+fO5ZxzziE+Pp4OHTpw6aWXsnXr1gbnVFVVMWXKFFJTU4mLi2P8+PHs2bPHoha3HQ8++CCGYTB9+vTgMV3r0Nm1axe/+tWvSE1NJTo6mn79+rF27drg46Zpcvfdd9OxY0eio6MZNWoUX3/9tYUtbr18Ph933XUX3bp1Izo6mtNOO4377ruvwf5Eut4ts3LlSsaOHUtGRgaGYbB48eIGjzfluh48eJAJEyaQkJBAUlIS119/PeXl5SfeOFNO2MKFC02Xy2U+//zz5n//+1/zhhtuMJOSksw9e/ZY3bRWbfTo0ea8efPMTZs2mXl5eeZFF11kZmVlmeXl5cFzJk+ebGZmZprLly83165da/7oRz8yzz33XAtb3fp99tlnZteuXc3+/fubt9xyS/C4rnVoHDx40OzSpYt57bXXmp9++qm5fft2c8mSJea2bduC5zz44INmYmKiuXjxYnPDhg3mz372M7Nbt25mZWWlhS1vne6//34zNTXVfPPNN80dO3aYr7zyihkXF2c+/vjjwXN0vVvm7bffNmfNmmW++uqrJmAuWrSoweNNua5jxowxs7OzzU8++cT88MMPzdNPP9288sorT7htCjchMGjQIHPKlCnB+z6fz8zIyDDnzp1rYavanr1795qA+cEHH5imaZrFxcWm0+k0X3nlleA5W7ZsMQHz448/tqqZrVpZWZnZo0cPc+nSpeawYcOC4UbXOnRuv/1287zzzjvu436/30xPTzcfeuih4LHi4mLT7XabL730UiSa2KZcfPHF5nXXXdfg2Lhx48wJEyaYpqnrHSrfDzdNua6bN282AXPNmjXBc9555x3TMAxz165dJ9QedUudoOrqaj7//HNGjRoVPGaz2Rg1ahQff/yxhS1re0pKSgBISUkB4PPPP8fr9Ta49r169SIrK0vXvoWmTJnCxRdf3OCagq51KL3++usMHDiQX/ziF3To0IGcnBz+/ve/Bx/fsWMHRUVFDa51YmIigwcP1rVugXPPPZfly5fz1VdfAbBhwwZWrVpFbm4uoOsdLk25rh9//DFJSUkMHDgweM6oUaOw2Wx8+umnJ/T+p9zGmaG2f/9+fD4faWlpDY6npaXx5ZdfWtSqtsfv9zN9+nSGDh3KmWeeCUBRUREul4ukpKQG56alpVFUVGRBK1u3hQsXsm7dOtasWXPUY7rWobN9+3aefvppZsyYwf/8z/+wZs0abr75ZlwuFxMnTgxez2P9m6Jr3Xx33HEHpaWl9OrVC7vdjs/n4/7772fChAkAut5h0pTrWlRURIcOHRo87nA4SElJOeFrr3AjrcKUKVPYtGkTq1atsropbVJBQQG33HILS5cuJSoqyurmtGl+v5+BAwfywAMPAJCTk8OmTZt45plnmDhxosWta3tefvllXnjhBV588UX69u1LXl4e06dPJyMjQ9e7DVO31Alq164ddrv9qFkje/bsIT093aJWtS1Tp07lzTffZMWKFXTu3Dl4PD09nerqaoqLixucr2vffJ9//jl79+7l7LPPxuFw4HA4+OCDD/jLX/6Cw+EgLS1N1zpEOnbsSJ8+fRoc6927N/n5+QDB66l/U0Lj97//PXfccQe//OUv6devH1dffTW33norc+fOBXS9w6Up1zU9PZ29e/c2eLympoaDBw+e8LVXuDlBLpeLAQMGsHz58uAxv9/P8uXLGTJkiIUta/1M02Tq1KksWrSI9957j27dujV4fMCAATidzgbXfuvWreTn5+vaN9PIkSPZuHEjeXl5wdvAgQOZMGFC8Htd69AYOnToUUsafPXVV3Tp0gWAbt26kZ6e3uBal5aW8umnn+pat8Dhw4ex2Rr+qrPb7fj9fkDXO1yacl2HDBlCcXExn3/+efCc9957D7/fz+DBg0+sASc0HFlM0wxMBXe73eb8+fPNzZs3mzfeeKOZlJRkFhUVWd20Vu2mm24yExMTzffff98sLCwM3g4fPhw8Z/LkyWZWVpb53nvvmWvXrjWHDBliDhkyxMJWtx31Z0uZpq51qHz22Wemw+Ew77//fvPrr782X3jhBTMmJsb85z//GTznwQcfNJOSkszXXnvN/OKLL8xLLrlEU5NbaOLEiWanTp2CU8FfffVVs127duYf/vCH4Dm63i1TVlZmrl+/3ly/fr0JmH/+85/N9evXmzt37jRNs2nXdcyYMWZOTo756aefmqtWrTJ79OihqeAnkyeeeMLMysoyXS6XOWjQIPOTTz6xukmtHnDM27x584LnVFZWmr/97W/N5ORkMyYmxrzsssvMwsJC6xrdhnw/3Ohah84bb7xhnnnmmabb7TZ79eplPvvssw0e9/v95l133WWmpaWZbrfbHDlypLl161aLWtu6lZaWmrfccouZlZVlRkVFmd27dzdnzZplejye4Dm63i2zYsWKY/4bPXHiRNM0m3ZdDxw4YF555ZVmXFycmZCQYP761782y8rKTrhthmnWW6ZRREREpJXTmBsRERFpUxRuREREpE1RuBEREZE2ReFGRERE2hSFGxEREWlTFG5ERESkTVG4ERERkTZF4UZERETaFIUbERHAMAwWL15sdTNEJAQUbkTEctdeey2GYRx1GzNmjNVNE5FWyGF1A0REAMaMGcO8efMaHHO73Ra1RkRaM1VuROSk4Ha7SU9Pb3BLTk4GAl1GTz/9NLm5uURHR9O9e3f+/e9/N3j+xo0bueCCC4iOjiY1NZUbb7yR8vLyBuc8//zz9O3bF7fbTceOHZk6dWqDx/fv389ll11GTEwMPXr04PXXXw/vhxaRsFC4EZFW4a677mL8+PFs2LCBCRMm8Mtf/pItW7YAUFFRwejRo0lOTmbNmjW88sorLFu2rEF4efrpp5kyZQo33ngjGzdu5PXXX+f0009v8B5z5szh8ssv54svvuCiiy5iwoQJHDx4MKKfU0RC4IT3FRcROUETJ0407Xa7GRsb2+B2//33m6ZpmoA5efLkBs8ZPHiwedNNN5mmaZrPPvusmZycbJaXlwcff+utt0ybzWYWFRWZpmmaGRkZ5qxZs47bBsC88847g/fLy8tNwHznnXdC9jlFJDI05kZETgojRozg6aefbnAsJSUl+P2QIUMaPDZkyBDy8vIA2LJlC9nZ2cTGxgYfHzp0KH6/n61bt2IYBrt372bkyJGNtqF///7B72NjY0lISGDv3r0t/UgiYhGFGxE5KcTGxh7VTRQq0dHRTTrP6XQ2uG8YBn6/PxxNEpEw0pgbEWkVPvnkk6Pu9+7dG4DevXuzYcMGKioqgo+vXr0am83GGWecQXx8PF27dmX58uURbbOIWEOVGxE5KXg8HoqKihocczgctGvXDoBXXnmFgQMHct555/HCCy/w2Wef8X//938ATJgwgXvuuYeJEycye/Zs9u3bx7Rp07j66qtJS0sDYPbs2UyePJkOHTqQm5tLWVkZq1evZtq0aZH9oCISdgo3InJS+M9//kPHjh0bHDvjjDP48ssvgcBMpoULF/Lb3/6Wjh078tJLL9GnTx8AYmJiWLJkCbfccgvnnHMOMTExjB8/nj//+c/B15o4cSJVVVU8+uij3HbbbbRr146f//znkfuAIhIxhmmaptWNEBFpjGEYLFq0iEsvvdTqpohIK6AxNyIiItKmKNyIiIhIm6IxNyJy0lPvuYg0hyo3IiIi0qYo3IiIiEibonAjIiIibYrCjYiIiLQpCjciIiLSpijciIiISJuicCMiIiJtisKNiIiItCn/H893NxCxgG0PAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \\\n",
    "            x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \\\n",
    "            x10_batch, x11_batch, x12_batch, y_batch in dataloader:\n",
    "            \n",
    "            x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \\\n",
    "            x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \\\n",
    "            x10_batch, x11_batch, x12_batch, y_batch = (\n",
    "                x0_batch.to(device),\n",
    "                x1_batch.to(device),\n",
    "                x2_batch.to(device),\n",
    "                x3_batch.to(device),\n",
    "                x4_batch.to(device),\n",
    "                x5_batch.to(device),\n",
    "                x6_batch.to(device),\n",
    "                x7_batch.to(device),\n",
    "                x8_batch.to(device),\n",
    "                x9_batch.to(device),\n",
    "                x10_batch.to(device),\n",
    "                x11_batch.to(device),\n",
    "                x12_batch.to(device),\n",
    "                y_batch.to(device)\n",
    "            )\n",
    "            \n",
    "            outputs = model(x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \n",
    "                             x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \n",
    "                             x10_batch, x11_batch, x12_batch)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_targets, all_predictions)\n",
    "    class_report = classification_report(all_targets, all_predictions, zero_division=0)\n",
    "\n",
    "    return accuracy, conf_matrix, class_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9618528610354223\n",
      "\n",
      "Confusion Matrix:\n",
      "[[408   3   0   7   2]\n",
      " [  1  79   0   0   0]\n",
      " [  0   0  34   0   0]\n",
      " [  2   1  12 167   0]\n",
      " [  0   0   0   0  18]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       420\n",
      "           1       0.95      0.99      0.97        80\n",
      "           2       0.74      1.00      0.85        34\n",
      "           3       0.96      0.92      0.94       182\n",
      "           4       0.90      1.00      0.95        18\n",
      "\n",
      "    accuracy                           0.96       734\n",
      "   macro avg       0.91      0.98      0.94       734\n",
      "weighted avg       0.97      0.96      0.96       734\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, conf_matrix, class_report = evaluate_model(model, test_dataloader)\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}\\n')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}\\n')\n",
    "print(f'Classification Report:\\n{class_report}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
