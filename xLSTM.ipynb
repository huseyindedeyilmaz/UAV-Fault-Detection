{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hy138\\AppData\\Local\\Temp\\ipykernel_3372\\3508959855.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_name(flight_name, file_name):\n",
    "    topic_name = file_name.split(flight_name)\n",
    "    topic_name =  topic_name[1].strip(\"-\") if len(topic_name)>1 else \"\"\n",
    "    topic_name = topic_name.split(\".csv\")[0]\n",
    "    return topic_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(full_path):\n",
    "    df_tmp = pd.read_csv(full_path,on_bad_lines=\"skip\")\n",
    "    df_tmp = df_tmp.rename(columns={\"%time\": \"timestamp\"})\n",
    "    \n",
    "    df_tmp[\"timestamp\"] = pd.to_datetime(df_tmp[\"timestamp\"], unit=\"ns\")\n",
    "    df_tmp.set_index(\"timestamp\", inplace=True) \n",
    "    return df_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/processed/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_topic_list = [\"diagnostics\", \"emergency_responder-traj_file\",\"mavlink-from\",\"mavros-state\",\n",
    "                    \"global_position\",\n",
    "                     \"local_position\", \n",
    "                     \"mavctrl-rpy\",\n",
    "                     \"mavros-battery\", \"field_raw\",\n",
    "                     \"mavros-imu-mag\", \"mavros-mission-reached\",\n",
    "                     \"mavros-rc\",\n",
    "                      \"setpoint_raw\",\"mavros-imu-data_raw\"]\n",
    "\n",
    "unused_column_list = [\"field.header.seq\", \"field.header.stamp\", \"field.header.frame_id\", \n",
    "                      \"field.commanded\", \"field.variance\", \"%time\",\"field.x\",\"field.twist.angular.x\",\"field.twist.angular.y\",\"field.twist.angular.z\",\n",
    "                      'field.orientation_covariance0',\n",
    "       'field.orientation_covariance1',\n",
    "       'field.orientation_covariance2',\n",
    "       'field.orientation_covariance3',\n",
    "       'field.orientation_covariance4',\n",
    "       'field.orientation_covariance5',\n",
    "       'field.orientation_covariance6',\n",
    "       'field.orientation_covariance7',\n",
    "       'field.orientation_covariance8',\n",
    "\n",
    "       'field.angular_velocity_covariance0',\n",
    "       'field.angular_velocity_covariance1',\n",
    "       'field.angular_velocity_covariance2',\n",
    "       'field.angular_velocity_covariance3',\n",
    "       'field.angular_velocity_covariance4',\n",
    "       'field.angular_velocity_covariance5',\n",
    "       'field.angular_velocity_covariance6',\n",
    "       'field.angular_velocity_covariance7',\n",
    "       'field.angular_velocity_covariance8',\n",
    "\n",
    "       'field.linear_acceleration_covariance0',\n",
    "       'field.linear_acceleration_covariance1',\n",
    "       'field.linear_acceleration_covariance2',\n",
    "       'field.linear_acceleration_covariance3',\n",
    "       'field.linear_acceleration_covariance4',\n",
    "       'field.linear_acceleration_covariance5',\n",
    "       'field.linear_acceleration_covariance6',\n",
    "       'field.linear_acceleration_covariance7',\n",
    "       'field.linear_acceleration_covariance8',\n",
    "       \"field.coordinate_frame\",\"field.source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hy138\\Desktop\\UAV\n",
      "['.vscode', 'best_model.pth', 'BiLSTM.ipynb', 'BiLSTMs.ipynb', 'CNN1D.ipynb', 'CNN1Ds.ipynb', 'data', 'LSTM copy.ipynb', 'LSTM.ipynb', 'LSTMAutoEncoder.ipynb', 'LSTMs.ipynb', 'mLSTM.ipynb', 'sLSTM.ipynb', 'xLSTM.ipynb', 'Yeni klas√∂r']\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = {}\n",
    "flight_topic_list = []\n",
    "topic_list = []\n",
    "all_columns = []\n",
    "df_dict = {}\n",
    "failure_status_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_1_engine_failure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_2_engine_failure\n",
      "carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-16-39-00_1_engine_failure\n",
      "carbonZ_2018-07-30-16-39-00_2_engine_failure\n",
      "carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj\n",
      "carbonZ_2018-09-11-11-56-30_engine_failure\n",
      "carbonZ_2018-09-11-14-22-07_1_engine_failure\n",
      "carbonZ_2018-09-11-14-22-07_2_engine_failure\n",
      "carbonZ_2018-09-11-14-41-51_elevator_failure\n",
      "carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\n",
      "carbonZ_2018-09-11-15-05-11_1_elevator_failure\n",
      "carbonZ_2018-09-11-15-06-34_1_rudder_right_failure\n",
      "carbonZ_2018-09-11-15-06-34_2_rudder_right_failure\n",
      "carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure\n",
      "carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure\n",
      "carbonZ_2018-09-11-17-55-30_1_right_aileron_failure\n",
      "carbonZ_2018-09-11-17-55-30_2_left_aileron_failure\n",
      "carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-14-37-22_2_right_aileron_failure\n",
      "carbonZ_2018-10-05-14-37-22_3_left_aileron_failure\n",
      "carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj\n"
     ]
    }
   ],
   "source": [
    "for i,flight in enumerate(glob.glob(data_path+\"*\")):\n",
    "    \n",
    "    flight_name = os.path.basename(flight)\n",
    "    \n",
    "    if \"no_ground_truth\" in flight_name:\n",
    "        continue\n",
    "    if \"no_failure\" in flight_name:\n",
    "            continue\n",
    "    # This folder has not path-dev csv file\n",
    "    if \"carbonZ_2018-09-11-15-06-34_3_rudder_left_failure\" in flight_name:\n",
    "        continue\n",
    "    print(flight_name)\n",
    "\n",
    "    if flight_name not in flight_topic_list:\n",
    "        flight_topic_list.append(flight_name)\n",
    "    \n",
    "\n",
    "    df_failure = None\n",
    "    failure_duration_start = pd.Timestamp('1970-01-01')\n",
    "    failure_duration_finish = pd.Timestamp('1970-01-01')\n",
    "\n",
    "    dfs = list()\n",
    "    for i , file in enumerate(glob.glob(os.path.join(data_path,flight_name,\"*.csv\"))):\n",
    "        \n",
    "        if any(x in file for x in unused_topic_list):\n",
    "            continue\n",
    "        \n",
    "\n",
    "        if \"failure_status\" in os.path.basename(file):\n",
    "            file_name = os.path.basename(file)\n",
    "            \n",
    "            df = read_data(file)\n",
    "            topic_name = extract_topic_name(flight_name,file_name)\n",
    "\n",
    "            \n",
    "            failure_duration_start = min(df.index)\n",
    "            failure_duration_end = max(df.index)\n",
    "\n",
    "            failure_status_dict[flight_name] = (failure_duration_start,failure_duration_end)\n",
    "\n",
    "            continue\n",
    "        \n",
    "        file_name = os.path.basename(file)\n",
    "\n",
    "        df = read_data(file)\n",
    "        topic_name = extract_topic_name(flight_name,file_name)\n",
    "        for col in unused_column_list:\n",
    "            if col in df.columns:\n",
    "                df.drop(col,axis=1,inplace=True)\n",
    "        new_columns = list(map(lambda x: f\"{topic_name}.{x.replace('field.', '')}\", df.columns))\n",
    "        df = df.set_axis(new_columns, axis=1)\n",
    "       \n",
    "     \n",
    "        \n",
    "        dfs.append(df)\n",
    "\n",
    "   \n",
    "    df_dict[flight_name] = dfs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dict[\"carbonZ_2018-07-18-15-53-31_1_engine_failure\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6619,\n",
       " mavctrl-path_dev.y    0\n",
       " mavctrl-path_dev.z    0\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dict[\"carbonZ_2018-07-18-15-53-31_1_engine_failure\"][0]),df_dict[\"carbonZ_2018-07-18-15-53-31_1_engine_failure\"][0].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'carbonZ_2018-07-18-15-53-31_1_engine_failure': (Timestamp('2018-07-18 19:58:47.129305993'),\n",
       "  Timestamp('2018-07-18 19:59:03.134074845')),\n",
       " 'carbonZ_2018-07-18-15-53-31_2_engine_failure': (Timestamp('2018-07-18 20:01:44.729304590'),\n",
       "  Timestamp('2018-07-18 20:01:59.928010526')),\n",
       " 'carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj': (Timestamp('2018-07-18 20:32:26.878396465'),\n",
       "  Timestamp('2018-07-18 20:32:42.672828037')),\n",
       " 'carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj': (Timestamp('2018-07-18 20:46:37.888445250'),\n",
       "  Timestamp('2018-07-18 20:46:54.288954056')),\n",
       " 'carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj': (Timestamp('2018-07-21 00:35:22.170194'),\n",
       "  Timestamp('2018-07-21 00:35:41.166058768')),\n",
       " 'carbonZ_2018-07-30-16-39-00_1_engine_failure': (Timestamp('2018-07-21 01:00:21.380769776'),\n",
       "  Timestamp('2018-07-21 01:00:36.175981200')),\n",
       " 'carbonZ_2018-07-30-16-39-00_2_engine_failure': (Timestamp('2018-07-21 01:04:00.575490608'),\n",
       "  Timestamp('2018-07-21 01:04:15.194122640')),\n",
       " 'carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj': (Timestamp('2018-07-30 21:16:33.314179152'),\n",
       "  Timestamp('2018-07-30 21:16:49.124017251')),\n",
       " 'carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj': (Timestamp('2018-07-30 21:25:25.757112162'),\n",
       "  Timestamp('2018-07-30 21:25:44.754748978')),\n",
       " 'carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj': (Timestamp('2018-07-30 21:41:15.634859201'),\n",
       "  Timestamp('2018-07-30 21:41:39.036278834')),\n",
       " 'carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj': (Timestamp('2018-07-30 21:49:47.539406723'),\n",
       "  Timestamp('2018-07-30 21:50:09.732858508')),\n",
       " 'carbonZ_2018-09-11-11-56-30_engine_failure': (Timestamp('2018-09-11 16:05:02.419301184'),\n",
       "  Timestamp('2018-09-11 16:05:22.921275104')),\n",
       " 'carbonZ_2018-09-11-14-22-07_1_engine_failure': (Timestamp('2018-09-11 18:26:14.324825782'),\n",
       "  Timestamp('2018-09-11 18:26:23.326658597')),\n",
       " 'carbonZ_2018-09-11-14-22-07_2_engine_failure': (Timestamp('2018-09-11 18:28:20.325374057'),\n",
       "  Timestamp('2018-09-11 18:28:32.326442789')),\n",
       " 'carbonZ_2018-09-11-14-41-51_elevator_failure': (Timestamp('2018-09-11 18:48:33.193239864'),\n",
       "  Timestamp('2018-09-11 18:48:43.702938424')),\n",
       " 'carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure': (Timestamp('2018-09-11 18:57:29.985030040'),\n",
       "  Timestamp('2018-09-11 18:59:37.993673016')),\n",
       " 'carbonZ_2018-09-11-15-05-11_1_elevator_failure': (Timestamp('2018-09-11 19:12:08.654835256'),\n",
       "  Timestamp('2018-09-11 19:12:21.172710456')),\n",
       " 'carbonZ_2018-09-11-15-06-34_1_rudder_right_failure': (Timestamp('2018-09-11 19:17:55.038442264'),\n",
       "  Timestamp('2018-09-11 19:18:09.537019480')),\n",
       " 'carbonZ_2018-09-11-15-06-34_2_rudder_right_failure': (Timestamp('2018-09-11 19:19:58.533612056'),\n",
       "  Timestamp('2018-09-11 19:20:15.542582552')),\n",
       " 'carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure': (Timestamp('2018-09-11 21:36:15.255535472'),\n",
       "  Timestamp('2018-09-11 21:36:42.254787570')),\n",
       " 'carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure': (Timestamp('2018-09-11 21:38:23.751688384'),\n",
       "  Timestamp('2018-09-11 21:38:59.249605331')),\n",
       " 'carbonZ_2018-09-11-17-55-30_1_right_aileron_failure': (Timestamp('2018-09-11 22:04:29.915932200'),\n",
       "  Timestamp('2018-09-11 22:04:50.916117253')),\n",
       " 'carbonZ_2018-09-11-17-55-30_2_left_aileron_failure': (Timestamp('2018-09-11 22:06:18.909972946'),\n",
       "  Timestamp('2018-09-11 22:06:49.930133544')),\n",
       " 'carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj': (Timestamp('2018-10-05 18:58:10.456220968'),\n",
       "  Timestamp('2018-10-05 18:58:19.953767048')),\n",
       " 'carbonZ_2018-10-05-14-37-22_2_right_aileron_failure': (Timestamp('2018-10-05 19:41:12.176343271'),\n",
       "  Timestamp('2018-10-05 19:42:23.177067207')),\n",
       " 'carbonZ_2018-10-05-14-37-22_3_left_aileron_failure': (Timestamp('2018-10-05 19:43:36.175449735'),\n",
       "  Timestamp('2018-10-05 19:44:00.176733447')),\n",
       " 'carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj': (Timestamp('2018-10-05 20:07:04.743298528'),\n",
       "  Timestamp('2018-10-05 20:07:21.746703360')),\n",
       " 'carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj': (Timestamp('2018-10-05 19:59:59.479181792'),\n",
       "  Timestamp('2018-10-05 20:00:12.477307424')),\n",
       " 'carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj': (Timestamp('2018-10-05 20:09:30.477759008'),\n",
       "  Timestamp('2018-10-05 20:09:46.474778400')),\n",
       " 'carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj': (Timestamp('2018-10-18 15:09:31.419064456'),\n",
       "  Timestamp('2018-10-18 15:09:43.416809576')),\n",
       " 'carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj': (Timestamp('2018-10-18 15:09:27.628350888'),\n",
       "  Timestamp('2018-10-18 15:09:38.630940104')),\n",
       " 'carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj': (Timestamp('2018-10-18 15:09:39.270985672'),\n",
       "  Timestamp('2018-10-18 15:09:53.269556136')),\n",
       " 'carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj': (Timestamp('2018-10-18 15:19:45.268859464'),\n",
       "  Timestamp('2018-10-18 15:20:04.768932520')),\n",
       " 'carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj': (Timestamp('2018-10-18 15:08:37.878232904'),\n",
       "  Timestamp('2018-10-18 15:08:45.380344968')),\n",
       " 'carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj': (Timestamp('2018-10-18 15:12:17.734694408'),\n",
       "  Timestamp('2018-10-18 15:12:31.237794664'))}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failure_status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key,data in df_dict.items():\n",
    "#     df_list = []\n",
    "#     for df in data:\n",
    "#         for col in df.columns:\n",
    "#             df[f'{col}_copy'] = df[col] \n",
    "    \n",
    "#         df_list.append(df)\n",
    "#     df_dict[key] = df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_1_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-18-15-53-31_2_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-16-39-00_1_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-30-16-39-00_2_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-09-11-11-56-30_engine_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-22-07_1_engine_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-22-07_2_engine_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-41-51_elevator_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\n",
      "13\n",
      "carbonZ_2018-09-11-15-05-11_1_elevator_failure\n",
      "13\n",
      "carbonZ_2018-09-11-15-06-34_1_rudder_right_failure\n",
      "13\n",
      "carbonZ_2018-09-11-15-06-34_2_rudder_right_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-55-30_1_right_aileron_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-55-30_2_left_aileron_failure\n",
      "13\n",
      "carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-05-14-37-22_2_right_aileron_failure\n",
      "13\n",
      "carbonZ_2018-10-05-14-37-22_3_left_aileron_failure\n",
      "13\n",
      "carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for key, df in df_dict.items():\n",
    "    print(key)\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, data in df_dict.items():\n",
    "    if \"rudder_zero__left_aileron_failure\" in key:\n",
    "        \n",
    "\n",
    "        df_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\"] = df_dict['carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure'].copy()\n",
    " \n",
    " \n",
    "\n",
    "        df_dict[\"carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\"] = df_dict['carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure'].copy()\n",
    "\n",
    "   \n",
    "\n",
    "        del df_dict['carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure']\n",
    "\n",
    "\n",
    "        break\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_1_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-18-15-53-31_2_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-16-39-00_1_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-30-16-39-00_2_engine_failure\n",
      "13\n",
      "carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-09-11-11-56-30_engine_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-22-07_1_engine_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-22-07_2_engine_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-41-51_elevator_failure\n",
      "13\n",
      "carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\n",
      "13\n",
      "carbonZ_2018-09-11-15-05-11_1_elevator_failure\n",
      "13\n",
      "carbonZ_2018-09-11-15-06-34_1_rudder_right_failure\n",
      "13\n",
      "carbonZ_2018-09-11-15-06-34_2_rudder_right_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-55-30_1_right_aileron_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-55-30_2_left_aileron_failure\n",
      "13\n",
      "carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-05-14-37-22_2_right_aileron_failure\n",
      "13\n",
      "carbonZ_2018-10-05-14-37-22_3_left_aileron_failure\n",
      "13\n",
      "carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj\n",
      "13\n",
      "carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\n",
      "13\n",
      "carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for key, df in df_dict.items():\n",
    "    print(key)\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, data in failure_status_dict.items():\n",
    "    if \"rudder_zero__left_aileron_failure\" in key:\n",
    "        failure_status_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\"] = failure_status_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure\"]\n",
    "        failure_status_dict[\"carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\"] = failure_status_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure\"]\n",
    "        del failure_status_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure\"]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mavros-imu-atm_pressure.fluid_pressure'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\"][1].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mavctrl-path_dev.y</th>\n",
       "      <th>mavctrl-path_dev.z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.820860792</th>\n",
       "      <td>141.284886</td>\n",
       "      <td>14.438259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.840917944</th>\n",
       "      <td>141.284886</td>\n",
       "      <td>14.438259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.861388914</th>\n",
       "      <td>141.284886</td>\n",
       "      <td>14.438259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.880812090</th>\n",
       "      <td>141.284886</td>\n",
       "      <td>14.438259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.901712334</th>\n",
       "      <td>144.798264</td>\n",
       "      <td>14.394005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.108804965</th>\n",
       "      <td>8.337481</td>\n",
       "      <td>21.670137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.130722624</th>\n",
       "      <td>8.337481</td>\n",
       "      <td>21.670137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.149511356</th>\n",
       "      <td>8.337481</td>\n",
       "      <td>21.670137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.171009244</th>\n",
       "      <td>8.337481</td>\n",
       "      <td>21.670137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.190838957</th>\n",
       "      <td>8.337481</td>\n",
       "      <td>21.670137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6619 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mavctrl-path_dev.y  mavctrl-path_dev.z\n",
       "timestamp                                                            \n",
       "2018-07-18 19:56:50.820860792          141.284886           14.438259\n",
       "2018-07-18 19:56:50.840917944          141.284886           14.438259\n",
       "2018-07-18 19:56:50.861388914          141.284886           14.438259\n",
       "2018-07-18 19:56:50.880812090          141.284886           14.438259\n",
       "2018-07-18 19:56:50.901712334          144.798264           14.394005\n",
       "...                                           ...                 ...\n",
       "2018-07-18 19:59:03.108804965            8.337481           21.670137\n",
       "2018-07-18 19:59:03.130722624            8.337481           21.670137\n",
       "2018-07-18 19:59:03.149511356            8.337481           21.670137\n",
       "2018-07-18 19:59:03.171009244            8.337481           21.670137\n",
       "2018-07-18 19:59:03.190838957            8.337481           21.670137\n",
       "\n",
       "[6619 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict[\"carbonZ_2018-07-18-15-53-31_1_engine_failure\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_topic_list.append(\"carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\")\n",
    "flight_topic_list.append(\"carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\")\n",
    "flight_topic_list.remove(\"carbonZ_2018-09-11-17-27-13_1_rudder_zero__left_aileron_failure\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flight_topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2018-07-18 19:58:47.129305993'),\n",
       " Timestamp('2018-07-18 19:59:03.134074845'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failure_status_dict[\"carbonZ_2018-07-18-15-53-31_1_engine_failure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                               mavctrl-path_dev.y  mavctrl-path_dev.z\n",
       " timestamp                                                            \n",
       " 2018-09-11 18:55:44.748852696          226.349940          -16.530594\n",
       " 2018-09-11 18:55:44.768693912          226.349940          -16.530594\n",
       " 2018-09-11 18:55:44.793884088          207.499653          -16.530594\n",
       " 2018-09-11 18:55:44.809683352          207.499366          -16.530594\n",
       " 2018-09-11 18:55:44.831660792          207.499366          -16.530594\n",
       " ...                                           ...                 ...\n",
       " 2018-09-11 18:59:38.029159576            5.033987            3.622774\n",
       " 2018-09-11 18:59:38.049874936            5.033987            3.622774\n",
       " 2018-09-11 18:59:38.073666904            5.033987            3.622774\n",
       " 2018-09-11 18:59:38.089277368            4.698973            4.732197\n",
       " 2018-09-11 18:59:38.109449304            4.698973            4.732197\n",
       " \n",
       " [11669 rows x 2 columns],\n",
       "                                mavros-imu-atm_pressure.fluid_pressure\n",
       " timestamp                                                            \n",
       " 2018-09-11 18:55:44.739465240                            97497.888184\n",
       " 2018-09-11 18:55:44.821564344                            97497.888184\n",
       " 2018-09-11 18:55:44.938280408                            97485.864258\n",
       " 2018-09-11 18:55:45.038424536                            97479.797363\n",
       " 2018-09-11 18:55:45.123441848                            97479.797363\n",
       " ...                                                               ...\n",
       " 2018-09-11 18:59:37.626786712                            97689.019775\n",
       " 2018-09-11 18:59:37.734683928                            97696.691895\n",
       " 2018-09-11 18:59:37.833203736                            97704.296875\n",
       " 2018-09-11 18:59:37.924305528                            97704.296875\n",
       " 2018-09-11 18:59:38.018054744                            97711.303711\n",
       " \n",
       " [2334 rows x 1 columns],\n",
       "                                mavros-imu-data.orientation.x  \\\n",
       " timestamp                                                      \n",
       " 2018-09-11 18:55:44.815328696                      -0.005702   \n",
       " 2018-09-11 18:55:45.302239320                      -0.027779   \n",
       " 2018-09-11 18:55:45.691413272                      -0.004409   \n",
       " 2018-09-11 18:55:46.060852152                       0.022710   \n",
       " 2018-09-11 18:55:46.443526008                       0.048855   \n",
       " ...                                                      ...   \n",
       " 2018-09-11 18:59:36.240079096                      -0.120860   \n",
       " 2018-09-11 18:59:36.683336696                      -0.126285   \n",
       " 2018-09-11 18:59:37.197629496                      -0.091700   \n",
       " 2018-09-11 18:59:37.632176952                      -0.057563   \n",
       " 2018-09-11 18:59:37.978509688                      -0.021296   \n",
       " \n",
       "                                mavros-imu-data.orientation.y  \\\n",
       " timestamp                                                      \n",
       " 2018-09-11 18:55:44.815328696                      -0.024376   \n",
       " 2018-09-11 18:55:45.302239320                      -0.232940   \n",
       " 2018-09-11 18:55:45.691413272                      -0.276876   \n",
       " 2018-09-11 18:55:46.060852152                      -0.314628   \n",
       " 2018-09-11 18:55:46.443526008                      -0.314407   \n",
       " ...                                                      ...   \n",
       " 2018-09-11 18:59:36.240079096                      -0.012206   \n",
       " 2018-09-11 18:59:36.683336696                      -0.054971   \n",
       " 2018-09-11 18:59:37.197629496                      -0.115521   \n",
       " 2018-09-11 18:59:37.632176952                      -0.169438   \n",
       " 2018-09-11 18:59:37.978509688                      -0.193174   \n",
       " \n",
       "                                mavros-imu-data.orientation.z  \\\n",
       " timestamp                                                      \n",
       " 2018-09-11 18:55:44.815328696                      -0.986004   \n",
       " 2018-09-11 18:55:45.302239320                      -0.958538   \n",
       " 2018-09-11 18:55:45.691413272                      -0.941414   \n",
       " 2018-09-11 18:55:46.060852152                      -0.920235   \n",
       " 2018-09-11 18:55:46.443526008                      -0.909142   \n",
       " ...                                                      ...   \n",
       " 2018-09-11 18:59:36.240079096                      -0.003183   \n",
       " 2018-09-11 18:59:36.683336696                       0.015267   \n",
       " 2018-09-11 18:59:37.197629496                       0.006238   \n",
       " 2018-09-11 18:59:37.632176952                       0.018558   \n",
       " 2018-09-11 18:59:37.978509688                       0.032839   \n",
       " \n",
       "                                mavros-imu-data.orientation.w  \\\n",
       " timestamp                                                      \n",
       " 2018-09-11 18:55:44.815328696                      -0.164833   \n",
       " 2018-09-11 18:55:45.302239320                      -0.161779   \n",
       " 2018-09-11 18:55:45.691413272                      -0.192508   \n",
       " 2018-09-11 18:55:46.060852152                      -0.231648   \n",
       " 2018-09-11 18:55:46.443526008                      -0.268742   \n",
       " ...                                                      ...   \n",
       " 2018-09-11 18:59:36.240079096                      -0.992589   \n",
       " 2018-09-11 18:59:36.683336696                      -0.990352   \n",
       " 2018-09-11 18:59:37.197629496                      -0.989044   \n",
       " 2018-09-11 18:59:37.632176952                      -0.983683   \n",
       " 2018-09-11 18:59:37.978509688                      -0.980384   \n",
       " \n",
       "                                mavros-imu-data.angular_velocity.x  \\\n",
       " timestamp                                                           \n",
       " 2018-09-11 18:55:44.815328696                            1.198352   \n",
       " 2018-09-11 18:55:45.302239320                            0.344504   \n",
       " 2018-09-11 18:55:45.691413272                            0.204759   \n",
       " 2018-09-11 18:55:46.060852152                           -0.013796   \n",
       " 2018-09-11 18:55:46.443526008                           -0.006801   \n",
       " ...                                                           ...   \n",
       " 2018-09-11 18:59:36.240079096                            0.044647   \n",
       " 2018-09-11 18:59:36.683336696                            0.073532   \n",
       " 2018-09-11 18:59:37.197629496                           -0.298716   \n",
       " 2018-09-11 18:59:37.632176952                           -0.146525   \n",
       " 2018-09-11 18:59:37.978509688                           -0.147029   \n",
       " \n",
       "                                mavros-imu-data.angular_velocity.y  \\\n",
       " timestamp                                                           \n",
       " 2018-09-11 18:55:44.815328696                           -0.003927   \n",
       " 2018-09-11 18:55:45.302239320                            0.093244   \n",
       " 2018-09-11 18:55:45.691413272                            0.108409   \n",
       " 2018-09-11 18:55:46.060852152                            0.087250   \n",
       " 2018-09-11 18:55:46.443526008                            0.039049   \n",
       " ...                                                           ...   \n",
       " 2018-09-11 18:59:36.240079096                            0.161116   \n",
       " 2018-09-11 18:59:36.683336696                            0.198077   \n",
       " 2018-09-11 18:59:37.197629496                            0.236909   \n",
       " 2018-09-11 18:59:37.632176952                            0.182599   \n",
       " 2018-09-11 18:59:37.978509688                            0.090268   \n",
       " \n",
       "                                mavros-imu-data.angular_velocity.z  \\\n",
       " timestamp                                                           \n",
       " 2018-09-11 18:55:44.815328696                           -0.046569   \n",
       " 2018-09-11 18:55:45.302239320                           -0.081719   \n",
       " 2018-09-11 18:55:45.691413272                           -0.277471   \n",
       " 2018-09-11 18:55:46.060852152                           -0.250878   \n",
       " 2018-09-11 18:55:46.443526008                           -0.169687   \n",
       " ...                                                           ...   \n",
       " 2018-09-11 18:59:36.240079096                           -0.134020   \n",
       " 2018-09-11 18:59:36.683336696                           -0.064672   \n",
       " 2018-09-11 18:59:37.197629496                            0.000992   \n",
       " 2018-09-11 18:59:37.632176952                           -0.147860   \n",
       " 2018-09-11 18:59:37.978509688                           -0.082429   \n",
       " \n",
       "                                mavros-imu-data.linear_acceleration.x  \\\n",
       " timestamp                                                              \n",
       " 2018-09-11 18:55:44.815328696                              -0.402073   \n",
       " 2018-09-11 18:55:45.302239320                              -1.353318   \n",
       " 2018-09-11 18:55:45.691413272                              -1.637711   \n",
       " 2018-09-11 18:55:46.060852152                              -1.980943   \n",
       " 2018-09-11 18:55:46.443526008                              -2.216303   \n",
       " ...                                                              ...   \n",
       " 2018-09-11 18:59:36.240079096                              -0.529559   \n",
       " 2018-09-11 18:59:36.683336696                              -1.108151   \n",
       " 2018-09-11 18:59:37.197629496                              -1.353318   \n",
       " 2018-09-11 18:59:37.632176952                              -1.520031   \n",
       " 2018-09-11 18:59:37.978509688                              -1.696550   \n",
       " \n",
       "                                mavros-imu-data.linear_acceleration.y  \\\n",
       " timestamp                                                              \n",
       " 2018-09-11 18:55:44.815328696                               0.127486   \n",
       " 2018-09-11 18:55:45.302239320                               0.539366   \n",
       " 2018-09-11 18:55:45.691413272                               0.480526   \n",
       " 2018-09-11 18:55:46.060852152                               0.647239   \n",
       " 2018-09-11 18:55:46.443526008                               0.931632   \n",
       " ...                                                              ...   \n",
       " 2018-09-11 18:59:36.240079096                               0.323619   \n",
       " 2018-09-11 18:59:36.683336696                               0.421686   \n",
       " 2018-09-11 18:59:37.197629496                               0.823759   \n",
       " 2018-09-11 18:59:37.632176952                               0.657046   \n",
       " 2018-09-11 18:59:37.978509688                               0.049033   \n",
       " \n",
       "                                mavros-imu-data.linear_acceleration.z  \n",
       " timestamp                                                             \n",
       " 2018-09-11 18:55:44.815328696                               8.463139  \n",
       " 2018-09-11 18:55:45.302239320                               7.874740  \n",
       " 2018-09-11 18:55:45.691413272                               4.138406  \n",
       " 2018-09-11 18:55:46.060852152                               5.864377  \n",
       " 2018-09-11 18:55:46.443526008                               7.757060  \n",
       " ...                                                              ...  \n",
       " 2018-09-11 18:59:36.240079096                               7.678607  \n",
       " 2018-09-11 18:59:36.683336696                               6.246836  \n",
       " 2018-09-11 18:59:37.197629496                               5.060231  \n",
       " 2018-09-11 18:59:37.632176952                               4.501252  \n",
       " 2018-09-11 18:59:37.978509688                               7.139241  \n",
       " \n",
       " [574 rows x 10 columns],\n",
       "                                mavros-imu-temperature.temperature\n",
       " timestamp                                                        \n",
       " 2018-09-11 18:55:44.739417624                               41.89\n",
       " 2018-09-11 18:55:44.821516824                               41.89\n",
       " 2018-09-11 18:55:44.938254904                               41.88\n",
       " 2018-09-11 18:55:45.038322712                               41.87\n",
       " 2018-09-11 18:55:45.123392344                               41.87\n",
       " ...                                                           ...\n",
       " 2018-09-11 18:59:37.626645912                               37.06\n",
       " 2018-09-11 18:59:37.734617272                               37.05\n",
       " 2018-09-11 18:59:37.833124216                               37.05\n",
       " 2018-09-11 18:59:37.924084376                               37.05\n",
       " 2018-09-11 18:59:38.018026488                               37.06\n",
       " \n",
       " [2334 rows x 1 columns],\n",
       "                                mavros-nav_info-airspeed.measured\n",
       " timestamp                                                       \n",
       " 2018-09-11 18:55:44.751156440                                0.0\n",
       " 2018-09-11 18:55:44.811619032                                0.0\n",
       " 2018-09-11 18:55:44.837812728                                0.0\n",
       " 2018-09-11 18:55:44.900119192                                0.0\n",
       " 2018-09-11 18:55:44.965182744                                0.0\n",
       " ...                                                          ...\n",
       " 2018-09-11 18:59:37.860557656                                0.0\n",
       " 2018-09-11 18:59:37.920680280                                0.0\n",
       " 2018-09-11 18:59:37.974660856                                0.0\n",
       " 2018-09-11 18:59:38.013118776                                0.0\n",
       " 2018-09-11 18:59:38.073726968                                0.0\n",
       " \n",
       " [4348 rows x 1 columns],\n",
       "                                mavros-nav_info-errors.alt_error  \\\n",
       " timestamp                                                         \n",
       " 2018-09-11 18:55:44.733756664                         -0.300000   \n",
       " 2018-09-11 18:55:44.747403736                         -0.300000   \n",
       " 2018-09-11 18:55:44.786480952                         -0.300000   \n",
       " 2018-09-11 18:55:44.836539000                         -0.430000   \n",
       " 2018-09-11 18:55:44.898553720                         -0.430000   \n",
       " ...                                                         ...   \n",
       " 2018-09-11 18:59:37.860320760                         20.289999   \n",
       " 2018-09-11 18:59:37.917356952                         20.289999   \n",
       " 2018-09-11 18:59:37.969003608                         20.859999   \n",
       " 2018-09-11 18:59:38.011773848                         20.859999   \n",
       " 2018-09-11 18:59:38.057258392                         21.469999   \n",
       " \n",
       "                                mavros-nav_info-errors.aspd_error  \\\n",
       " timestamp                                                          \n",
       " 2018-09-11 18:55:44.733756664                        -681.648438   \n",
       " 2018-09-11 18:55:44.747403736                        -681.648438   \n",
       " 2018-09-11 18:55:44.786480952                        -681.648438   \n",
       " 2018-09-11 18:55:44.836539000                        -684.812134   \n",
       " 2018-09-11 18:55:44.898553720                        -684.812134   \n",
       " ...                                                          ...   \n",
       " 2018-09-11 18:59:37.860320760                         -41.753960   \n",
       " 2018-09-11 18:59:37.917356952                         -41.753960   \n",
       " 2018-09-11 18:59:37.969003608                         -41.753960   \n",
       " 2018-09-11 18:59:38.011773848                         -41.753960   \n",
       " 2018-09-11 18:59:38.057258392                         -42.135429   \n",
       " \n",
       "                                mavros-nav_info-errors.xtrack_error  \\\n",
       " timestamp                                                            \n",
       " 2018-09-11 18:55:44.733756664                           -40.078075   \n",
       " 2018-09-11 18:55:44.747403736                           -40.078075   \n",
       " 2018-09-11 18:55:44.786480952                           -40.078075   \n",
       " 2018-09-11 18:55:44.836539000                           -37.936432   \n",
       " 2018-09-11 18:55:44.898553720                           -37.936432   \n",
       " ...                                                            ...   \n",
       " 2018-09-11 18:59:37.860320760                             5.042744   \n",
       " 2018-09-11 18:59:37.917356952                             5.042744   \n",
       " 2018-09-11 18:59:37.969003608                             4.909161   \n",
       " 2018-09-11 18:59:38.011773848                             4.909161   \n",
       " 2018-09-11 18:59:38.057258392                             4.764447   \n",
       " \n",
       "                                mavros-nav_info-errors.wp_dist  \n",
       " timestamp                                                      \n",
       " 2018-09-11 18:55:44.733756664                               4  \n",
       " 2018-09-11 18:55:44.747403736                               4  \n",
       " 2018-09-11 18:55:44.786480952                               4  \n",
       " 2018-09-11 18:55:44.836539000                               6  \n",
       " 2018-09-11 18:55:44.898553720                               6  \n",
       " ...                                                       ...  \n",
       " 2018-09-11 18:59:37.860320760                             165  \n",
       " 2018-09-11 18:59:37.917356952                             165  \n",
       " 2018-09-11 18:59:37.969003608                             165  \n",
       " 2018-09-11 18:59:38.011773848                             165  \n",
       " 2018-09-11 18:59:38.057258392                             165  \n",
       " \n",
       " [4440 rows x 4 columns],\n",
       "                                mavros-nav_info-pitch.measured\n",
       " timestamp                                                    \n",
       " 2018-09-11 18:55:44.733683000                        0.797831\n",
       " 2018-09-11 18:55:44.747360408                        0.797831\n",
       " 2018-09-11 18:55:44.786440792                        0.797831\n",
       " 2018-09-11 18:55:44.836514104                        0.183815\n",
       " 2018-09-11 18:55:44.898461464                        0.183815\n",
       " ...                                                       ...\n",
       " 2018-09-11 18:59:37.860291352                      -19.601870\n",
       " 2018-09-11 18:59:37.917281208                      -19.601870\n",
       " 2018-09-11 18:59:37.967814840                      -19.601870\n",
       " 2018-09-11 18:59:38.011744568                      -22.344082\n",
       " 2018-09-11 18:59:38.057027960                      -22.344082\n",
       " \n",
       " [4440 rows x 1 columns],\n",
       "                                mavros-nav_info-roll.measured\n",
       " timestamp                                                   \n",
       " 2018-09-11 18:55:44.733653240                     -11.961676\n",
       " 2018-09-11 18:55:44.747348664                     -11.961676\n",
       " 2018-09-11 18:55:44.785582360                     -11.961676\n",
       " 2018-09-11 18:55:44.836487064                       2.863091\n",
       " 2018-09-11 18:55:44.898428952                       2.863091\n",
       " ...                                                      ...\n",
       " 2018-09-11 18:59:37.917263992                       6.519372\n",
       " 2018-09-11 18:59:37.967593688                       6.519372\n",
       " 2018-09-11 18:59:38.011726584                       1.801064\n",
       " 2018-09-11 18:59:38.056942072                       1.801064\n",
       " 2018-09-11 18:59:38.118799704                       1.801064\n",
       " \n",
       " [4441 rows x 1 columns],\n",
       "                                mavros-nav_info-velocity.des_x  \\\n",
       " timestamp                                                       \n",
       " 2018-09-11 18:55:44.751172664                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.811629816                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.837833816                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.900179000                    1.600000e+01   \n",
       " 2018-09-11 18:55:44.964750008                    1.600000e+01   \n",
       " ...                                                       ...   \n",
       " 2018-09-11 18:59:37.860689592                    3.199951e-15   \n",
       " 2018-09-11 18:59:37.920835864                    3.169954e-15   \n",
       " 2018-09-11 18:59:37.974727832                    3.169954e-15   \n",
       " 2018-09-11 18:59:38.013186072                    3.169954e-15   \n",
       " 2018-09-11 18:59:38.073708088                    3.169954e-15   \n",
       " \n",
       "                                mavros-nav_info-velocity.des_y  \\\n",
       " timestamp                                                       \n",
       " 2018-09-11 18:55:44.751172664                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.811629816                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.837833816                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.900179000                   -3.552714e-15   \n",
       " 2018-09-11 18:55:44.964750008                   -3.552714e-15   \n",
       " ...                                                       ...   \n",
       " 2018-09-11 18:59:37.860689592                    1.580050e+01   \n",
       " 2018-09-11 18:59:37.920835864                    1.576931e+01   \n",
       " 2018-09-11 18:59:37.974727832                    1.576931e+01   \n",
       " 2018-09-11 18:59:38.013186072                    1.576931e+01   \n",
       " 2018-09-11 18:59:38.073708088                    1.576931e+01   \n",
       " \n",
       "                                mavros-nav_info-velocity.des_z  \\\n",
       " timestamp                                                       \n",
       " 2018-09-11 18:55:44.751172664                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.811629816                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.837833816                    0.000000e+00   \n",
       " 2018-09-11 18:55:44.900179000                    1.959435e-15   \n",
       " 2018-09-11 18:55:44.964750008                    1.959435e-15   \n",
       " ...                                                       ...   \n",
       " 2018-09-11 18:59:37.860689592                    2.518796e+00   \n",
       " 2018-09-11 18:59:37.920835864                    2.707196e+00   \n",
       " 2018-09-11 18:59:37.974727832                    2.707196e+00   \n",
       " 2018-09-11 18:59:38.013186072                    2.707196e+00   \n",
       " 2018-09-11 18:59:38.073708088                    2.707196e+00   \n",
       " \n",
       "                                mavros-nav_info-velocity.meas_x  \\\n",
       " timestamp                                                        \n",
       " 2018-09-11 18:55:44.751172664                         5.448525   \n",
       " 2018-09-11 18:55:44.811629816                         5.435284   \n",
       " 2018-09-11 18:55:44.837833816                         5.446200   \n",
       " 2018-09-11 18:55:44.900179000                         5.503401   \n",
       " 2018-09-11 18:55:44.964750008                         5.601675   \n",
       " ...                                                        ...   \n",
       " 2018-09-11 18:59:37.860689592                        -1.700567   \n",
       " 2018-09-11 18:59:37.920835864                        -1.696898   \n",
       " 2018-09-11 18:59:37.974727832                        -1.686711   \n",
       " 2018-09-11 18:59:38.013186072                        -1.687362   \n",
       " 2018-09-11 18:59:38.073708088                        -1.674494   \n",
       " \n",
       "                                mavros-nav_info-velocity.meas_y  \\\n",
       " timestamp                                                        \n",
       " 2018-09-11 18:55:44.751172664                       -20.630127   \n",
       " 2018-09-11 18:55:44.811629816                       -20.569389   \n",
       " 2018-09-11 18:55:44.837833816                       -20.520662   \n",
       " 2018-09-11 18:55:44.900179000                       -20.415331   \n",
       " 2018-09-11 18:55:44.964750008                       -20.283449   \n",
       " ...                                                        ...   \n",
       " 2018-09-11 18:59:37.860689592                        16.762850   \n",
       " 2018-09-11 18:59:37.920835864                        16.808422   \n",
       " 2018-09-11 18:59:37.974727832                        16.851337   \n",
       " 2018-09-11 18:59:38.013186072                        16.912659   \n",
       " 2018-09-11 18:59:38.073708088                        17.052553   \n",
       " \n",
       "                                mavros-nav_info-velocity.meas_z  \n",
       " timestamp                                                       \n",
       " 2018-09-11 18:55:44.751172664                        -1.310496  \n",
       " 2018-09-11 18:55:44.811629816                        -1.264987  \n",
       " 2018-09-11 18:55:44.837833816                        -1.268989  \n",
       " 2018-09-11 18:55:44.900179000                        -1.315569  \n",
       " 2018-09-11 18:55:44.964750008                        -1.351536  \n",
       " ...                                                        ...  \n",
       " 2018-09-11 18:59:37.860689592                         5.994641  \n",
       " 2018-09-11 18:59:37.920835864                         6.185676  \n",
       " 2018-09-11 18:59:37.974727832                         6.284477  \n",
       " 2018-09-11 18:59:38.013186072                         6.342451  \n",
       " 2018-09-11 18:59:38.073708088                         6.416114  \n",
       " \n",
       " [4348 rows x 6 columns],\n",
       "                                mavros-nav_info-yaw.measured\n",
       " timestamp                                                  \n",
       " 2018-09-11 18:55:44.733721848                    -70.156189\n",
       " 2018-09-11 18:55:44.747371832                    -70.156189\n",
       " 2018-09-11 18:55:44.786465720                    -70.156189\n",
       " 2018-09-11 18:55:44.836527032                    -71.014297\n",
       " 2018-09-11 18:55:44.898478872                    -71.014297\n",
       " ...                                                     ...\n",
       " 2018-09-11 18:59:37.860307032                     91.034180\n",
       " 2018-09-11 18:59:37.917339416                     91.034180\n",
       " 2018-09-11 18:59:37.967848504                     91.034180\n",
       " 2018-09-11 18:59:38.011759000                     93.481194\n",
       " 2018-09-11 18:59:38.057181560                     93.481194\n",
       " \n",
       " [4440 rows x 1 columns],\n",
       "                                mavros-time_reference.time_ref\n",
       " timestamp                                                    \n",
       " 2018-09-11 18:55:44.736821976             1536692377361000000\n",
       " 2018-09-11 18:55:45.206083608             1536692377841000000\n",
       " 2018-09-11 18:55:45.815905752             1536692378440000000\n",
       " 2018-09-11 18:55:46.251725048             1536692378879000000\n",
       " 2018-09-11 18:55:46.631150328             1536692379261000000\n",
       " ...                                                       ...\n",
       " 2018-09-11 18:59:35.671488152             1536692608301000000\n",
       " 2018-09-11 18:59:36.138563064             1536692608778000000\n",
       " 2018-09-11 18:59:36.746569464             1536692609360000000\n",
       " 2018-09-11 18:59:37.272394168             1536692609917000000\n",
       " 2018-09-11 18:59:37.862205048             1536692610500000000\n",
       " \n",
       " [477 rows x 1 columns],\n",
       "                                mavros-vfr_hud.airspeed  \\\n",
       " timestamp                                                \n",
       " 2018-09-11 18:55:44.813606648                      0.0   \n",
       " 2018-09-11 18:55:45.288424728                      0.0   \n",
       " 2018-09-11 18:55:45.689541272                      0.0   \n",
       " 2018-09-11 18:55:46.059403576                      0.0   \n",
       " 2018-09-11 18:55:46.439877208                      0.0   \n",
       " ...                                                ...   \n",
       " 2018-09-11 18:59:36.236687256                      0.0   \n",
       " 2018-09-11 18:59:36.682041176                      0.0   \n",
       " 2018-09-11 18:59:37.193585176                      0.0   \n",
       " 2018-09-11 18:59:37.628674168                      0.0   \n",
       " 2018-09-11 18:59:37.976107736                      0.0   \n",
       " \n",
       "                                mavros-vfr_hud.groundspeed  \\\n",
       " timestamp                                                   \n",
       " 2018-09-11 18:55:44.813606648                   21.275387   \n",
       " 2018-09-11 18:55:45.288424728                   20.532625   \n",
       " 2018-09-11 18:55:45.689541272                   19.939848   \n",
       " 2018-09-11 18:55:46.059403576                   19.413969   \n",
       " 2018-09-11 18:55:46.439877208                   19.055178   \n",
       " ...                                                   ...   \n",
       " 2018-09-11 18:59:36.236687256                   17.511122   \n",
       " 2018-09-11 18:59:36.682041176                   17.207243   \n",
       " 2018-09-11 18:59:37.193585176                   16.966467   \n",
       " 2018-09-11 18:59:37.628674168                   16.777716   \n",
       " 2018-09-11 18:59:37.976107736                   16.935541   \n",
       " \n",
       "                                mavros-vfr_hud.heading  \\\n",
       " timestamp                                               \n",
       " 2018-09-11 18:55:44.813606648                     288   \n",
       " 2018-09-11 18:55:45.288424728                     288   \n",
       " 2018-09-11 18:55:45.689541272                     291   \n",
       " 2018-09-11 18:55:46.059403576                     294   \n",
       " 2018-09-11 18:55:46.439877208                     298   \n",
       " ...                                               ...   \n",
       " 2018-09-11 18:59:36.236687256                      89   \n",
       " 2018-09-11 18:59:36.682041176                      90   \n",
       " 2018-09-11 18:59:37.193585176                      89   \n",
       " 2018-09-11 18:59:37.628674168                      91   \n",
       " 2018-09-11 18:59:37.976107736                      93   \n",
       " \n",
       "                                mavros-vfr_hud.throttle  \\\n",
       " timestamp                                                \n",
       " 2018-09-11 18:55:44.813606648                     0.42   \n",
       " 2018-09-11 18:55:45.288424728                     0.31   \n",
       " 2018-09-11 18:55:45.689541272                     0.19   \n",
       " 2018-09-11 18:55:46.059403576                     0.13   \n",
       " 2018-09-11 18:55:46.439877208                     0.07   \n",
       " ...                                                ...   \n",
       " 2018-09-11 18:59:36.236687256                     0.28   \n",
       " 2018-09-11 18:59:36.682041176                     0.13   \n",
       " 2018-09-11 18:59:37.193585176                     0.02   \n",
       " 2018-09-11 18:59:37.628674168                     0.00   \n",
       " 2018-09-11 18:59:37.976107736                     0.00   \n",
       " \n",
       "                                mavros-vfr_hud.altitude  mavros-vfr_hud.climb  \n",
       " timestamp                                                                     \n",
       " 2018-09-11 18:55:44.813606648               410.979980              2.028334  \n",
       " 2018-09-11 18:55:45.288424728               411.619995              5.472738  \n",
       " 2018-09-11 18:55:45.689541272               411.729980              2.834411  \n",
       " 2018-09-11 18:55:46.059403576               411.119995             -0.804450  \n",
       " 2018-09-11 18:55:46.439877208               409.899994             -2.400985  \n",
       " ...                                                ...                   ...  \n",
       " 2018-09-11 18:59:36.236687256               393.539978              3.508874  \n",
       " 2018-09-11 18:59:36.682041176               393.690002              2.542262  \n",
       " 2018-09-11 18:59:37.193585176               393.039978              0.578386  \n",
       " 2018-09-11 18:59:37.628674168               391.380005             -2.801878  \n",
       " 2018-09-11 18:59:37.976107736               389.500000             -4.096410  \n",
       " \n",
       " [574 rows x 6 columns],\n",
       "                                mavros-wind_estimation.twist.linear.x  \\\n",
       " timestamp                                                              \n",
       " 2018-09-11 18:55:44.735729400                              -0.244657   \n",
       " 2018-09-11 18:55:45.204481688                              -0.246626   \n",
       " 2018-09-11 18:55:45.815334840                              -0.256024   \n",
       " 2018-09-11 18:55:46.251742712                              -0.263208   \n",
       " 2018-09-11 18:55:46.630086456                              -0.272890   \n",
       " ...                                                              ...   \n",
       " 2018-09-11 18:59:35.669880760                              -0.392364   \n",
       " 2018-09-11 18:59:36.137095000                              -0.391501   \n",
       " 2018-09-11 18:59:36.745169944                              -0.392030   \n",
       " 2018-09-11 18:59:37.272128440                              -0.391855   \n",
       " 2018-09-11 18:59:37.860959800                              -0.388418   \n",
       " \n",
       "                                mavros-wind_estimation.twist.linear.y  \\\n",
       " timestamp                                                              \n",
       " 2018-09-11 18:55:44.735729400                               1.064529   \n",
       " 2018-09-11 18:55:45.204481688                               1.066408   \n",
       " 2018-09-11 18:55:45.815334840                               1.059074   \n",
       " 2018-09-11 18:55:46.251742712                               1.054658   \n",
       " 2018-09-11 18:55:46.630086456                               1.050315   \n",
       " ...                                                              ...   \n",
       " 2018-09-11 18:59:35.669880760                               1.266036   \n",
       " 2018-09-11 18:59:36.137095000                               1.265149   \n",
       " 2018-09-11 18:59:36.745169944                               1.257739   \n",
       " 2018-09-11 18:59:37.272128440                               1.251944   \n",
       " 2018-09-11 18:59:37.860959800                               1.251336   \n",
       " \n",
       "                                mavros-wind_estimation.twist.linear.z  \n",
       " timestamp                                                             \n",
       " 2018-09-11 18:55:44.735729400                                    0.0  \n",
       " 2018-09-11 18:55:45.204481688                                    0.0  \n",
       " 2018-09-11 18:55:45.815334840                                    0.0  \n",
       " 2018-09-11 18:55:46.251742712                                    0.0  \n",
       " 2018-09-11 18:55:46.630086456                                    0.0  \n",
       " ...                                                              ...  \n",
       " 2018-09-11 18:59:35.669880760                                    0.0  \n",
       " 2018-09-11 18:59:36.137095000                                    0.0  \n",
       " 2018-09-11 18:59:36.745169944                                    0.0  \n",
       " 2018-09-11 18:59:37.272128440                                    0.0  \n",
       " 2018-09-11 18:59:37.860959800                                    0.0  \n",
       " \n",
       " [477 rows x 3 columns]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict[\"carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failure Status:\n",
    "    \n",
    "    Engine     = 1\n",
    "    Rudder     = 2\n",
    "    Aileron    = 3\n",
    "    Elevator   = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_failure(flight_name,df_failure,failure_duration_start,failure_duration_end):\n",
    "    if \"engine\" in flight_name:\n",
    "        failure_status = [\n",
    "        1 if (x > failure_duration_start and x < failure_duration_end) else 0 \n",
    "        for x in df_failure.index\n",
    "                ]\n",
    "    elif \"rudder\" in flight_name:\n",
    "        failure_status = [\n",
    "        2 if (x > failure_duration_start and x < failure_duration_end) else 0 \n",
    "        for x in df_failure.index\n",
    "                ]\n",
    "    elif \"aileron\" in flight_name:\n",
    "        failure_status = [\n",
    "        3 if (x > failure_duration_start and x < failure_duration_end) else 0 \n",
    "        for x in df_failure.index\n",
    "                ]\n",
    "    elif \"elevator\" in flight_name:\n",
    "        failure_status = [\n",
    "        4 if (x > failure_duration_start and x < failure_duration_end) else 0 \n",
    "        for x in df_failure.index\n",
    "                ]\n",
    "        \n",
    "    df_failure[\"failure_status\"] = failure_status\n",
    "        \n",
    "    return df_failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carbonZ_2018-07-18-15-53-31_1_engine_failure',\n",
       " 'carbonZ_2018-07-18-15-53-31_2_engine_failure',\n",
       " 'carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-07-30-16-39-00_1_engine_failure',\n",
       " 'carbonZ_2018-07-30-16-39-00_2_engine_failure',\n",
       " 'carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-09-11-11-56-30_engine_failure',\n",
       " 'carbonZ_2018-09-11-14-22-07_1_engine_failure',\n",
       " 'carbonZ_2018-09-11-14-22-07_2_engine_failure',\n",
       " 'carbonZ_2018-09-11-14-41-51_elevator_failure',\n",
       " 'carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure',\n",
       " 'carbonZ_2018-09-11-15-05-11_1_elevator_failure',\n",
       " 'carbonZ_2018-09-11-15-06-34_1_rudder_right_failure',\n",
       " 'carbonZ_2018-09-11-15-06-34_2_rudder_right_failure',\n",
       " 'carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure',\n",
       " 'carbonZ_2018-09-11-17-55-30_1_right_aileron_failure',\n",
       " 'carbonZ_2018-09-11-17-55-30_2_left_aileron_failure',\n",
       " 'carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-05-14-37-22_2_right_aileron_failure',\n",
       " 'carbonZ_2018-10-05-14-37-22_3_left_aileron_failure',\n",
       " 'carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj',\n",
       " 'carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure',\n",
       " 'carbonZ_2018-09-11-17-27-13_1_left_aileron_failure']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_1_engine_failure\n",
      "carbonZ_2018-07-18-15-53-31_2_engine_failure\n",
      "carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-16-39-00_1_engine_failure\n",
      "carbonZ_2018-07-30-16-39-00_2_engine_failure\n",
      "carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj\n",
      "carbonZ_2018-09-11-11-56-30_engine_failure\n",
      "carbonZ_2018-09-11-14-22-07_1_engine_failure\n",
      "carbonZ_2018-09-11-14-22-07_2_engine_failure\n",
      "carbonZ_2018-09-11-14-41-51_elevator_failure\n",
      "carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\n",
      "carbonZ_2018-09-11-15-05-11_1_elevator_failure\n",
      "carbonZ_2018-09-11-15-06-34_1_rudder_right_failure\n",
      "carbonZ_2018-09-11-15-06-34_2_rudder_right_failure\n",
      "carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure\n",
      "carbonZ_2018-09-11-17-55-30_1_right_aileron_failure\n",
      "carbonZ_2018-09-11-17-55-30_2_left_aileron_failure\n",
      "carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-14-37-22_2_right_aileron_failure\n",
      "carbonZ_2018-10-05-14-37-22_3_left_aileron_failure\n",
      "carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj\n",
      "carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\n",
      "carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\n"
     ]
    }
   ],
   "source": [
    "for flight_name in flight_topic_list:\n",
    "    print(flight_name)\n",
    "    dfs = df_dict[flight_name] \n",
    "    \n",
    "    start_time = min(min(dfs[0].index),min(dfs[1].index),min(dfs[2].index),min(dfs[3].index),min(dfs[4].index),min(dfs[5].index),min(dfs[6].index),min(dfs[7].index),min(dfs[8].index),min(dfs[9].index),min(dfs[10].index),min(dfs[11].index),min(dfs[12].index))\n",
    "    end_time = max(max(dfs[0].index),max(dfs[1].index),max(dfs[2].index),max(dfs[3].index),max(dfs[4].index),max(dfs[5].index),max(dfs[6].index),max(dfs[7].index),max(dfs[8].index),max(dfs[9].index),max(dfs[10].index),max(dfs[11].index),max(dfs[12].index))\n",
    "    \n",
    "    time_index = pd.date_range(start=start_time, end=end_time, freq=\"200ms\")  \n",
    "    data = [0] * len(time_index)\n",
    "\n",
    "    df_failure = pd.DataFrame(data, index=time_index, columns=[\"failure_status\"])\n",
    "\n",
    "    failure_duration_start = failure_status_dict[flight_name][0]\n",
    "    failure_duration_end   = failure_status_dict[flight_name][1]\n",
    "    \n",
    "    \n",
    "    df_dict[flight_name].append(add_failure(flight_name,df_failure,failure_duration_start,failure_duration_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mavros-wind_estimation.twist.linear.x</th>\n",
       "      <th>mavros-wind_estimation.twist.linear.y</th>\n",
       "      <th>mavros-wind_estimation.twist.linear.z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:19.188548395</th>\n",
       "      <td>0.360470</td>\n",
       "      <td>1.439026</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:19.566847897</th>\n",
       "      <td>0.352583</td>\n",
       "      <td>1.463511</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:19.954091484</th>\n",
       "      <td>0.345375</td>\n",
       "      <td>1.496819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:20.421941571</th>\n",
       "      <td>0.328750</td>\n",
       "      <td>1.548180</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:20.868431200</th>\n",
       "      <td>0.321009</td>\n",
       "      <td>1.588851</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:40.589048991</th>\n",
       "      <td>-0.256345</td>\n",
       "      <td>2.252147</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:40.936268682</th>\n",
       "      <td>-0.255210</td>\n",
       "      <td>2.252631</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:41.422716742</th>\n",
       "      <td>-0.253657</td>\n",
       "      <td>2.253724</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:41.785506158</th>\n",
       "      <td>-0.252744</td>\n",
       "      <td>2.256600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:42.286168071</th>\n",
       "      <td>-0.252200</td>\n",
       "      <td>2.260911</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mavros-wind_estimation.twist.linear.x  \\\n",
       "timestamp                                                              \n",
       "2018-09-11 21:34:19.188548395                               0.360470   \n",
       "2018-09-11 21:34:19.566847897                               0.352583   \n",
       "2018-09-11 21:34:19.954091484                               0.345375   \n",
       "2018-09-11 21:34:20.421941571                               0.328750   \n",
       "2018-09-11 21:34:20.868431200                               0.321009   \n",
       "...                                                              ...   \n",
       "2018-09-11 21:36:40.589048991                              -0.256345   \n",
       "2018-09-11 21:36:40.936268682                              -0.255210   \n",
       "2018-09-11 21:36:41.422716742                              -0.253657   \n",
       "2018-09-11 21:36:41.785506158                              -0.252744   \n",
       "2018-09-11 21:36:42.286168071                              -0.252200   \n",
       "\n",
       "                               mavros-wind_estimation.twist.linear.y  \\\n",
       "timestamp                                                              \n",
       "2018-09-11 21:34:19.188548395                               1.439026   \n",
       "2018-09-11 21:34:19.566847897                               1.463511   \n",
       "2018-09-11 21:34:19.954091484                               1.496819   \n",
       "2018-09-11 21:34:20.421941571                               1.548180   \n",
       "2018-09-11 21:34:20.868431200                               1.588851   \n",
       "...                                                              ...   \n",
       "2018-09-11 21:36:40.589048991                               2.252147   \n",
       "2018-09-11 21:36:40.936268682                               2.252631   \n",
       "2018-09-11 21:36:41.422716742                               2.253724   \n",
       "2018-09-11 21:36:41.785506158                               2.256600   \n",
       "2018-09-11 21:36:42.286168071                               2.260911   \n",
       "\n",
       "                               mavros-wind_estimation.twist.linear.z  \n",
       "timestamp                                                             \n",
       "2018-09-11 21:34:19.188548395                                    0.0  \n",
       "2018-09-11 21:34:19.566847897                                    0.0  \n",
       "2018-09-11 21:34:19.954091484                                    0.0  \n",
       "2018-09-11 21:34:20.421941571                                    0.0  \n",
       "2018-09-11 21:34:20.868431200                                    0.0  \n",
       "...                                                              ...  \n",
       "2018-09-11 21:36:40.589048991                                    0.0  \n",
       "2018-09-11 21:36:40.936268682                                    0.0  \n",
       "2018-09-11 21:36:41.422716742                                    0.0  \n",
       "2018-09-11 21:36:41.785506158                                    0.0  \n",
       "2018-09-11 21:36:42.286168071                                    0.0  \n",
       "\n",
       "[311 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict[\"carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\"]\n",
    "df[12].iloc[: , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mavros-wind_estimation.twist.linear.x</th>\n",
       "      <th>mavros-wind_estimation.twist.linear.y</th>\n",
       "      <th>mavros-wind_estimation.twist.linear.z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:19.188548395</th>\n",
       "      <td>0.360470</td>\n",
       "      <td>1.439026</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:19.566847897</th>\n",
       "      <td>0.352583</td>\n",
       "      <td>1.463511</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:19.954091484</th>\n",
       "      <td>0.345375</td>\n",
       "      <td>1.496819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:20.421941571</th>\n",
       "      <td>0.328750</td>\n",
       "      <td>1.548180</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:34:20.868431200</th>\n",
       "      <td>0.321009</td>\n",
       "      <td>1.588851</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:40.589048991</th>\n",
       "      <td>-0.256345</td>\n",
       "      <td>2.252147</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:40.936268682</th>\n",
       "      <td>-0.255210</td>\n",
       "      <td>2.252631</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:41.422716742</th>\n",
       "      <td>-0.253657</td>\n",
       "      <td>2.253724</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:41.785506158</th>\n",
       "      <td>-0.252744</td>\n",
       "      <td>2.256600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11 21:36:42.286168071</th>\n",
       "      <td>-0.252200</td>\n",
       "      <td>2.260911</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mavros-wind_estimation.twist.linear.x  \\\n",
       "timestamp                                                              \n",
       "2018-09-11 21:34:19.188548395                               0.360470   \n",
       "2018-09-11 21:34:19.566847897                               0.352583   \n",
       "2018-09-11 21:34:19.954091484                               0.345375   \n",
       "2018-09-11 21:34:20.421941571                               0.328750   \n",
       "2018-09-11 21:34:20.868431200                               0.321009   \n",
       "...                                                              ...   \n",
       "2018-09-11 21:36:40.589048991                              -0.256345   \n",
       "2018-09-11 21:36:40.936268682                              -0.255210   \n",
       "2018-09-11 21:36:41.422716742                              -0.253657   \n",
       "2018-09-11 21:36:41.785506158                              -0.252744   \n",
       "2018-09-11 21:36:42.286168071                              -0.252200   \n",
       "\n",
       "                               mavros-wind_estimation.twist.linear.y  \\\n",
       "timestamp                                                              \n",
       "2018-09-11 21:34:19.188548395                               1.439026   \n",
       "2018-09-11 21:34:19.566847897                               1.463511   \n",
       "2018-09-11 21:34:19.954091484                               1.496819   \n",
       "2018-09-11 21:34:20.421941571                               1.548180   \n",
       "2018-09-11 21:34:20.868431200                               1.588851   \n",
       "...                                                              ...   \n",
       "2018-09-11 21:36:40.589048991                               2.252147   \n",
       "2018-09-11 21:36:40.936268682                               2.252631   \n",
       "2018-09-11 21:36:41.422716742                               2.253724   \n",
       "2018-09-11 21:36:41.785506158                               2.256600   \n",
       "2018-09-11 21:36:42.286168071                               2.260911   \n",
       "\n",
       "                               mavros-wind_estimation.twist.linear.z  \n",
       "timestamp                                                             \n",
       "2018-09-11 21:34:19.188548395                                    0.0  \n",
       "2018-09-11 21:34:19.566847897                                    0.0  \n",
       "2018-09-11 21:34:19.954091484                                    0.0  \n",
       "2018-09-11 21:34:20.421941571                                    0.0  \n",
       "2018-09-11 21:34:20.868431200                                    0.0  \n",
       "...                                                              ...  \n",
       "2018-09-11 21:36:40.589048991                                    0.0  \n",
       "2018-09-11 21:36:40.936268682                                    0.0  \n",
       "2018-09-11 21:36:41.422716742                                    0.0  \n",
       "2018-09-11 21:36:41.785506158                                    0.0  \n",
       "2018-09-11 21:36:42.286168071                                    0.0  \n",
       "\n",
       "[311 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict[\"carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\"]\n",
    "df[12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_1_engine_failure\n",
      "14\n",
      "carbonZ_2018-07-18-15-53-31_2_engine_failure\n",
      "14\n",
      "carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-07-30-16-39-00_1_engine_failure\n",
      "14\n",
      "carbonZ_2018-07-30-16-39-00_2_engine_failure\n",
      "14\n",
      "carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-09-11-11-56-30_engine_failure\n",
      "14\n",
      "carbonZ_2018-09-11-14-22-07_1_engine_failure\n",
      "14\n",
      "carbonZ_2018-09-11-14-22-07_2_engine_failure\n",
      "14\n",
      "carbonZ_2018-09-11-14-41-51_elevator_failure\n",
      "14\n",
      "carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\n",
      "14\n",
      "carbonZ_2018-09-11-15-05-11_1_elevator_failure\n",
      "14\n",
      "carbonZ_2018-09-11-15-06-34_1_rudder_right_failure\n",
      "14\n",
      "carbonZ_2018-09-11-15-06-34_2_rudder_right_failure\n",
      "14\n",
      "carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure\n",
      "14\n",
      "carbonZ_2018-09-11-17-55-30_1_right_aileron_failure\n",
      "14\n",
      "carbonZ_2018-09-11-17-55-30_2_left_aileron_failure\n",
      "14\n",
      "carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-05-14-37-22_2_right_aileron_failure\n",
      "14\n",
      "carbonZ_2018-10-05-14-37-22_3_left_aileron_failure\n",
      "14\n",
      "carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj\n",
      "14\n",
      "carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\n",
      "14\n",
      "carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "for key, df in df_dict.items():\n",
    "    print(key)\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Birle≈ütirme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mavros-imu-atm_pressure.fluid_pressure</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.883316762</th>\n",
       "      <td>97310.498047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:50.958577559</th>\n",
       "      <td>97310.498047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:51.062952770</th>\n",
       "      <td>97329.193115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:51.153503583</th>\n",
       "      <td>97341.693115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:56:51.245127118</th>\n",
       "      <td>97329.223633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:02.784104260</th>\n",
       "      <td>97410.662842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:02.886696084</th>\n",
       "      <td>97414.984131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:02.954867820</th>\n",
       "      <td>97414.984131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.072783613</th>\n",
       "      <td>97418.975830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-18 19:59:03.128568663</th>\n",
       "      <td>97418.975830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1323 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mavros-imu-atm_pressure.fluid_pressure\n",
       "timestamp                                                            \n",
       "2018-07-18 19:56:50.883316762                            97310.498047\n",
       "2018-07-18 19:56:50.958577559                            97310.498047\n",
       "2018-07-18 19:56:51.062952770                            97329.193115\n",
       "2018-07-18 19:56:51.153503583                            97341.693115\n",
       "2018-07-18 19:56:51.245127118                            97329.223633\n",
       "...                                                               ...\n",
       "2018-07-18 19:59:02.784104260                            97410.662842\n",
       "2018-07-18 19:59:02.886696084                            97414.984131\n",
       "2018-07-18 19:59:02.954867820                            97414.984131\n",
       "2018-07-18 19:59:03.072783613                            97418.975830\n",
       "2018-07-18 19:59:03.128568663                            97418.975830\n",
       "\n",
       "[1323 rows x 1 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_dict[\"carbonZ_2018-07-18-15-53-31_1_engine_failure\"][1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carbonZ_2018-07-18-15-53-31_1_engine_failure\n",
      "carbonZ_2018-07-18-15-53-31_2_engine_failure\n",
      "carbonZ_2018-07-18-16-22-01_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-18-16-37-39_2_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-16-29-45_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-16-39-00_1_engine_failure\n",
      "carbonZ_2018-07-30-16-39-00_2_engine_failure\n",
      "carbonZ_2018-07-30-17-10-45_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-20-01_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-36-35_engine_failure_with_emr_traj\n",
      "carbonZ_2018-07-30-17-46-31_engine_failure_with_emr_traj\n",
      "carbonZ_2018-09-11-11-56-30_engine_failure\n",
      "carbonZ_2018-09-11-14-22-07_1_engine_failure\n",
      "carbonZ_2018-09-11-14-22-07_2_engine_failure\n",
      "carbonZ_2018-09-11-14-41-51_elevator_failure\n",
      "carbonZ_2018-09-11-14-52-54_left_aileron__right_aileron__failure\n",
      "carbonZ_2018-09-11-15-05-11_1_elevator_failure\n",
      "carbonZ_2018-09-11-15-06-34_1_rudder_right_failure\n",
      "carbonZ_2018-09-11-15-06-34_2_rudder_right_failure\n",
      "carbonZ_2018-09-11-17-27-13_2_both_ailerons_failure\n",
      "carbonZ_2018-09-11-17-55-30_1_right_aileron_failure\n",
      "carbonZ_2018-09-11-17-55-30_2_left_aileron_failure\n",
      "carbonZ_2018-10-05-14-34-20_2_right_aileron_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-14-37-22_2_right_aileron_failure\n",
      "carbonZ_2018-10-05-14-37-22_3_left_aileron_failure\n",
      "carbonZ_2018-10-05-15-52-12_3_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-15-55-10_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-05-16-04-46_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-03-57_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-00_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-08_1_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-08_2_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-04-35_engine_failure_with_emr_traj\n",
      "carbonZ_2018-10-18-11-06-06_engine_failure_with_emr_traj\n",
      "carbonZ_2018-09-11-17-27-13_1_rudder_zero_failure\n",
      "carbonZ_2018-09-11-17-27-13_1_left_aileron_failure\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "w_label = 66\n",
    "s_label = 6\n",
    "\n",
    "window_sizes = [661, 132, 32, 132, 247, 253, 253, 253, 247, 253, 27, 32, 27]\n",
    "step_sizes =   [66, 13, 3, 13, 24, 25, 25, 25, 24, 25, 2, 3, 2]\n",
    "\n",
    "x_arrays = [np.zeros((0, w, c)) for w, c in zip(window_sizes, [2, 1, 10, 1, 1, 4, 1, 1, 6, 1, 1, 6, 3])]\n",
    "y = []\n",
    "\n",
    "for key, data in df_dict.items():\n",
    "    print(key)\n",
    "    \n",
    "    scaled_data = [pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index).dropna() for df in data[:-1]]\n",
    "    labels_data = data[-1]\n",
    "\n",
    "    len_label = len(labels_data)\n",
    "    len_data = [len(df) for df in scaled_data]\n",
    "\n",
    "    while all(len_d - w >= 0 for len_d, w in zip(len_data, window_sizes)):\n",
    "        labels = labels_data[len_label - w_label : len_label].values\n",
    "        len_label -= s_label\n",
    "\n",
    "        slices = [df[len_d - w : len_d].values for df, len_d, w in zip(scaled_data, len_data, window_sizes)]\n",
    "        reshaped_slices = [\n",
    "            sl.reshape(-1, w, c) for sl, w, c in zip(slices, window_sizes, [2, 1, 10, 1, 1, 4, 1, 1, 6, 1, 1, 6, 3])\n",
    "        ]\n",
    "        len_data = [len_d - s for len_d, s in zip(len_data, step_sizes)]\n",
    "\n",
    "        if 2 in labels:\n",
    "            repeat_count = 4\n",
    "        elif 3 in labels:\n",
    "            repeat_count = 3\n",
    "        elif 4 in labels:\n",
    "            repeat_count = 5\n",
    "        else:\n",
    "            repeat_count = 1\n",
    "        \n",
    "        for _ in range(repeat_count):\n",
    "            for i, sl in enumerate(reshaped_slices):\n",
    "                x_arrays[i] = np.vstack((x_arrays[i], sl))\n",
    "            y.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3666, 661, 2),\n",
       " (3666, 132, 1),\n",
       " (3666, 32, 10),\n",
       " (3666, 132, 1),\n",
       " (3666, 247, 1),\n",
       " (3666, 253, 4),\n",
       " (3666, 253, 1),\n",
       " (3666, 253, 1),\n",
       " (3666, 247, 6),\n",
       " (3666, 253, 1),\n",
       " (3666, 27, 1),\n",
       " (3666, 32, 6),\n",
       " (3666, 27, 3)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes = [x.shape for x in x_arrays]\n",
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2548107])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_arrays[9][495][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3666"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3666, 1), (3666, 66, 1))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "y_ = np.array([max(m) for m in y])\n",
    "y_.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch shapes: torch.Size([32, 661, 2]) torch.Size([32, 132, 1]) torch.Size([32, 32, 10]) torch.Size([32, 132, 1]) torch.Size([32, 247, 1]) torch.Size([32, 253, 4]) torch.Size([32, 253, 1]) torch.Size([32, 253, 1]) torch.Size([32, 247, 6]) torch.Size([32, 253, 1]) torch.Size([32, 27, 1]) torch.Size([32, 32, 6]) torch.Size([32, 27, 3]) torch.Size([32])\n",
      "Test batch shapes: torch.Size([32, 661, 2]) torch.Size([32, 132, 1]) torch.Size([32, 32, 10]) torch.Size([32, 132, 1]) torch.Size([32, 247, 1]) torch.Size([32, 253, 4]) torch.Size([32, 253, 1]) torch.Size([32, 253, 1]) torch.Size([32, 247, 6]) torch.Size([32, 253, 1]) torch.Size([32, 27, 1]) torch.Size([32, 32, 6]) torch.Size([32, 27, 3]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_arrays, y):\n",
    "        self.x0 = torch.tensor(x_arrays[0], dtype=torch.float32)\n",
    "        self.x1 = torch.tensor(x_arrays[1], dtype=torch.float32)\n",
    "        self.x2 = torch.tensor(x_arrays[2], dtype=torch.float32)\n",
    "        self.x3 = torch.tensor(x_arrays[3], dtype=torch.float32)\n",
    "        self.x4 = torch.tensor(x_arrays[4], dtype=torch.float32)\n",
    "        self.x5 = torch.tensor(x_arrays[5], dtype=torch.float32)\n",
    "        self.x6 = torch.tensor(x_arrays[6], dtype=torch.float32)\n",
    "        self.x7 = torch.tensor(x_arrays[7], dtype=torch.float32)\n",
    "        self.x8 = torch.tensor(x_arrays[8], dtype=torch.float32)\n",
    "        self.x9 = torch.tensor(x_arrays[9], dtype=torch.float32)\n",
    "        self.x10 = torch.tensor(x_arrays[10], dtype=torch.float32)\n",
    "        self.x11 = torch.tensor(x_arrays[11], dtype=torch.float32)\n",
    "        self.x12 = torch.tensor(x_arrays[12], dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long).squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.x0[idx], self.x1[idx], self.x2[idx], self.x3[idx], self.x4[idx],\n",
    "            self.x5[idx], self.x6[idx], self.x7[idx], self.x8[idx], self.x9[idx],\n",
    "            self.x10[idx], self.x11[idx], self.x12[idx], self.y[idx]\n",
    "        )\n",
    "\n",
    "dataset = CustomDataset(x_arrays, y_)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    (\n",
    "        x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \n",
    "        x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \n",
    "        x10_batch, x11_batch, x12_batch, y_batch\n",
    "    ) = batch\n",
    "    print(\"Train batch shapes:\", x0_batch.shape, x1_batch.shape, x2_batch.shape, x3_batch.shape, \n",
    "          x4_batch.shape, x5_batch.shape, x6_batch.shape, x7_batch.shape, x8_batch.shape, \n",
    "          x9_batch.shape, x10_batch.shape, x11_batch.shape, x12_batch.shape, y_batch.shape)\n",
    "    break\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    (\n",
    "        x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \n",
    "        x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \n",
    "        x10_batch, x11_batch, x12_batch, y_batch\n",
    "    ) = batch\n",
    "    print(\"Test batch shapes:\", x0_batch.shape, x1_batch.shape, x2_batch.shape, x3_batch.shape, \n",
    "          x4_batch.shape, x5_batch.shape, x6_batch.shape, x7_batch.shape, x8_batch.shape, \n",
    "          x9_batch.shape, x10_batch.shape, x11_batch.shape, x12_batch.shape, y_batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CausalConv1D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, **kwargs):\n",
    "        super(CausalConv1D, self).__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=self.padding, dilation=dilation, **kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x[:, :, :-self.padding]\n",
    "\n",
    "class BlockDiagonal(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_blocks):\n",
    "        super(BlockDiagonal, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        assert out_features % num_blocks == 0\n",
    "        \n",
    "        block_out_features = out_features // num_blocks\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Linear(in_features, block_out_features)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = [block(x) for block in self.blocks]\n",
    "        x = torch.cat(x, dim=-1)\n",
    "        return x\n",
    "\n",
    "class sLSTMBlock(nn.Module):\n",
    "    def __init__(self, input_size, head_size, num_heads, proj_factor=4/3):\n",
    "        super(sLSTMBlock, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = head_size * num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_factor = proj_factor\n",
    "\n",
    "        assert proj_factor > 0\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(input_size)\n",
    "        self.causal_conv = CausalConv1D(1, 1, 4)\n",
    "\n",
    "        self.Wz = BlockDiagonal(input_size, self.hidden_size, num_heads)\n",
    "        self.Wi = BlockDiagonal(input_size, self.hidden_size, num_heads)\n",
    "        self.Wf = BlockDiagonal(input_size, self.hidden_size, num_heads)\n",
    "        self.Wo = BlockDiagonal(input_size, self.hidden_size, num_heads)\n",
    "\n",
    "        self.Rz = BlockDiagonal(self.hidden_size, self.hidden_size, num_heads)\n",
    "        self.Ri = BlockDiagonal(self.hidden_size, self.hidden_size, num_heads)\n",
    "        self.Rf = BlockDiagonal(self.hidden_size, self.hidden_size, num_heads)\n",
    "        self.Ro = BlockDiagonal(self.hidden_size, self.hidden_size, num_heads)\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_heads, self.hidden_size)\n",
    "\n",
    "        self.up_proj_left = nn.Linear(self.hidden_size, int(self.hidden_size * proj_factor))\n",
    "        self.up_proj_right = nn.Linear(self.hidden_size, int(self.hidden_size * proj_factor))\n",
    "        self.down_proj = nn.Linear(int(self.hidden_size * proj_factor), input_size)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        assert x.size(-1) == self.input_size\n",
    "        h_prev, c_prev, n_prev, m_prev = prev_state\n",
    "\n",
    "        h_prev = h_prev\n",
    "        c_prev = c_prev\n",
    "        n_prev = n_prev\n",
    "        m_prev = m_prev\n",
    "        \n",
    "        x_norm = self.layer_norm(x)\n",
    "        x_conv = F.silu(self.causal_conv(x_norm.unsqueeze(1)).squeeze(1))\n",
    "\n",
    "        z = torch.tanh(self.Wz(x_norm) + self.Rz(h_prev))\n",
    "        o = torch.sigmoid(self.Wo(x_norm) + self.Ro(h_prev))\n",
    "        i_tilde = self.Wi(x_conv) + self.Ri(h_prev)\n",
    "        f_tilde = self.Wf(x_conv) + self.Rf(h_prev)\n",
    "\n",
    "        m_t = torch.max(f_tilde + m_prev, i_tilde)\n",
    "        i = torch.exp(i_tilde - m_t)\n",
    "        f = torch.exp(f_tilde + m_prev - m_t)\n",
    "\n",
    "        c_t = f * c_prev + i * z\n",
    "        n_t = f * n_prev + i\n",
    "        h_t = o * c_t / n_t\n",
    "\n",
    "        output = h_t\n",
    "        output_norm = self.group_norm(output)\n",
    "        output_left = self.up_proj_left(output_norm)\n",
    "        output_right = self.up_proj_right(output_norm)\n",
    "        output_gated = F.gelu(output_right)\n",
    "        output = output_left * output_gated\n",
    "        output = self.down_proj(output)\n",
    "        final_output = output + x\n",
    "\n",
    "        return final_output, (h_t, c_t, n_t, m_t)\n",
    "    \n",
    "class sLSTM(nn.Module):\n",
    "    # TODO: Add bias, dropout, bidirectional\n",
    "    def __init__(self, input_size, head_size, num_heads, num_layers=1, batch_first=False, proj_factor=4/3):\n",
    "        super(sLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = head_size * num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.proj_factor_slstm = proj_factor\n",
    "\n",
    "        self.layers = nn.ModuleList([sLSTMBlock(input_size, head_size, num_heads, proj_factor) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        assert x.ndim == 3\n",
    "        if self.batch_first: x = x.transpose(0, 1)\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        \n",
    "        if state is not None:\n",
    "            state = torch.stack(list(state)).to(x.device)\n",
    "            assert state.ndim == 4\n",
    "            num_hidden, state_num_layers, state_batch_size, state_input_size = state.size()\n",
    "            assert num_hidden == 4\n",
    "            assert state_num_layers == self.num_layers\n",
    "            assert state_batch_size == batch_size\n",
    "            assert state_input_size == self.input_size\n",
    "            state = state.transpose(0, 1)\n",
    "        else:\n",
    "            state = torch.zeros(self.num_layers, 4, batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t]\n",
    "            for layer in range(self.num_layers):\n",
    "                x_t, state_tuple = self.layers[layer](x_t, tuple(state[layer].clone()))\n",
    "                state[layer] = torch.stack(list(state_tuple))\n",
    "            output.append(x_t)\n",
    "        \n",
    "        output = torch.stack(output)\n",
    "        if self.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "        state = tuple(state.transpose(0, 1))\n",
    "        return output, state\n",
    "\n",
    "class mLSTMBlock(nn.Module):\n",
    "    def __init__(self, input_size, head_size, num_heads, proj_factor=2):\n",
    "        super(mLSTMBlock, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = head_size * num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.proj_factor = proj_factor\n",
    "\n",
    "        assert proj_factor > 0\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(input_size)\n",
    "        self.up_proj_left = nn.Linear(input_size, int(input_size * proj_factor))\n",
    "        self.up_proj_right = nn.Linear(input_size, self.hidden_size)\n",
    "        self.down_proj = nn.Linear(self.hidden_size, input_size)\n",
    "\n",
    "        self.causal_conv = CausalConv1D(1, 1, 4)\n",
    "        self.skip_connection = nn.Linear(int(input_size * proj_factor), self.hidden_size)\n",
    "\n",
    "        self.Wq = BlockDiagonal(int(input_size * proj_factor), self.hidden_size, num_heads)\n",
    "        self.Wk = BlockDiagonal(int(input_size * proj_factor), self.hidden_size, num_heads)\n",
    "        self.Wv = BlockDiagonal(int(input_size * proj_factor), self.hidden_size, num_heads)\n",
    "        self.Wi = nn.Linear(int(input_size * proj_factor), self.hidden_size)\n",
    "        self.Wf = nn.Linear(int(input_size * proj_factor), self.hidden_size)\n",
    "        self.Wo = nn.Linear(int(input_size * proj_factor), self.hidden_size)\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_heads, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        h_prev, c_prev, n_prev, m_prev = prev_state\n",
    "\n",
    "        h_prev = h_prev\n",
    "        c_prev = c_prev\n",
    "        n_prev = n_prev\n",
    "        m_prev = m_prev\n",
    "        \n",
    "        assert x.size(-1) == self.input_size\n",
    "        x_norm = self.layer_norm(x)\n",
    "        x_up_left = self.up_proj_left(x_norm)\n",
    "        x_up_right = self.up_proj_right(x_norm)\n",
    "\n",
    "        x_conv = F.silu(self.causal_conv(x_up_left.unsqueeze(1)).squeeze(1))\n",
    "        x_skip = self.skip_connection(x_conv)\n",
    "\n",
    "        q = self.Wq(x_conv)\n",
    "        k = self.Wk(x_conv) / (self.head_size ** 0.5)\n",
    "        v = self.Wv(x_up_left)\n",
    "\n",
    "        i_tilde = self.Wi(x_conv)\n",
    "        f_tilde = self.Wf(x_conv)\n",
    "        o = torch.sigmoid(self.Wo(x_up_left))\n",
    "\n",
    "        m_t = torch.max(f_tilde + m_prev, i_tilde)\n",
    "        i = torch.exp(i_tilde - m_t)\n",
    "        f = torch.exp(f_tilde + m_prev - m_t)\n",
    "\n",
    "        c_t = f * c_prev + i * (v * k) # v @ k.T\n",
    "        n_t = f * n_prev + i * k\n",
    "        h_t = o * (c_t * q) / torch.max(torch.abs(n_t.T @ q), 1)[0] # o * (c @ q) / max{|n.T @ q|, 1}\n",
    "\n",
    "        output = h_t\n",
    "        output_norm = self.group_norm(output)\n",
    "        output = output_norm + x_skip\n",
    "        output = output * F.silu(x_up_right)\n",
    "        output = self.down_proj(output)\n",
    "        final_output = output + x\n",
    "\n",
    "        return final_output, (h_t, c_t, n_t, m_t)\n",
    "    \n",
    "class mLSTM(nn.Module):\n",
    "    # TODO: Add bias, dropout, bidirectional\n",
    "    def __init__(self, input_size, head_size, num_heads, num_layers=1, batch_first=False, proj_factor=2):\n",
    "        super(mLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = head_size * num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.proj_factor_slstm = proj_factor\n",
    "\n",
    "        self.layers = nn.ModuleList([mLSTMBlock(input_size, head_size, num_heads, proj_factor) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        assert x.ndim == 3\n",
    "        if self.batch_first: x = x.transpose(0, 1)\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        \n",
    "        if state is not None:\n",
    "            state = torch.stack(list(state))\n",
    "            assert state.ndim == 4\n",
    "            num_hidden, state_num_layers, state_batch_size, state_input_size = state.size()\n",
    "            assert num_hidden == 4\n",
    "            assert state_num_layers == self.num_layers\n",
    "            assert state_batch_size == batch_size\n",
    "            assert state_input_size == self.input_size\n",
    "            state = state.transpose(0, 1)\n",
    "        else:\n",
    "            state = torch.zeros(self.num_layers, 4, batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t]\n",
    "            for layer in range(self.num_layers):\n",
    "                x_t, state_tuple = self.layers[layer](x_t, tuple(state[layer].clone()))\n",
    "                state[layer] = torch.stack(list(state_tuple))\n",
    "            output.append(x_t)\n",
    "        \n",
    "        output = torch.stack(output)\n",
    "        if self.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "        state = tuple(state.transpose(0, 1))\n",
    "        return output, state\n",
    "\n",
    "\n",
    "\n",
    "class xLSTM(nn.Module):\n",
    "    # Added dropout and bias options\n",
    "    def __init__(self, input_size, head_size, num_heads, layers, batch_first=False, \n",
    "                 proj_factor_slstm=4/3, proj_factor_mlstm=2, dropout=0.0):\n",
    "        super(xLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.head_size = head_size\n",
    "        self.hidden_size = head_size * num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(layers)\n",
    "        self.batch_first = batch_first\n",
    "        self.proj_factor_slstm = proj_factor_slstm\n",
    "        self.proj_factor_mlstm = proj_factor_mlstm\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for layer_type in layers:\n",
    "            if layer_type == 's':\n",
    "                layer = sLSTMBlock(input_size, head_size, num_heads, proj_factor_slstm)\n",
    "            elif layer_type == 'm':\n",
    "                layer = mLSTMBlock(input_size, head_size, num_heads, proj_factor_mlstm)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid layer type: {layer_type}. Choose 's' for sLSTM or 'm' for mLSTM.\")\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout) if dropout > 0 else None\n",
    "\n",
    "    def forward(self, x, state=None):\n",
    "        assert x.ndim == 3\n",
    "        if self.batch_first: \n",
    "            x = x.transpose(0, 1)\n",
    "        seq_len, batch_size, _ = x.size()\n",
    "        \n",
    "        if state is not None:\n",
    "            state = torch.stack(list(state))\n",
    "            assert state.ndim == 4\n",
    "            num_hidden, state_num_layers, state_batch_size, state_input_size = state.size()\n",
    "            assert num_hidden == 4\n",
    "            assert state_num_layers == self.num_layers\n",
    "            assert state_batch_size == batch_size\n",
    "            assert state_input_size == self.input_size\n",
    "            state = state.transpose(0, 1)\n",
    "        else:\n",
    "            state = torch.zeros(self.num_layers, 4, batch_size, self.hidden_size, device=x.device)\n",
    "\n",
    "        output = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[t]\n",
    "            for layer in range(self.num_layers):\n",
    "                x_t, state_tuple = self.layers[layer](x_t, tuple(state[layer].clone()))\n",
    "                state[layer] = torch.stack(list(state_tuple))\n",
    "                if self.dropout_layer is not None:\n",
    "                    x_t = self.dropout_layer(x_t)\n",
    "            output.append(x_t)\n",
    "        \n",
    "        output = torch.stack(output)\n",
    "        if self.batch_first:\n",
    "            output = output.transpose(0, 1)\n",
    "        state = tuple(state.transpose(0, 1))\n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm_layers): ModuleList(\n",
      "    (0): xLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): mLSTMBlock(\n",
      "          (layer_norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
      "          (up_proj_left): Linear(in_features=2, out_features=4, bias=True)\n",
      "          (up_proj_right): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (down_proj): Linear(in_features=16, out_features=2, bias=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (skip_connection): Linear(in_features=4, out_features=16, bias=True)\n",
      "          (Wq): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wk): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wv): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): Linear(in_features=4, out_features=16, bias=True)\n",
      "          (Wf): Linear(in_features=4, out_features=16, bias=True)\n",
      "          (Wo): Linear(in_features=4, out_features=16, bias=True)\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): sLSTMBlock(\n",
      "          (layer_norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (Wz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wo): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ri): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ro): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "          (up_proj_left): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (up_proj_right): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (down_proj): Linear(in_features=21, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): xLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): mLSTMBlock(\n",
      "          (layer_norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "          (up_proj_left): Linear(in_features=1, out_features=2, bias=True)\n",
      "          (up_proj_right): Linear(in_features=1, out_features=16, bias=True)\n",
      "          (down_proj): Linear(in_features=16, out_features=1, bias=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (skip_connection): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (Wq): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wk): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wv): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (Wf): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (Wo): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): sLSTMBlock(\n",
      "          (layer_norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (Wz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wo): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ri): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ro): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "          (up_proj_left): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (up_proj_right): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (down_proj): Linear(in_features=21, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (2): xLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): mLSTMBlock(\n",
      "          (layer_norm): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "          (up_proj_left): Linear(in_features=10, out_features=20, bias=True)\n",
      "          (up_proj_right): Linear(in_features=10, out_features=16, bias=True)\n",
      "          (down_proj): Linear(in_features=16, out_features=10, bias=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (skip_connection): Linear(in_features=20, out_features=16, bias=True)\n",
      "          (Wq): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=20, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wk): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=20, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wv): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=20, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): Linear(in_features=20, out_features=16, bias=True)\n",
      "          (Wf): Linear(in_features=20, out_features=16, bias=True)\n",
      "          (Wo): Linear(in_features=20, out_features=16, bias=True)\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): sLSTMBlock(\n",
      "          (layer_norm): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (Wz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=10, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=10, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=10, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wo): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=10, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ri): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ro): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "          (up_proj_left): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (up_proj_right): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (down_proj): Linear(in_features=21, out_features=10, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (3-4): 2 x xLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): mLSTMBlock(\n",
      "          (layer_norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "          (up_proj_left): Linear(in_features=1, out_features=2, bias=True)\n",
      "          (up_proj_right): Linear(in_features=1, out_features=16, bias=True)\n",
      "          (down_proj): Linear(in_features=16, out_features=1, bias=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (skip_connection): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (Wq): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wk): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wv): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (Wf): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (Wo): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): sLSTMBlock(\n",
      "          (layer_norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (Wz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wo): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ri): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ro): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "          (up_proj_left): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (up_proj_right): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (down_proj): Linear(in_features=21, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (5): xLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): mLSTMBlock(\n",
      "          (layer_norm): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "          (up_proj_left): Linear(in_features=4, out_features=8, bias=True)\n",
      "          (up_proj_right): Linear(in_features=4, out_features=16, bias=True)\n",
      "          (down_proj): Linear(in_features=16, out_features=4, bias=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (skip_connection): Linear(in_features=8, out_features=16, bias=True)\n",
      "          (Wq): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=8, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wk): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=8, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wv): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=8, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): Linear(in_features=8, out_features=16, bias=True)\n",
      "          (Wf): Linear(in_features=8, out_features=16, bias=True)\n",
      "          (Wo): Linear(in_features=8, out_features=16, bias=True)\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): sLSTMBlock(\n",
      "          (layer_norm): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (Wz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wo): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ri): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ro): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "          (up_proj_left): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (up_proj_right): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (down_proj): Linear(in_features=21, out_features=4, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (6-7): 2 x xLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): mLSTMBlock(\n",
      "          (layer_norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "          (up_proj_left): Linear(in_features=1, out_features=2, bias=True)\n",
      "          (up_proj_right): Linear(in_features=1, out_features=16, bias=True)\n",
      "          (down_proj): Linear(in_features=16, out_features=1, bias=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (skip_connection): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (Wq): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wk): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wv): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (Wf): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (Wo): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): sLSTMBlock(\n",
      "          (layer_norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (Wz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wo): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ri): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ro): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "          (up_proj_left): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (up_proj_right): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (down_proj): Linear(in_features=21, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (8): xLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): mLSTMBlock(\n",
      "          (layer_norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "          (up_proj_left): Linear(in_features=6, out_features=12, bias=True)\n",
      "          (up_proj_right): Linear(in_features=6, out_features=16, bias=True)\n",
      "          (down_proj): Linear(in_features=16, out_features=6, bias=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (skip_connection): Linear(in_features=12, out_features=16, bias=True)\n",
      "          (Wq): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=12, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wk): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=12, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wv): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=12, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): Linear(in_features=12, out_features=16, bias=True)\n",
      "          (Wf): Linear(in_features=12, out_features=16, bias=True)\n",
      "          (Wo): Linear(in_features=12, out_features=16, bias=True)\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): sLSTMBlock(\n",
      "          (layer_norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (Wz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wo): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ri): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ro): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "          (up_proj_left): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (up_proj_right): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (down_proj): Linear(in_features=21, out_features=6, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (9-10): 2 x xLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): mLSTMBlock(\n",
      "          (layer_norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "          (up_proj_left): Linear(in_features=1, out_features=2, bias=True)\n",
      "          (up_proj_right): Linear(in_features=1, out_features=16, bias=True)\n",
      "          (down_proj): Linear(in_features=16, out_features=1, bias=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (skip_connection): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (Wq): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wk): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wv): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=2, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (Wf): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (Wo): Linear(in_features=2, out_features=16, bias=True)\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): sLSTMBlock(\n",
      "          (layer_norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (Wz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wo): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ri): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ro): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "          (up_proj_left): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (up_proj_right): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (down_proj): Linear(in_features=21, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (11): xLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): mLSTMBlock(\n",
      "          (layer_norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "          (up_proj_left): Linear(in_features=6, out_features=12, bias=True)\n",
      "          (up_proj_right): Linear(in_features=6, out_features=16, bias=True)\n",
      "          (down_proj): Linear(in_features=16, out_features=6, bias=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (skip_connection): Linear(in_features=12, out_features=16, bias=True)\n",
      "          (Wq): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=12, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wk): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=12, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wv): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=12, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): Linear(in_features=12, out_features=16, bias=True)\n",
      "          (Wf): Linear(in_features=12, out_features=16, bias=True)\n",
      "          (Wo): Linear(in_features=12, out_features=16, bias=True)\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): sLSTMBlock(\n",
      "          (layer_norm): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (Wz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wo): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ri): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ro): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "          (up_proj_left): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (up_proj_right): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (down_proj): Linear(in_features=21, out_features=6, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (12): xLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): mLSTMBlock(\n",
      "          (layer_norm): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "          (up_proj_left): Linear(in_features=3, out_features=6, bias=True)\n",
      "          (up_proj_right): Linear(in_features=3, out_features=16, bias=True)\n",
      "          (down_proj): Linear(in_features=16, out_features=3, bias=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (skip_connection): Linear(in_features=6, out_features=16, bias=True)\n",
      "          (Wq): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wk): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wv): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=6, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): Linear(in_features=6, out_features=16, bias=True)\n",
      "          (Wf): Linear(in_features=6, out_features=16, bias=True)\n",
      "          (Wo): Linear(in_features=6, out_features=16, bias=True)\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): sLSTMBlock(\n",
      "          (layer_norm): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
      "          (causal_conv): CausalConv1D(\n",
      "            (conv): Conv1d(1, 1, kernel_size=(4,), stride=(1,), padding=(3,))\n",
      "          )\n",
      "          (Wz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wi): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Wo): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=3, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rz): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ri): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Rf): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (Ro): BlockDiagonal(\n",
      "            (blocks): ModuleList(\n",
      "              (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (group_norm): GroupNorm(1, 16, eps=1e-05, affine=True)\n",
      "          (up_proj_left): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (up_proj_right): Linear(in_features=16, out_features=21, bias=True)\n",
      "          (down_proj): Linear(in_features=21, out_features=3, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=38, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=5, bias=True)\n",
      "    (3): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_shapes, hidden_sizes, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            xLSTM(input_shapes[i][2], hidden_sizes[i], layers=\"ms\", num_heads=1, batch_first=True, dropout=0.2)\n",
    "            for i in range(13)\n",
    "        ])\n",
    "\n",
    "        self.flatten_size = self.get_flatten_size(input_shapes)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flatten_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def get_flatten_size(self, input_shapes):\n",
    "        with torch.no_grad():\n",
    "            x = [torch.zeros(1, input_shapes[i][1], input_shapes[i][2]) for i in range(13)]\n",
    "         \n",
    "            outputs = [self.lstm_layers[i](x[i])[0][:, -1, :].view(1, -1) for i in range(13)]\n",
    "          \n",
    "            flat_sizes = [output.shape[1] for output in outputs]\n",
    "            return sum(flat_sizes)\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        \n",
    "        lstm_outputs = [self.lstm_layers[i](inputs[i])[0][:, -1, :] for i in range(13)]\n",
    "\n",
    "        out = torch.cat(lstm_outputs, dim=1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "hidden_sizes = [16 for _ in range(13)]\n",
    "input_shapes = [\n",
    "    (3666, 661, 2),\n",
    "    (3666, 132, 1),\n",
    "    (3666, 32, 10),\n",
    "    (3666, 132, 1),\n",
    "    (3666, 247, 1),\n",
    "    (3666, 253, 4),\n",
    "    (3666, 253, 1),\n",
    "    (3666, 253, 1),\n",
    "    (3666, 247, 6),\n",
    "    (3666, 253, 1),\n",
    "    (3666, 27, 1),\n",
    "    (3666, 32, 6),\n",
    "    (3666, 27, 3)\n",
    "]\n",
    "num_classes = 5\n",
    "\n",
    "model = LSTMModel(input_shapes, hidden_sizes, num_classes).to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3382, 2.3500, 3.5941, 0.8313, 7.3320])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Assuming y is a NumPy array or can be converted to one\n",
    "\n",
    "\n",
    "num_samples_per_class = [np.sum(y_ == i) for i in range(5)]\n",
    "\n",
    "total_samples = sum(num_samples_per_class)\n",
    "class_weights = [total_samples / (5 * num_samples) for num_samples in num_samples_per_class]\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class ModelCheckpoint:\n",
    "    def __init__(self, filepath, monitor='val_loss', mode='min'):\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.best_state_dict = None\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.best_score = float('inf')\n",
    "        else:\n",
    "            self.best_score = float('-inf')\n",
    "\n",
    "    def __call__(self, score, model_state_dict):\n",
    "        if (self.mode == 'min' and score < self.best_score) or (self.mode == 'max' and score > self.best_score):\n",
    "            print(f\"Saving model with {self.monitor}: {score}\")\n",
    "            self.best_score = score\n",
    "            self.best_state_dict = model_state_dict\n",
    "            torch.save(model_state_dict, self.filepath)\n",
    "\n",
    "    def get_best_state_dict(self):\n",
    "        return self.best_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss(weight=weights)  # Assuming you're using CrossEntropyLoss for classification\n",
    "# scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\",patience=30,factor=0.1)\n",
    "# # scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[150,200], gamma=0.5)\n",
    "# checkpoint = ModelCheckpoint(filepath='best_model.pth', monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights).to(device)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\",patience=5,factor=0.1)\n",
    "checkpoint = ModelCheckpoint(filepath='best_model.pth', monitor='val_loss', mode='min')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hy138\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 10, Training Loss: 1.6046567797660827, Training Accuracy: 11.875%\n",
      "Epoch 1, Batch 20, Training Loss: 1.6004219710826875, Training Accuracy: 17.1875%\n",
      "Epoch 1, Batch 30, Training Loss: 1.5907228549321493, Training Accuracy: 25.104166666666668%\n",
      "Epoch 1, Batch 40, Training Loss: 1.5768080264329911, Training Accuracy: 28.59375%\n",
      "Epoch 1, Batch 50, Training Loss: 1.566940689086914, Training Accuracy: 31.4375%\n",
      "Epoch 1, Batch 60, Training Loss: 1.5544060250123342, Training Accuracy: 34.89583333333333%\n",
      "Epoch 1, Batch 70, Training Loss: 1.5411405886922565, Training Accuracy: 38.526785714285715%\n",
      "Epoch 1, Batch 80, Training Loss: 1.527623225748539, Training Accuracy: 41.7578125%\n",
      "Epoch 1, Batch 90, Training Loss: 1.514706089761522, Training Accuracy: 44.02777777777778%\n",
      "Epoch 1, Validation Loss: 1.3143903120704319, Validation Accuracy: 67.98365122615803%\n",
      "Epoch 1: Adam lr 0.0010\n",
      "Epoch 2, Batch 10, Training Loss: 1.3237672567367553, Training Accuracy: 67.1875%\n",
      "Epoch 2, Batch 20, Training Loss: 1.3394689738750458, Training Accuracy: 66.25%\n",
      "Epoch 2, Batch 30, Training Loss: 1.3070731004079184, Training Accuracy: 68.54166666666667%\n",
      "Epoch 2, Batch 40, Training Loss: 1.2896441489458084, Training Accuracy: 69.765625%\n",
      "Epoch 2, Batch 50, Training Loss: 1.2739457702636718, Training Accuracy: 70.3125%\n",
      "Epoch 2, Batch 60, Training Loss: 1.2552273511886596, Training Accuracy: 69.84375%\n",
      "Epoch 2, Batch 70, Training Loss: 1.24334693806512, Training Accuracy: 70.0%\n",
      "Epoch 2, Batch 80, Training Loss: 1.2299801245331765, Training Accuracy: 71.171875%\n",
      "Epoch 2, Batch 90, Training Loss: 1.221162916554345, Training Accuracy: 71.31944444444444%\n",
      "Epoch 2, Validation Loss: 1.0838915757510974, Validation Accuracy: 81.74386920980926%\n",
      "Epoch 2: Adam lr 0.0010\n",
      "Epoch 3, Batch 10, Training Loss: 1.1177257537841796, Training Accuracy: 75.9375%\n",
      "Epoch 3, Batch 20, Training Loss: 1.1065847277641296, Training Accuracy: 73.59375%\n",
      "Epoch 3, Batch 30, Training Loss: 1.12197292248408, Training Accuracy: 73.54166666666667%\n",
      "Epoch 3, Batch 40, Training Loss: 1.1208589628338814, Training Accuracy: 75.234375%\n",
      "Epoch 3, Batch 50, Training Loss: 1.1153501951694489, Training Accuracy: 75.75%\n",
      "Epoch 3, Batch 60, Training Loss: 1.1089356333017348, Training Accuracy: 76.09375%\n",
      "Epoch 3, Batch 70, Training Loss: 1.1054347344807216, Training Accuracy: 76.11607142857143%\n",
      "Epoch 3, Batch 80, Training Loss: 1.1050563678145409, Training Accuracy: 75.703125%\n",
      "Epoch 3, Batch 90, Training Loss: 1.0974848979049259, Training Accuracy: 76.52777777777777%\n",
      "Epoch 3, Validation Loss: 1.0362435268319172, Validation Accuracy: 85.69482288828338%\n",
      "Epoch 3: Adam lr 0.0010\n",
      "Epoch 4, Batch 10, Training Loss: 1.046844583749771, Training Accuracy: 80.0%\n",
      "Epoch 4, Batch 20, Training Loss: 1.0501226305961608, Training Accuracy: 78.90625%\n",
      "Epoch 4, Batch 30, Training Loss: 1.0522955814997355, Training Accuracy: 77.5%\n",
      "Epoch 4, Batch 40, Training Loss: 1.059617094695568, Training Accuracy: 77.734375%\n",
      "Epoch 4, Batch 50, Training Loss: 1.0483503437042236, Training Accuracy: 78.9375%\n",
      "Epoch 4, Batch 60, Training Loss: 1.0480383942524591, Training Accuracy: 79.47916666666667%\n",
      "Epoch 4, Batch 70, Training Loss: 1.0512884599821908, Training Accuracy: 79.77678571428571%\n",
      "Epoch 4, Batch 80, Training Loss: 1.048888560384512, Training Accuracy: 79.9609375%\n",
      "Epoch 4, Batch 90, Training Loss: 1.0493931611378988, Training Accuracy: 79.61805555555556%\n",
      "Epoch 4, Validation Loss: 1.0194268978160361, Validation Accuracy: 85.42234332425068%\n",
      "Epoch 4: Adam lr 0.0010\n",
      "Epoch 5, Batch 10, Training Loss: 1.0407783567905426, Training Accuracy: 84.0625%\n",
      "Epoch 5, Batch 20, Training Loss: 1.053429913520813, Training Accuracy: 83.75%\n",
      "Epoch 5, Batch 30, Training Loss: 1.047869078318278, Training Accuracy: 80.20833333333334%\n",
      "Epoch 5, Batch 40, Training Loss: 1.0512024447321893, Training Accuracy: 79.375%\n",
      "Epoch 5, Batch 50, Training Loss: 1.0434147489070893, Training Accuracy: 79.75%\n",
      "Epoch 5, Batch 60, Training Loss: 1.0409354358911513, Training Accuracy: 80.05208333333333%\n",
      "Epoch 5, Batch 70, Training Loss: 1.0387533204896109, Training Accuracy: 80.44642857142857%\n",
      "Epoch 5, Batch 80, Training Loss: 1.0374367184937001, Training Accuracy: 80.8984375%\n",
      "Epoch 5, Batch 90, Training Loss: 1.035835733678606, Training Accuracy: 80.76388888888889%\n",
      "Epoch 5, Validation Loss: 1.009664722110914, Validation Accuracy: 87.19346049046321%\n",
      "Epoch 5: Adam lr 0.0010\n",
      "Epoch 6, Batch 10, Training Loss: 1.0353075802326202, Training Accuracy: 82.1875%\n",
      "Epoch 6, Batch 20, Training Loss: 1.0271059423685074, Training Accuracy: 83.125%\n",
      "Epoch 6, Batch 30, Training Loss: 1.0130421698093415, Training Accuracy: 84.6875%\n",
      "Epoch 6, Batch 40, Training Loss: 1.01284601688385, Training Accuracy: 84.21875%\n",
      "Epoch 6, Batch 50, Training Loss: 1.0175310969352722, Training Accuracy: 83.8125%\n",
      "Epoch 6, Batch 60, Training Loss: 1.015392748514811, Training Accuracy: 84.21875%\n",
      "Epoch 6, Batch 70, Training Loss: 1.0160555584090096, Training Accuracy: 83.92857142857143%\n",
      "Epoch 6, Batch 80, Training Loss: 1.0113726638257503, Training Accuracy: 84.4921875%\n",
      "Epoch 6, Batch 90, Training Loss: 1.0107930137051477, Training Accuracy: 84.51388888888889%\n",
      "Epoch 6, Validation Loss: 0.9818151282227557, Validation Accuracy: 87.87465940054496%\n",
      "Epoch 6: Adam lr 0.0010\n",
      "Epoch 7, Batch 10, Training Loss: 0.9966516315937042, Training Accuracy: 87.8125%\n",
      "Epoch 7, Batch 20, Training Loss: 0.9915193885564804, Training Accuracy: 88.4375%\n",
      "Epoch 7, Batch 30, Training Loss: 0.9939005116621653, Training Accuracy: 88.125%\n",
      "Epoch 7, Batch 40, Training Loss: 0.9994742780923843, Training Accuracy: 86.953125%\n",
      "Epoch 7, Batch 50, Training Loss: 1.0026755011081696, Training Accuracy: 87.0%\n",
      "Epoch 7, Batch 60, Training Loss: 0.9988601575295131, Training Accuracy: 87.03125%\n",
      "Epoch 7, Batch 70, Training Loss: 1.0010484533650534, Training Accuracy: 86.33928571428572%\n",
      "Epoch 7, Batch 80, Training Loss: 0.9990289501845837, Training Accuracy: 86.71875%\n",
      "Epoch 7, Batch 90, Training Loss: 0.9969714363416036, Training Accuracy: 86.94444444444444%\n",
      "Epoch 7, Validation Loss: 0.9842413689779199, Validation Accuracy: 88.41961852861036%\n",
      "Epoch 7: Adam lr 0.0010\n",
      "Epoch 8, Batch 10, Training Loss: 0.9975683271884919, Training Accuracy: 85.0%\n",
      "Epoch 8, Batch 20, Training Loss: 1.0051411479711532, Training Accuracy: 85.625%\n",
      "Epoch 8, Batch 30, Training Loss: 1.000557444492976, Training Accuracy: 86.14583333333333%\n",
      "Epoch 8, Batch 40, Training Loss: 0.9995714768767356, Training Accuracy: 85.546875%\n",
      "Epoch 8, Batch 50, Training Loss: 0.9983368194103241, Training Accuracy: 86.125%\n",
      "Epoch 8, Batch 60, Training Loss: 1.0004022717475891, Training Accuracy: 86.25%\n",
      "Epoch 8, Batch 70, Training Loss: 0.9981637844017573, Training Accuracy: 86.60714285714286%\n",
      "Epoch 8, Batch 80, Training Loss: 0.9935874812304973, Training Accuracy: 87.1484375%\n",
      "Epoch 8, Batch 90, Training Loss: 0.9938969168398115, Training Accuracy: 87.08333333333333%\n",
      "Epoch 8, Validation Loss: 0.9953365248182545, Validation Accuracy: 89.23705722070845%\n",
      "Epoch 8: Adam lr 0.0010\n",
      "Epoch 9, Batch 10, Training Loss: 1.00311998128891, Training Accuracy: 88.75%\n",
      "Epoch 9, Batch 20, Training Loss: 0.9928251296281815, Training Accuracy: 88.4375%\n",
      "Epoch 9, Batch 30, Training Loss: 0.9886505385239919, Training Accuracy: 88.125%\n",
      "Epoch 9, Batch 40, Training Loss: 0.9864041790366173, Training Accuracy: 88.4375%\n",
      "Epoch 9, Batch 50, Training Loss: 0.9843119180202484, Training Accuracy: 88.6875%\n",
      "Epoch 9, Batch 60, Training Loss: 0.980098573366801, Training Accuracy: 89.16666666666667%\n",
      "Epoch 9, Batch 70, Training Loss: 0.9791791141033173, Training Accuracy: 89.50892857142857%\n",
      "Epoch 9, Batch 80, Training Loss: 0.9808259844779968, Training Accuracy: 89.21875%\n",
      "Epoch 9, Batch 90, Training Loss: 0.9835774554146661, Training Accuracy: 89.13194444444444%\n",
      "Epoch 9, Validation Loss: 0.9798623504845992, Validation Accuracy: 91.41689373297002%\n",
      "Epoch 9: Adam lr 0.0010\n",
      "Epoch 10, Batch 10, Training Loss: 0.9768032431602478, Training Accuracy: 88.125%\n",
      "Epoch 10, Batch 20, Training Loss: 0.9889037400484085, Training Accuracy: 86.71875%\n",
      "Epoch 10, Batch 30, Training Loss: 0.9880528628826142, Training Accuracy: 87.5%\n",
      "Epoch 10, Batch 40, Training Loss: 0.9837822154164314, Training Accuracy: 88.046875%\n",
      "Epoch 10, Batch 50, Training Loss: 0.9855177557468414, Training Accuracy: 88.0625%\n",
      "Epoch 10, Batch 60, Training Loss: 0.9850664496421814, Training Accuracy: 88.33333333333333%\n",
      "Epoch 10, Batch 70, Training Loss: 0.986404948575156, Training Accuracy: 88.21428571428571%\n",
      "Epoch 10, Batch 80, Training Loss: 0.986039063334465, Training Accuracy: 88.4375%\n",
      "Epoch 10, Batch 90, Training Loss: 0.9839574111832513, Training Accuracy: 88.81944444444444%\n",
      "Epoch 10, Validation Loss: 0.9898062737091727, Validation Accuracy: 89.64577656675749%\n",
      "Epoch 10: Adam lr 0.0010\n",
      "Epoch 11, Batch 10, Training Loss: 0.9791539669036865, Training Accuracy: 90.3125%\n",
      "Epoch 11, Batch 20, Training Loss: 0.9801718682050705, Training Accuracy: 89.6875%\n",
      "Epoch 11, Batch 30, Training Loss: 0.9755679309368134, Training Accuracy: 89.89583333333333%\n",
      "Epoch 11, Batch 40, Training Loss: 0.9732037931680679, Training Accuracy: 90.78125%\n",
      "Epoch 11, Batch 50, Training Loss: 0.9764472782611847, Training Accuracy: 90.9375%\n",
      "Epoch 11, Batch 60, Training Loss: 0.9796201378107071, Training Accuracy: 90.57291666666667%\n",
      "Epoch 11, Batch 70, Training Loss: 0.9767419295651573, Training Accuracy: 90.625%\n",
      "Epoch 11, Batch 80, Training Loss: 0.9767534501850605, Training Accuracy: 90.5078125%\n",
      "Epoch 11, Batch 90, Training Loss: 0.9761526617738936, Training Accuracy: 90.41666666666667%\n",
      "Epoch 11, Validation Loss: 0.9794036357299142, Validation Accuracy: 92.09809264305177%\n",
      "Epoch 11: Adam lr 0.0010\n",
      "Epoch 12, Batch 10, Training Loss: 0.9600691735744477, Training Accuracy: 91.875%\n",
      "Epoch 12, Batch 20, Training Loss: 0.9720645695924759, Training Accuracy: 91.5625%\n",
      "Epoch 12, Batch 30, Training Loss: 0.96966193318367, Training Accuracy: 92.39583333333333%\n",
      "Epoch 12, Batch 40, Training Loss: 0.9699333190917969, Training Accuracy: 92.03125%\n",
      "Epoch 12, Batch 50, Training Loss: 0.9670797383785248, Training Accuracy: 91.9375%\n",
      "Epoch 12, Batch 60, Training Loss: 0.9644808739423751, Training Accuracy: 91.92708333333334%\n",
      "Epoch 12, Batch 70, Training Loss: 0.9636110527174814, Training Accuracy: 91.96428571428571%\n",
      "Epoch 12, Batch 80, Training Loss: 0.9666214972734452, Training Accuracy: 91.640625%\n",
      "Epoch 12, Batch 90, Training Loss: 0.9667115410168966, Training Accuracy: 91.35416666666667%\n",
      "Epoch 12, Validation Loss: 0.9570907820825991, Validation Accuracy: 93.73297002724796%\n",
      "Epoch 12: Adam lr 0.0010\n",
      "Epoch 13, Batch 10, Training Loss: 0.9635721802711487, Training Accuracy: 91.875%\n",
      "Epoch 13, Batch 20, Training Loss: 0.9616452783346177, Training Accuracy: 91.5625%\n",
      "Epoch 13, Batch 30, Training Loss: 0.9553121348222097, Training Accuracy: 92.39583333333333%\n",
      "Epoch 13, Batch 40, Training Loss: 0.9582322403788567, Training Accuracy: 92.109375%\n",
      "Epoch 13, Batch 50, Training Loss: 0.959448391199112, Training Accuracy: 92.375%\n",
      "Epoch 13, Batch 60, Training Loss: 0.9606024493773778, Training Accuracy: 92.08333333333333%\n",
      "Epoch 13, Batch 70, Training Loss: 0.9635739939553397, Training Accuracy: 91.83035714285714%\n",
      "Epoch 13, Batch 80, Training Loss: 0.9627356804907322, Training Accuracy: 91.8359375%\n",
      "Epoch 13, Batch 90, Training Loss: 0.9619198004404704, Training Accuracy: 92.11805555555556%\n",
      "Epoch 13, Validation Loss: 0.9519951136215873, Validation Accuracy: 92.91553133514986%\n",
      "Epoch 13: Adam lr 0.0010\n",
      "Epoch 14, Batch 10, Training Loss: 0.9782794117927551, Training Accuracy: 88.4375%\n",
      "Epoch 14, Batch 20, Training Loss: 0.964097061753273, Training Accuracy: 90.9375%\n",
      "Epoch 14, Batch 30, Training Loss: 0.9685563564300537, Training Accuracy: 90.9375%\n",
      "Epoch 14, Batch 40, Training Loss: 0.9639305904507637, Training Accuracy: 91.015625%\n",
      "Epoch 14, Batch 50, Training Loss: 0.9644796335697174, Training Accuracy: 90.9375%\n",
      "Epoch 14, Batch 60, Training Loss: 0.965664608279864, Training Accuracy: 91.19791666666667%\n",
      "Epoch 14, Batch 70, Training Loss: 0.9646507373877934, Training Accuracy: 91.47321428571429%\n",
      "Epoch 14, Batch 80, Training Loss: 0.9624881200492382, Training Accuracy: 91.7578125%\n",
      "Epoch 14, Batch 90, Training Loss: 0.963991234699885, Training Accuracy: 91.90972222222223%\n",
      "Epoch 14, Validation Loss: 0.951297420522441, Validation Accuracy: 94.6866485013624%\n",
      "Epoch 14: Adam lr 0.0010\n",
      "Epoch 15, Batch 10, Training Loss: 0.9444165349006652, Training Accuracy: 92.8125%\n",
      "Epoch 15, Batch 20, Training Loss: 0.9586896032094956, Training Accuracy: 91.25%\n",
      "Epoch 15, Batch 30, Training Loss: 0.9570283730824788, Training Accuracy: 91.66666666666666%\n",
      "Epoch 15, Batch 40, Training Loss: 0.9611142501235008, Training Accuracy: 91.484375%\n",
      "Epoch 15, Batch 50, Training Loss: 0.9567977237701416, Training Accuracy: 92.375%\n",
      "Epoch 15, Batch 60, Training Loss: 0.9573712110519409, Training Accuracy: 92.39583333333333%\n",
      "Epoch 15, Batch 70, Training Loss: 0.9553872108459472, Training Accuracy: 92.72321428571428%\n",
      "Epoch 15, Batch 80, Training Loss: 0.9570335082709789, Training Accuracy: 92.578125%\n",
      "Epoch 15, Batch 90, Training Loss: 0.957397276825375, Training Accuracy: 92.36111111111111%\n",
      "Epoch 15, Validation Loss: 0.9424899868343187, Validation Accuracy: 94.95912806539509%\n",
      "Epoch 15: Adam lr 0.0010\n",
      "Epoch 16, Batch 10, Training Loss: 0.9541866779327393, Training Accuracy: 92.1875%\n",
      "Epoch 16, Batch 20, Training Loss: 0.9564614117145538, Training Accuracy: 92.34375%\n",
      "Epoch 16, Batch 30, Training Loss: 0.9568664054075877, Training Accuracy: 92.60416666666667%\n",
      "Epoch 16, Batch 40, Training Loss: 0.9559548616409301, Training Accuracy: 92.8125%\n",
      "Epoch 16, Batch 50, Training Loss: 0.9564387714862823, Training Accuracy: 93.3125%\n",
      "Epoch 16, Batch 60, Training Loss: 0.9660089949766795, Training Accuracy: 92.70833333333334%\n",
      "Epoch 16, Batch 70, Training Loss: 0.9638753150190625, Training Accuracy: 92.72321428571428%\n",
      "Epoch 16, Batch 80, Training Loss: 0.9638965159654618, Training Accuracy: 92.7734375%\n",
      "Epoch 16, Batch 90, Training Loss: 0.9605427556567722, Training Accuracy: 93.15972222222221%\n",
      "Epoch 16, Validation Loss: 0.9356913048288097, Validation Accuracy: 96.45776566757493%\n",
      "Epoch 16: Adam lr 0.0010\n",
      "Epoch 17, Batch 10, Training Loss: 0.9479556500911712, Training Accuracy: 95.0%\n",
      "Epoch 17, Batch 20, Training Loss: 0.9577343821525574, Training Accuracy: 93.75%\n",
      "Epoch 17, Batch 30, Training Loss: 0.9584118763605753, Training Accuracy: 93.22916666666666%\n",
      "Epoch 17, Batch 40, Training Loss: 0.9546121075749397, Training Accuracy: 93.125%\n",
      "Epoch 17, Batch 50, Training Loss: 0.9518588423728943, Training Accuracy: 93.3125%\n",
      "Epoch 17, Batch 60, Training Loss: 0.9528400113185247, Training Accuracy: 93.38541666666667%\n",
      "Epoch 17, Batch 70, Training Loss: 0.9550254847322192, Training Accuracy: 93.34821428571428%\n",
      "Epoch 17, Batch 80, Training Loss: 0.9533940620720387, Training Accuracy: 93.3984375%\n",
      "Epoch 17, Batch 90, Training Loss: 0.9524485283427768, Training Accuracy: 93.50694444444444%\n",
      "Epoch 17, Validation Loss: 0.9376485969709314, Validation Accuracy: 96.04904632152589%\n",
      "Epoch 17: Adam lr 0.0010\n",
      "Epoch 18, Batch 10, Training Loss: 0.9364045619964599, Training Accuracy: 95.625%\n",
      "Epoch 18, Batch 20, Training Loss: 0.9467642128467559, Training Accuracy: 93.59375%\n",
      "Epoch 18, Batch 30, Training Loss: 0.9472444951534271, Training Accuracy: 93.75%\n",
      "Epoch 18, Batch 40, Training Loss: 0.9507244825363159, Training Accuracy: 93.59375%\n",
      "Epoch 18, Batch 50, Training Loss: 0.9524639236927033, Training Accuracy: 93.5625%\n",
      "Epoch 18, Batch 60, Training Loss: 0.9501072148482005, Training Accuracy: 93.69791666666667%\n",
      "Epoch 18, Batch 70, Training Loss: 0.9520084857940674, Training Accuracy: 93.34821428571428%\n",
      "Epoch 18, Batch 80, Training Loss: 0.9510624699294568, Training Accuracy: 93.3203125%\n",
      "Epoch 18, Batch 90, Training Loss: 0.9496156533559164, Training Accuracy: 93.50694444444444%\n",
      "Epoch 18, Validation Loss: 0.9416017402773318, Validation Accuracy: 95.2316076294278%\n",
      "Epoch 18: Adam lr 0.0010\n",
      "Epoch 19, Batch 10, Training Loss: 0.9755111038684845, Training Accuracy: 93.75%\n",
      "Epoch 19, Batch 20, Training Loss: 0.967821654677391, Training Accuracy: 93.28125%\n",
      "Epoch 19, Batch 30, Training Loss: 0.9682208140691121, Training Accuracy: 93.02083333333333%\n",
      "Epoch 19, Batch 40, Training Loss: 0.9616826698184013, Training Accuracy: 93.4375%\n",
      "Epoch 19, Batch 50, Training Loss: 0.9564577209949493, Training Accuracy: 93.8125%\n",
      "Epoch 19, Batch 60, Training Loss: 0.9570580323537191, Training Accuracy: 93.33333333333333%\n",
      "Epoch 19, Batch 70, Training Loss: 0.9552756062575749, Training Accuracy: 93.48214285714286%\n",
      "Epoch 19, Batch 80, Training Loss: 0.9543568156659603, Training Accuracy: 93.828125%\n",
      "Epoch 19, Batch 90, Training Loss: 0.9532538758383857, Training Accuracy: 93.92361111111111%\n",
      "Epoch 19, Validation Loss: 0.9353957176208496, Validation Accuracy: 96.32152588555857%\n",
      "Epoch 19: Adam lr 0.0010\n",
      "Epoch 20, Batch 10, Training Loss: 0.9681759178638458, Training Accuracy: 91.5625%\n",
      "Epoch 20, Batch 20, Training Loss: 0.9597813129425049, Training Accuracy: 92.34375%\n",
      "Epoch 20, Batch 30, Training Loss: 0.9595111091931661, Training Accuracy: 91.97916666666667%\n",
      "Epoch 20, Batch 40, Training Loss: 0.9540474444627762, Training Accuracy: 92.578125%\n",
      "Epoch 20, Batch 50, Training Loss: 0.9518152868747711, Training Accuracy: 93.0625%\n",
      "Epoch 20, Batch 60, Training Loss: 0.9494299918413163, Training Accuracy: 93.59375%\n",
      "Epoch 20, Batch 70, Training Loss: 0.9505084472043174, Training Accuracy: 93.66071428571429%\n",
      "Epoch 20, Batch 80, Training Loss: 0.9493683911859989, Training Accuracy: 93.6328125%\n",
      "Epoch 20, Batch 90, Training Loss: 0.9483834637535943, Training Accuracy: 94.02777777777777%\n",
      "Epoch 20, Validation Loss: 0.9385145824888478, Validation Accuracy: 95.2316076294278%\n",
      "Epoch 20: Adam lr 0.0010\n",
      "Epoch 21, Batch 10, Training Loss: 0.962984049320221, Training Accuracy: 91.5625%\n",
      "Epoch 21, Batch 20, Training Loss: 0.9637643992900848, Training Accuracy: 91.875%\n",
      "Epoch 21, Batch 30, Training Loss: 0.9636484642823537, Training Accuracy: 91.875%\n",
      "Epoch 21, Batch 40, Training Loss: 0.9640946835279465, Training Accuracy: 92.421875%\n",
      "Epoch 21, Batch 50, Training Loss: 0.9574841558933258, Training Accuracy: 93.1875%\n",
      "Epoch 21, Batch 60, Training Loss: 0.953065092364947, Training Accuracy: 93.69791666666667%\n",
      "Epoch 21, Batch 70, Training Loss: 0.9514648990971701, Training Accuracy: 93.79464285714286%\n",
      "Epoch 21, Batch 80, Training Loss: 0.9525702059268951, Training Accuracy: 93.6328125%\n",
      "Epoch 21, Batch 90, Training Loss: 0.951134035984675, Training Accuracy: 93.64583333333333%\n",
      "Epoch 21, Validation Loss: 0.940653585869333, Validation Accuracy: 95.50408719346049%\n",
      "Epoch 21: Adam lr 0.0010\n",
      "Epoch 22, Batch 10, Training Loss: 0.9429536461830139, Training Accuracy: 93.4375%\n",
      "Epoch 22, Batch 20, Training Loss: 0.9417559087276459, Training Accuracy: 94.53125%\n",
      "Epoch 22, Batch 30, Training Loss: 0.9389090339342753, Training Accuracy: 94.58333333333333%\n",
      "Epoch 22, Batch 40, Training Loss: 0.9419890820980072, Training Accuracy: 94.609375%\n",
      "Epoch 22, Batch 50, Training Loss: 0.9421891736984253, Training Accuracy: 94.75%\n",
      "Epoch 22, Batch 60, Training Loss: 0.9445156534512837, Training Accuracy: 94.27083333333334%\n",
      "Epoch 22, Batch 70, Training Loss: 0.9441444167069026, Training Accuracy: 94.59821428571429%\n",
      "Epoch 22, Batch 80, Training Loss: 0.9441820092499256, Training Accuracy: 94.609375%\n",
      "Epoch 22, Batch 90, Training Loss: 0.9471912377410465, Training Accuracy: 94.34027777777779%\n",
      "Epoch 22, Validation Loss: 0.9340842729029448, Validation Accuracy: 96.04904632152589%\n",
      "Epoch 22: Adam lr 0.0010\n",
      "Epoch 23, Batch 10, Training Loss: 0.9328200101852417, Training Accuracy: 95.3125%\n",
      "Epoch 23, Batch 20, Training Loss: 0.939235270023346, Training Accuracy: 94.375%\n",
      "Epoch 23, Batch 30, Training Loss: 0.9488791068394978, Training Accuracy: 93.22916666666666%\n",
      "Epoch 23, Batch 40, Training Loss: 0.9468898490071297, Training Accuracy: 93.59375%\n",
      "Epoch 23, Batch 50, Training Loss: 0.9461127042770385, Training Accuracy: 93.625%\n",
      "Epoch 23, Batch 60, Training Loss: 0.9459879219532012, Training Accuracy: 93.59375%\n",
      "Epoch 23, Batch 70, Training Loss: 0.9452576160430908, Training Accuracy: 93.92857142857143%\n",
      "Epoch 23, Batch 80, Training Loss: 0.9459048822522164, Training Accuracy: 93.9453125%\n",
      "Epoch 23, Batch 90, Training Loss: 0.9436821573310428, Training Accuracy: 94.20138888888889%\n",
      "Epoch 23, Validation Loss: 0.9348702949026356, Validation Accuracy: 96.45776566757493%\n",
      "Epoch 23: Adam lr 0.0010\n",
      "Epoch 24, Batch 10, Training Loss: 0.9589166700839996, Training Accuracy: 95.0%\n",
      "Epoch 24, Batch 20, Training Loss: 0.9429850339889526, Training Accuracy: 95.46875%\n",
      "Epoch 24, Batch 30, Training Loss: 0.9404165625572205, Training Accuracy: 95.41666666666667%\n",
      "Epoch 24, Batch 40, Training Loss: 0.9424538820981979, Training Accuracy: 95.078125%\n",
      "Epoch 24, Batch 50, Training Loss: 0.9415037882328033, Training Accuracy: 95.1875%\n",
      "Epoch 24, Batch 60, Training Loss: 0.9472771088282267, Training Accuracy: 94.79166666666666%\n",
      "Epoch 24, Batch 70, Training Loss: 0.9473997064999171, Training Accuracy: 94.6875%\n",
      "Epoch 24, Batch 80, Training Loss: 0.9485309325158596, Training Accuracy: 94.6484375%\n",
      "Epoch 24, Batch 90, Training Loss: 0.9472727000713348, Training Accuracy: 94.75694444444444%\n",
      "Epoch 24, Validation Loss: 0.9356659210246542, Validation Accuracy: 96.32152588555857%\n",
      "Epoch 24: Adam lr 0.0010\n",
      "Epoch 25, Batch 10, Training Loss: 0.9525359094142913, Training Accuracy: 93.4375%\n",
      "Epoch 25, Batch 20, Training Loss: 0.9580636203289032, Training Accuracy: 93.59375%\n",
      "Epoch 25, Batch 30, Training Loss: 0.9560907701651256, Training Accuracy: 93.64583333333333%\n",
      "Epoch 25, Batch 40, Training Loss: 0.9536404848098755, Training Accuracy: 93.90625%\n",
      "Epoch 25, Batch 50, Training Loss: 0.9478910422325134, Training Accuracy: 94.5625%\n",
      "Epoch 25, Batch 60, Training Loss: 0.947756243745486, Training Accuracy: 94.42708333333333%\n",
      "Epoch 25, Batch 70, Training Loss: 0.9459437625748771, Training Accuracy: 94.6875%\n",
      "Epoch 25, Batch 80, Training Loss: 0.9422654002904892, Training Accuracy: 95.234375%\n",
      "Epoch 25, Batch 90, Training Loss: 0.9412200795279608, Training Accuracy: 95.20833333333333%\n",
      "Epoch 25, Validation Loss: 0.9326566768729169, Validation Accuracy: 96.73024523160763%\n",
      "Epoch 25: Adam lr 0.0010\n",
      "Epoch 26, Batch 10, Training Loss: 0.925876647233963, Training Accuracy: 96.5625%\n",
      "Epoch 26, Batch 20, Training Loss: 0.9317277550697327, Training Accuracy: 96.5625%\n",
      "Epoch 26, Batch 30, Training Loss: 0.9367941637833913, Training Accuracy: 96.14583333333333%\n",
      "Epoch 26, Batch 40, Training Loss: 0.9416293978691102, Training Accuracy: 95.546875%\n",
      "Epoch 26, Batch 50, Training Loss: 0.9417835485935211, Training Accuracy: 95.125%\n",
      "Epoch 26, Batch 60, Training Loss: 0.9429684698581695, Training Accuracy: 94.94791666666667%\n",
      "Epoch 26, Batch 70, Training Loss: 0.944315220628466, Training Accuracy: 94.64285714285714%\n",
      "Epoch 26, Batch 80, Training Loss: 0.9450484186410903, Training Accuracy: 94.5703125%\n",
      "Epoch 26, Batch 90, Training Loss: 0.9437048819330004, Training Accuracy: 94.79166666666666%\n",
      "Epoch 26, Validation Loss: 0.9361913929814878, Validation Accuracy: 96.04904632152589%\n",
      "Epoch 26: Adam lr 0.0010\n",
      "Epoch 27, Batch 10, Training Loss: 0.9416335046291351, Training Accuracy: 94.6875%\n",
      "Epoch 27, Batch 20, Training Loss: 0.9433504819869996, Training Accuracy: 94.84375%\n",
      "Epoch 27, Batch 30, Training Loss: 0.9401982108751933, Training Accuracy: 94.89583333333333%\n",
      "Epoch 27, Batch 40, Training Loss: 0.9413811072707177, Training Accuracy: 94.84375%\n",
      "Epoch 27, Batch 50, Training Loss: 0.9394526886940002, Training Accuracy: 95.3125%\n",
      "Epoch 27, Batch 60, Training Loss: 0.9405808647473654, Training Accuracy: 95.52083333333333%\n",
      "Epoch 27, Batch 70, Training Loss: 0.9418406920773642, Training Accuracy: 95.49107142857143%\n",
      "Epoch 27, Batch 80, Training Loss: 0.944089088588953, Training Accuracy: 95.15625%\n",
      "Epoch 27, Batch 90, Training Loss: 0.9436985181437598, Training Accuracy: 95.10416666666667%\n",
      "Epoch 27, Validation Loss: 0.9363481713377911, Validation Accuracy: 95.2316076294278%\n",
      "Epoch 27: Adam lr 0.0010\n",
      "Epoch 28, Batch 10, Training Loss: 0.9429316043853759, Training Accuracy: 93.75%\n",
      "Epoch 28, Batch 20, Training Loss: 0.9476201206445694, Training Accuracy: 94.0625%\n",
      "Epoch 28, Batch 30, Training Loss: 0.9386380771795909, Training Accuracy: 95.20833333333333%\n",
      "Epoch 28, Batch 40, Training Loss: 0.9361510440707207, Training Accuracy: 95.625%\n",
      "Epoch 28, Batch 50, Training Loss: 0.938723121881485, Training Accuracy: 95.3125%\n",
      "Epoch 28, Batch 60, Training Loss: 0.9399912854035696, Training Accuracy: 95.3125%\n",
      "Epoch 28, Batch 70, Training Loss: 0.9401197620800563, Training Accuracy: 95.26785714285714%\n",
      "Epoch 28, Batch 80, Training Loss: 0.9389386124908924, Training Accuracy: 95.3515625%\n",
      "Epoch 28, Batch 90, Training Loss: 0.9401489728026919, Training Accuracy: 95.13888888888889%\n",
      "Epoch 28, Validation Loss: 0.9336142747298531, Validation Accuracy: 96.04904632152589%\n",
      "Epoch 28: Adam lr 0.0010\n",
      "Epoch 29, Batch 10, Training Loss: 0.9245861589908599, Training Accuracy: 97.1875%\n",
      "Epoch 29, Batch 20, Training Loss: 0.9307199954986572, Training Accuracy: 96.40625%\n",
      "Epoch 29, Batch 30, Training Loss: 0.9365460693836212, Training Accuracy: 95.83333333333334%\n",
      "Epoch 29, Batch 40, Training Loss: 0.9366787314414978, Training Accuracy: 95.390625%\n",
      "Epoch 29, Batch 50, Training Loss: 0.936877111196518, Training Accuracy: 95.1875%\n",
      "Epoch 29, Batch 60, Training Loss: 0.9356015890836715, Training Accuracy: 95.20833333333333%\n",
      "Epoch 29, Batch 70, Training Loss: 0.9384058569158826, Training Accuracy: 95.26785714285714%\n",
      "Epoch 29, Batch 80, Training Loss: 0.939690400660038, Training Accuracy: 95.234375%\n",
      "Epoch 29, Batch 90, Training Loss: 0.9399338291751014, Training Accuracy: 95.20833333333333%\n",
      "Epoch 29, Validation Loss: 0.9413715782372848, Validation Accuracy: 95.36784741144415%\n",
      "Epoch 29: Adam lr 0.0010\n",
      "Epoch 30, Batch 10, Training Loss: 0.9321701228618622, Training Accuracy: 94.375%\n",
      "Epoch 30, Batch 20, Training Loss: 0.9400891631841659, Training Accuracy: 94.53125%\n",
      "Epoch 30, Batch 30, Training Loss: 0.9392183363437653, Training Accuracy: 94.47916666666667%\n",
      "Epoch 30, Batch 40, Training Loss: 0.9387337028980255, Training Accuracy: 94.84375%\n",
      "Epoch 30, Batch 50, Training Loss: 0.9375845527648926, Training Accuracy: 94.9375%\n",
      "Epoch 30, Batch 60, Training Loss: 0.9355228920777638, Training Accuracy: 95.20833333333333%\n",
      "Epoch 30, Batch 70, Training Loss: 0.9343088413987841, Training Accuracy: 95.44642857142858%\n",
      "Epoch 30, Batch 80, Training Loss: 0.9353299587965012, Training Accuracy: 95.390625%\n",
      "Epoch 30, Batch 90, Training Loss: 0.9365626235802968, Training Accuracy: 95.45138888888889%\n",
      "Epoch 30, Validation Loss: 0.9381387544714886, Validation Accuracy: 94.95912806539509%\n",
      "Epoch 30: Adam lr 0.0010\n",
      "Epoch 31, Batch 10, Training Loss: 0.9402370691299439, Training Accuracy: 95.3125%\n",
      "Epoch 31, Batch 20, Training Loss: 0.9301881700754165, Training Accuracy: 96.25%\n",
      "Epoch 31, Batch 30, Training Loss: 0.932479860385259, Training Accuracy: 96.14583333333333%\n",
      "Epoch 31, Batch 40, Training Loss: 0.9319763645529747, Training Accuracy: 96.09375%\n",
      "Epoch 31, Batch 50, Training Loss: 0.9343246817588806, Training Accuracy: 96.0%\n",
      "Epoch 31, Batch 60, Training Loss: 0.9336519052584966, Training Accuracy: 96.04166666666667%\n",
      "Epoch 31, Batch 70, Training Loss: 0.9369839583124433, Training Accuracy: 95.66964285714286%\n",
      "Epoch 31, Batch 80, Training Loss: 0.9390215523540973, Training Accuracy: 95.4296875%\n",
      "Epoch 31, Batch 90, Training Loss: 0.9415387908617655, Training Accuracy: 95.38194444444444%\n",
      "Epoch 31, Validation Loss: 0.9473452619884325, Validation Accuracy: 95.36784741144415%\n",
      "Epoch 31: Adam lr 0.0001\n",
      "Epoch 32, Batch 10, Training Loss: 0.9540316641330719, Training Accuracy: 96.5625%\n",
      "Epoch 32, Batch 20, Training Loss: 0.9457364052534103, Training Accuracy: 96.40625%\n",
      "Epoch 32, Batch 30, Training Loss: 0.9455411374568939, Training Accuracy: 96.04166666666667%\n",
      "Epoch 32, Batch 40, Training Loss: 0.9435279294848442, Training Accuracy: 95.859375%\n",
      "Epoch 32, Batch 50, Training Loss: 0.9422657597064972, Training Accuracy: 95.9375%\n",
      "Epoch 32, Batch 60, Training Loss: 0.9417019337415695, Training Accuracy: 95.67708333333333%\n",
      "Epoch 32, Batch 70, Training Loss: 0.9425543189048767, Training Accuracy: 95.625%\n",
      "Epoch 32, Batch 80, Training Loss: 0.9423765361309051, Training Accuracy: 95.5078125%\n",
      "Epoch 32, Batch 90, Training Loss: 0.9429224219587115, Training Accuracy: 95.38194444444444%\n",
      "Epoch 32, Validation Loss: 0.9352908989657527, Validation Accuracy: 96.59400544959128%\n",
      "Epoch 32: Adam lr 0.0001\n",
      "Epoch 33, Batch 10, Training Loss: 0.9293078303337097, Training Accuracy: 97.5%\n",
      "Epoch 33, Batch 20, Training Loss: 0.934182870388031, Training Accuracy: 96.40625%\n",
      "Epoch 33, Batch 30, Training Loss: 0.9346240381399791, Training Accuracy: 96.45833333333333%\n",
      "Epoch 33, Batch 40, Training Loss: 0.9350655391812325, Training Accuracy: 96.5625%\n",
      "Epoch 33, Batch 50, Training Loss: 0.9384448254108428, Training Accuracy: 96.1875%\n",
      "Epoch 33, Batch 60, Training Loss: 0.9388441652059555, Training Accuracy: 95.98958333333333%\n",
      "Epoch 33, Batch 70, Training Loss: 0.9388342303889138, Training Accuracy: 95.89285714285715%\n",
      "Epoch 33, Batch 80, Training Loss: 0.9376274541020393, Training Accuracy: 95.78125%\n",
      "Epoch 33, Batch 90, Training Loss: 0.9382742133405474, Training Accuracy: 95.79861111111111%\n",
      "Epoch 33, Validation Loss: 0.9321551193361697, Validation Accuracy: 96.59400544959128%\n",
      "Epoch 33: Adam lr 0.0001\n",
      "Epoch 34, Batch 10, Training Loss: 0.9598141849040985, Training Accuracy: 93.125%\n",
      "Epoch 34, Batch 20, Training Loss: 0.9495599269866943, Training Accuracy: 94.0625%\n",
      "Epoch 34, Batch 30, Training Loss: 0.9406718750794728, Training Accuracy: 94.89583333333333%\n",
      "Epoch 34, Batch 40, Training Loss: 0.9384966999292373, Training Accuracy: 95.390625%\n",
      "Epoch 34, Batch 50, Training Loss: 0.9383030593395233, Training Accuracy: 95.5625%\n",
      "Epoch 34, Batch 60, Training Loss: 0.9392050077517827, Training Accuracy: 95.625%\n",
      "Epoch 34, Batch 70, Training Loss: 0.9369986091341291, Training Accuracy: 95.9375%\n",
      "Epoch 34, Batch 80, Training Loss: 0.9357148021459579, Training Accuracy: 96.09375%\n",
      "Epoch 34, Batch 90, Training Loss: 0.935368886258867, Training Accuracy: 96.11111111111111%\n",
      "Epoch 34, Validation Loss: 0.9312464024709619, Validation Accuracy: 96.73024523160763%\n",
      "Epoch 34: Adam lr 0.0001\n",
      "Epoch 35, Batch 10, Training Loss: 0.9329045355319977, Training Accuracy: 95.3125%\n",
      "Epoch 35, Batch 20, Training Loss: 0.9400843173265457, Training Accuracy: 95.3125%\n",
      "Epoch 35, Batch 30, Training Loss: 0.9369432727495829, Training Accuracy: 95.83333333333334%\n",
      "Epoch 35, Batch 40, Training Loss: 0.9379426926374436, Training Accuracy: 95.625%\n",
      "Epoch 35, Batch 50, Training Loss: 0.9390531814098358, Training Accuracy: 95.375%\n",
      "Epoch 35, Batch 60, Training Loss: 0.9382412761449814, Training Accuracy: 95.78125%\n",
      "Epoch 35, Batch 70, Training Loss: 0.9351978233882359, Training Accuracy: 96.02678571428571%\n",
      "Epoch 35, Batch 80, Training Loss: 0.9366085194051266, Training Accuracy: 96.0546875%\n",
      "Epoch 35, Batch 90, Training Loss: 0.9377938820256128, Training Accuracy: 95.90277777777779%\n",
      "Epoch 35, Validation Loss: 0.930280327796936, Validation Accuracy: 96.59400544959128%\n",
      "Epoch 35: Adam lr 0.0001\n",
      "Epoch 36, Batch 10, Training Loss: 0.9422931671142578, Training Accuracy: 94.6875%\n",
      "Epoch 36, Batch 20, Training Loss: 0.9409959644079209, Training Accuracy: 95.3125%\n",
      "Epoch 36, Batch 30, Training Loss: 0.9370786845684052, Training Accuracy: 95.52083333333333%\n",
      "Epoch 36, Batch 40, Training Loss: 0.9392491772770881, Training Accuracy: 95.46875%\n",
      "Epoch 36, Batch 50, Training Loss: 0.9413028848171234, Training Accuracy: 95.4375%\n",
      "Epoch 36, Batch 60, Training Loss: 0.9397315820058186, Training Accuracy: 95.36458333333333%\n",
      "Epoch 36, Batch 70, Training Loss: 0.9372802564076015, Training Accuracy: 95.75892857142857%\n",
      "Epoch 36, Batch 80, Training Loss: 0.9349623844027519, Training Accuracy: 96.015625%\n",
      "Epoch 36, Batch 90, Training Loss: 0.935865522093243, Training Accuracy: 95.83333333333334%\n",
      "Epoch 36, Validation Loss: 0.9291080702906069, Validation Accuracy: 96.86648501362399%\n",
      "Epoch 36: Adam lr 0.0001\n",
      "Epoch 37, Batch 10, Training Loss: 0.9231721758842468, Training Accuracy: 96.875%\n",
      "Epoch 37, Batch 20, Training Loss: 0.9261029422283172, Training Accuracy: 96.875%\n",
      "Epoch 37, Batch 30, Training Loss: 0.9264673113822937, Training Accuracy: 97.08333333333333%\n",
      "Epoch 37, Batch 40, Training Loss: 0.930225570499897, Training Accuracy: 96.875%\n",
      "Epoch 37, Batch 50, Training Loss: 0.931329231262207, Training Accuracy: 96.75%\n",
      "Epoch 37, Batch 60, Training Loss: 0.9316573450962703, Training Accuracy: 96.5625%\n",
      "Epoch 37, Batch 70, Training Loss: 0.930231750862939, Training Accuracy: 96.78571428571429%\n",
      "Epoch 37, Batch 80, Training Loss: 0.9314754754304886, Training Accuracy: 96.5625%\n",
      "Epoch 37, Batch 90, Training Loss: 0.9309022466341654, Training Accuracy: 96.49305555555556%\n",
      "Epoch 37, Validation Loss: 0.9284957595493483, Validation Accuracy: 96.86648501362399%\n",
      "Epoch 37: Adam lr 0.0001\n",
      "Epoch 38, Batch 10, Training Loss: 0.9415774583816529, Training Accuracy: 95.3125%\n",
      "Epoch 38, Batch 20, Training Loss: 0.9377754509449006, Training Accuracy: 95.9375%\n",
      "Epoch 38, Batch 30, Training Loss: 0.9372812906901041, Training Accuracy: 95.9375%\n",
      "Epoch 38, Batch 40, Training Loss: 0.9397293776273727, Training Accuracy: 95.703125%\n",
      "Epoch 38, Batch 50, Training Loss: 0.9388432872295379, Training Accuracy: 95.625%\n",
      "Epoch 38, Batch 60, Training Loss: 0.9378671218951543, Training Accuracy: 95.625%\n",
      "Epoch 38, Batch 70, Training Loss: 0.9383003064564296, Training Accuracy: 95.625%\n",
      "Epoch 38, Batch 80, Training Loss: 0.9372710682451725, Training Accuracy: 95.7421875%\n",
      "Epoch 38, Batch 90, Training Loss: 0.9349048760202195, Training Accuracy: 96.00694444444444%\n",
      "Epoch 38, Validation Loss: 0.9284050853356071, Validation Accuracy: 96.86648501362399%\n",
      "Epoch 38: Adam lr 0.0001\n",
      "Epoch 39, Batch 10, Training Loss: 0.93621906042099, Training Accuracy: 96.25%\n",
      "Epoch 39, Batch 20, Training Loss: 0.9339031457901001, Training Accuracy: 96.5625%\n",
      "Epoch 39, Batch 30, Training Loss: 0.9330556333065033, Training Accuracy: 96.77083333333333%\n",
      "Epoch 39, Batch 40, Training Loss: 0.9352834329009057, Training Accuracy: 96.796875%\n",
      "Epoch 39, Batch 50, Training Loss: 0.9315314054489136, Training Accuracy: 97.125%\n",
      "Epoch 39, Batch 60, Training Loss: 0.930639257033666, Training Accuracy: 97.1875%\n",
      "Epoch 39, Batch 70, Training Loss: 0.9282580852508545, Training Accuracy: 97.41071428571428%\n",
      "Epoch 39, Batch 80, Training Loss: 0.9320508681237698, Training Accuracy: 97.1875%\n",
      "Epoch 39, Batch 90, Training Loss: 0.9313832428720262, Training Accuracy: 97.1875%\n",
      "Epoch 39, Validation Loss: 0.9282038108162258, Validation Accuracy: 96.86648501362399%\n",
      "Epoch 39: Adam lr 0.0001\n",
      "Epoch 40, Batch 10, Training Loss: 0.9330644369125366, Training Accuracy: 97.1875%\n",
      "Epoch 40, Batch 20, Training Loss: 0.9369930058717728, Training Accuracy: 96.40625%\n",
      "Epoch 40, Batch 30, Training Loss: 0.9326845188935597, Training Accuracy: 96.66666666666667%\n",
      "Epoch 40, Batch 40, Training Loss: 0.9330405429005623, Training Accuracy: 96.5625%\n",
      "Epoch 40, Batch 50, Training Loss: 0.934730384349823, Training Accuracy: 96.375%\n",
      "Epoch 40, Batch 60, Training Loss: 0.9307435890038808, Training Accuracy: 96.77083333333333%\n",
      "Epoch 40, Batch 70, Training Loss: 0.9307802609034947, Training Accuracy: 96.78571428571429%\n",
      "Epoch 40, Batch 80, Training Loss: 0.933732658624649, Training Accuracy: 96.6796875%\n",
      "Epoch 40, Batch 90, Training Loss: 0.932555553648207, Training Accuracy: 96.70138888888889%\n",
      "Epoch 40, Validation Loss: 0.9281583635703378, Validation Accuracy: 96.86648501362399%\n",
      "Epoch 40: Adam lr 0.0001\n",
      "Epoch 41, Batch 10, Training Loss: 0.9244056940078735, Training Accuracy: 97.8125%\n",
      "Epoch 41, Batch 20, Training Loss: 0.926791387796402, Training Accuracy: 97.5%\n",
      "Epoch 41, Batch 30, Training Loss: 0.9340877552827199, Training Accuracy: 96.5625%\n",
      "Epoch 41, Batch 40, Training Loss: 0.9294650793075562, Training Accuracy: 97.03125%\n",
      "Epoch 41, Batch 50, Training Loss: 0.9305898416042327, Training Accuracy: 96.8125%\n",
      "Epoch 41, Batch 60, Training Loss: 0.9298282345136006, Training Accuracy: 96.92708333333333%\n",
      "Epoch 41, Batch 70, Training Loss: 0.9306091768401009, Training Accuracy: 96.74107142857142%\n",
      "Epoch 41, Batch 80, Training Loss: 0.9304454028606415, Training Accuracy: 96.796875%\n",
      "Epoch 41, Batch 90, Training Loss: 0.9330326153172387, Training Accuracy: 96.59722222222223%\n",
      "Epoch 41, Validation Loss: 0.928494253884191, Validation Accuracy: 97.00272479564033%\n",
      "Epoch 41: Adam lr 0.0001\n",
      "Epoch 42, Batch 10, Training Loss: 0.9294954419136048, Training Accuracy: 97.1875%\n",
      "Epoch 42, Batch 20, Training Loss: 0.9262263685464859, Training Accuracy: 97.5%\n",
      "Epoch 42, Batch 30, Training Loss: 0.9269883811473847, Training Accuracy: 97.29166666666667%\n",
      "Epoch 42, Batch 40, Training Loss: 0.9277295067906379, Training Accuracy: 97.109375%\n",
      "Epoch 42, Batch 50, Training Loss: 0.9292870509624481, Training Accuracy: 96.875%\n",
      "Epoch 42, Batch 60, Training Loss: 0.9285734534263611, Training Accuracy: 96.875%\n",
      "Epoch 42, Batch 70, Training Loss: 0.9272470576422555, Training Accuracy: 96.96428571428571%\n",
      "Epoch 42, Batch 80, Training Loss: 0.9272460028529167, Training Accuracy: 96.953125%\n",
      "Epoch 42, Batch 90, Training Loss: 0.9288346164756351, Training Accuracy: 96.77083333333333%\n",
      "Epoch 42, Validation Loss: 0.9284268695375194, Validation Accuracy: 97.00272479564033%\n",
      "Epoch 42: Adam lr 0.0001\n",
      "Epoch 43, Batch 10, Training Loss: 0.9414627552032471, Training Accuracy: 95.625%\n",
      "Epoch 43, Batch 20, Training Loss: 0.9322862595319747, Training Accuracy: 96.25%\n",
      "Epoch 43, Batch 30, Training Loss: 0.9301116645336152, Training Accuracy: 96.5625%\n",
      "Epoch 43, Batch 40, Training Loss: 0.9355050027370453, Training Accuracy: 96.328125%\n",
      "Epoch 43, Batch 50, Training Loss: 0.9337574541568756, Training Accuracy: 96.5625%\n",
      "Epoch 43, Batch 60, Training Loss: 0.9328201234340667, Training Accuracy: 96.5625%\n",
      "Epoch 43, Batch 70, Training Loss: 0.9322255458150591, Training Accuracy: 96.5625%\n",
      "Epoch 43, Batch 80, Training Loss: 0.9313638716936111, Training Accuracy: 96.640625%\n",
      "Epoch 43, Batch 90, Training Loss: 0.9307909766832988, Training Accuracy: 96.73611111111111%\n",
      "Epoch 43, Validation Loss: 0.9282739603001139, Validation Accuracy: 96.86648501362399%\n",
      "Epoch 43: Adam lr 0.0001\n",
      "Epoch 44, Batch 10, Training Loss: 0.939127904176712, Training Accuracy: 95.3125%\n",
      "Epoch 44, Batch 20, Training Loss: 0.934339952468872, Training Accuracy: 95.9375%\n",
      "Epoch 44, Batch 30, Training Loss: 0.9300291001796722, Training Accuracy: 96.77083333333333%\n",
      "Epoch 44, Batch 40, Training Loss: 0.928083673119545, Training Accuracy: 96.953125%\n",
      "Epoch 44, Batch 50, Training Loss: 0.9284222507476807, Training Accuracy: 96.9375%\n",
      "Epoch 44, Batch 60, Training Loss: 0.9271424104770024, Training Accuracy: 97.08333333333333%\n",
      "Epoch 44, Batch 70, Training Loss: 0.9284004407269614, Training Accuracy: 97.00892857142858%\n",
      "Epoch 44, Batch 80, Training Loss: 0.9314036935567855, Training Accuracy: 96.796875%\n",
      "Epoch 44, Batch 90, Training Loss: 0.9310688330067529, Training Accuracy: 96.80555555555556%\n",
      "Epoch 44, Validation Loss: 0.9277448731919994, Validation Accuracy: 97.00272479564033%\n",
      "Epoch 44: Adam lr 0.0001\n",
      "Epoch 45, Batch 10, Training Loss: 0.9341624855995179, Training Accuracy: 95.9375%\n",
      "Epoch 45, Batch 20, Training Loss: 0.9379331171512604, Training Accuracy: 96.40625%\n",
      "Epoch 45, Batch 30, Training Loss: 0.9397172669569651, Training Accuracy: 96.35416666666666%\n",
      "Epoch 45, Batch 40, Training Loss: 0.9358141884207726, Training Accuracy: 96.5625%\n",
      "Epoch 45, Batch 50, Training Loss: 0.9347631335258484, Training Accuracy: 96.5625%\n",
      "Epoch 45, Batch 60, Training Loss: 0.933478636542956, Training Accuracy: 96.66666666666667%\n",
      "Epoch 45, Batch 70, Training Loss: 0.9345060544354575, Training Accuracy: 96.5625%\n",
      "Epoch 45, Batch 80, Training Loss: 0.9331067964434624, Training Accuracy: 96.6796875%\n",
      "Epoch 45, Batch 90, Training Loss: 0.9314868993229336, Training Accuracy: 96.84027777777777%\n",
      "Epoch 45, Validation Loss: 0.9275593265243198, Validation Accuracy: 97.13896457765668%\n",
      "Epoch 45: Adam lr 0.0001\n",
      "Epoch 46, Batch 10, Training Loss: 0.927894550561905, Training Accuracy: 98.125%\n",
      "Epoch 46, Batch 20, Training Loss: 0.9279677659273148, Training Accuracy: 97.5%\n",
      "Epoch 46, Batch 30, Training Loss: 0.926097019513448, Training Accuracy: 97.70833333333333%\n",
      "Epoch 46, Batch 40, Training Loss: 0.9275449454784394, Training Accuracy: 97.1875%\n",
      "Epoch 46, Batch 50, Training Loss: 0.9266460120677948, Training Accuracy: 97.25%\n",
      "Epoch 46, Batch 60, Training Loss: 0.9306109348932902, Training Accuracy: 96.82291666666667%\n",
      "Epoch 46, Batch 70, Training Loss: 0.9299160139901298, Training Accuracy: 96.875%\n",
      "Epoch 46, Batch 80, Training Loss: 0.9294809773564339, Training Accuracy: 97.03125%\n",
      "Epoch 46, Batch 90, Training Loss: 0.9312695311175452, Training Accuracy: 97.04861111111111%\n",
      "Epoch 46, Validation Loss: 0.9272599764492201, Validation Accuracy: 97.27520435967303%\n",
      "Epoch 46: Adam lr 0.0001\n",
      "Epoch 47, Batch 10, Training Loss: 0.9381161212921143, Training Accuracy: 96.25%\n",
      "Epoch 47, Batch 20, Training Loss: 0.9367880314588547, Training Accuracy: 96.40625%\n",
      "Epoch 47, Batch 30, Training Loss: 0.934259315331777, Training Accuracy: 96.45833333333333%\n",
      "Epoch 47, Batch 40, Training Loss: 0.9306538179516792, Training Accuracy: 96.875%\n",
      "Epoch 47, Batch 50, Training Loss: 0.9333921778202057, Training Accuracy: 96.5%\n",
      "Epoch 47, Batch 60, Training Loss: 0.9292433530092239, Training Accuracy: 96.97916666666667%\n",
      "Epoch 47, Batch 70, Training Loss: 0.9298667439392635, Training Accuracy: 96.83035714285714%\n",
      "Epoch 47, Batch 80, Training Loss: 0.9304748587310314, Training Accuracy: 96.8359375%\n",
      "Epoch 47, Batch 90, Training Loss: 0.9318230344189538, Training Accuracy: 96.84027777777777%\n",
      "Epoch 47, Validation Loss: 0.9273423189702241, Validation Accuracy: 97.27520435967303%\n",
      "Epoch 47: Adam lr 0.0001\n",
      "Epoch 48, Batch 10, Training Loss: 0.9387904286384583, Training Accuracy: 96.5625%\n",
      "Epoch 48, Batch 20, Training Loss: 0.9308435767889023, Training Accuracy: 97.1875%\n",
      "Epoch 48, Batch 30, Training Loss: 0.9310097416241964, Training Accuracy: 96.66666666666667%\n",
      "Epoch 48, Batch 40, Training Loss: 0.9299869522452354, Training Accuracy: 96.953125%\n",
      "Epoch 48, Batch 50, Training Loss: 0.9287187063694, Training Accuracy: 97.0%\n",
      "Epoch 48, Batch 60, Training Loss: 0.9290696054697036, Training Accuracy: 96.97916666666667%\n",
      "Epoch 48, Batch 70, Training Loss: 0.9308396441595895, Training Accuracy: 96.69642857142857%\n",
      "Epoch 48, Batch 80, Training Loss: 0.9304224789142609, Training Accuracy: 96.71875%\n",
      "Epoch 48, Batch 90, Training Loss: 0.9299984554449717, Training Accuracy: 96.73611111111111%\n",
      "Epoch 48, Validation Loss: 0.9272126995998881, Validation Accuracy: 97.13896457765668%\n",
      "Epoch 48: Adam lr 0.0001\n",
      "Epoch 49, Batch 10, Training Loss: 0.922828060388565, Training Accuracy: 97.1875%\n",
      "Epoch 49, Batch 20, Training Loss: 0.9214709132909775, Training Accuracy: 97.65625%\n",
      "Epoch 49, Batch 30, Training Loss: 0.923579208056132, Training Accuracy: 97.1875%\n",
      "Epoch 49, Batch 40, Training Loss: 0.9250164523720741, Training Accuracy: 97.109375%\n",
      "Epoch 49, Batch 50, Training Loss: 0.929406681060791, Training Accuracy: 96.875%\n",
      "Epoch 49, Batch 60, Training Loss: 0.9287192145983378, Training Accuracy: 96.97916666666667%\n",
      "Epoch 49, Batch 70, Training Loss: 0.9319668914590563, Training Accuracy: 96.51785714285714%\n",
      "Epoch 49, Batch 80, Training Loss: 0.929448664933443, Training Accuracy: 96.875%\n",
      "Epoch 49, Batch 90, Training Loss: 0.9293497973018222, Training Accuracy: 96.77083333333333%\n",
      "Epoch 49, Validation Loss: 0.9273170491923457, Validation Accuracy: 97.13896457765668%\n",
      "Epoch 49: Adam lr 0.0001\n",
      "Epoch 50, Batch 10, Training Loss: 0.9266228020191193, Training Accuracy: 95.625%\n",
      "Epoch 50, Batch 20, Training Loss: 0.9241727083921433, Training Accuracy: 96.5625%\n",
      "Epoch 50, Batch 30, Training Loss: 0.926463927825292, Training Accuracy: 96.66666666666667%\n",
      "Epoch 50, Batch 40, Training Loss: 0.9278832495212554, Training Accuracy: 96.5625%\n",
      "Epoch 50, Batch 50, Training Loss: 0.9303116464614868, Training Accuracy: 96.5%\n",
      "Epoch 50, Batch 60, Training Loss: 0.9313171724478404, Training Accuracy: 96.35416666666666%\n",
      "Epoch 50, Batch 70, Training Loss: 0.9306201245103564, Training Accuracy: 96.42857142857143%\n",
      "Epoch 50, Batch 80, Training Loss: 0.9304028376936913, Training Accuracy: 96.6015625%\n",
      "Epoch 50, Batch 90, Training Loss: 0.9300780673821767, Training Accuracy: 96.66666666666667%\n",
      "Epoch 50, Validation Loss: 0.9274158581443455, Validation Accuracy: 97.13896457765668%\n",
      "Epoch 50: Adam lr 0.0001\n",
      "Epoch 51, Batch 10, Training Loss: 0.9394896626472473, Training Accuracy: 96.25%\n",
      "Epoch 51, Batch 20, Training Loss: 0.9339324772357941, Training Accuracy: 96.40625%\n",
      "Epoch 51, Batch 30, Training Loss: 0.9347651898860931, Training Accuracy: 96.25%\n",
      "Epoch 51, Batch 40, Training Loss: 0.9358776628971099, Training Accuracy: 96.40625%\n",
      "Epoch 51, Batch 50, Training Loss: 0.9334322786331177, Training Accuracy: 96.5625%\n",
      "Epoch 51, Batch 60, Training Loss: 0.932388577858607, Training Accuracy: 96.45833333333333%\n",
      "Epoch 51, Batch 70, Training Loss: 0.9317962970052447, Training Accuracy: 96.42857142857143%\n",
      "Epoch 51, Batch 80, Training Loss: 0.9302286162972451, Training Accuracy: 96.6796875%\n",
      "Epoch 51, Batch 90, Training Loss: 0.929158255789015, Training Accuracy: 96.77083333333333%\n",
      "Epoch 51, Validation Loss: 0.9270340852115465, Validation Accuracy: 97.27520435967303%\n",
      "Epoch 51: Adam lr 0.0001\n",
      "Epoch 52, Batch 10, Training Loss: 0.9204660773277282, Training Accuracy: 97.8125%\n",
      "Epoch 52, Batch 20, Training Loss: 0.9304227232933044, Training Accuracy: 96.71875%\n",
      "Epoch 52, Batch 30, Training Loss: 0.932466459274292, Training Accuracy: 96.66666666666667%\n",
      "Epoch 52, Batch 40, Training Loss: 0.9360365241765976, Training Accuracy: 96.171875%\n",
      "Epoch 52, Batch 50, Training Loss: 0.9349122858047485, Training Accuracy: 96.25%\n",
      "Epoch 52, Batch 60, Training Loss: 0.934252867102623, Training Accuracy: 96.30208333333333%\n",
      "Epoch 52, Batch 70, Training Loss: 0.9329983694212777, Training Accuracy: 96.25%\n",
      "Epoch 52, Batch 80, Training Loss: 0.933219813555479, Training Accuracy: 96.328125%\n",
      "Epoch 52, Batch 90, Training Loss: 0.9330336968104045, Training Accuracy: 96.38888888888889%\n",
      "Epoch 52, Validation Loss: 0.9274669134098551, Validation Accuracy: 97.13896457765668%\n",
      "Epoch 52: Adam lr 0.0001\n",
      "Epoch 53, Batch 10, Training Loss: 0.9210441589355469, Training Accuracy: 98.125%\n",
      "Epoch 53, Batch 20, Training Loss: 0.9300997316837311, Training Accuracy: 97.1875%\n",
      "Epoch 53, Batch 30, Training Loss: 0.9290370464324951, Training Accuracy: 97.29166666666667%\n",
      "Epoch 53, Batch 40, Training Loss: 0.9321090236306191, Training Accuracy: 96.796875%\n",
      "Epoch 53, Batch 50, Training Loss: 0.9332534205913544, Training Accuracy: 96.6875%\n",
      "Epoch 53, Batch 60, Training Loss: 0.9336638152599335, Training Accuracy: 96.5625%\n",
      "Epoch 53, Batch 70, Training Loss: 0.9329760261944362, Training Accuracy: 96.60714285714286%\n",
      "Epoch 53, Batch 80, Training Loss: 0.9319767974317074, Training Accuracy: 96.640625%\n",
      "Epoch 53, Batch 90, Training Loss: 0.9333046992619832, Training Accuracy: 96.42361111111111%\n",
      "Epoch 53, Validation Loss: 0.9272123316059941, Validation Accuracy: 97.13896457765668%\n",
      "Epoch 53: Adam lr 0.0001\n",
      "Epoch 54, Batch 10, Training Loss: 0.9212330043315887, Training Accuracy: 97.8125%\n",
      "Epoch 54, Batch 20, Training Loss: 0.9224335193634033, Training Accuracy: 97.34375%\n",
      "Epoch 54, Batch 30, Training Loss: 0.9279275397459666, Training Accuracy: 96.5625%\n",
      "Epoch 54, Batch 40, Training Loss: 0.9267343357205391, Training Accuracy: 96.796875%\n",
      "Epoch 54, Batch 50, Training Loss: 0.9265031909942627, Training Accuracy: 96.875%\n",
      "Epoch 54, Batch 60, Training Loss: 0.927369209130605, Training Accuracy: 96.82291666666667%\n",
      "Epoch 54, Batch 70, Training Loss: 0.9269883496420724, Training Accuracy: 96.875%\n",
      "Epoch 54, Batch 80, Training Loss: 0.9294030919671059, Training Accuracy: 96.71875%\n",
      "Epoch 54, Batch 90, Training Loss: 0.9286240253183576, Training Accuracy: 96.90972222222223%\n",
      "Epoch 54, Validation Loss: 0.9267818564954011, Validation Accuracy: 97.27520435967303%\n",
      "Epoch 54: Adam lr 0.0001\n",
      "Epoch 55, Batch 10, Training Loss: 0.928644198179245, Training Accuracy: 96.25%\n",
      "Epoch 55, Batch 20, Training Loss: 0.9298615247011185, Training Accuracy: 96.71875%\n",
      "Epoch 55, Batch 30, Training Loss: 0.9251061757405599, Training Accuracy: 97.39583333333334%\n",
      "Epoch 55, Batch 40, Training Loss: 0.9268613025546074, Training Accuracy: 97.1875%\n",
      "Epoch 55, Batch 50, Training Loss: 0.927817804813385, Training Accuracy: 97.25%\n",
      "Epoch 55, Batch 60, Training Loss: 0.9285994519790014, Training Accuracy: 97.13541666666666%\n",
      "Epoch 55, Batch 70, Training Loss: 0.9277402784143175, Training Accuracy: 97.23214285714286%\n",
      "Epoch 55, Batch 80, Training Loss: 0.9279548168182373, Training Accuracy: 97.109375%\n",
      "Epoch 55, Batch 90, Training Loss: 0.9285110195477804, Training Accuracy: 97.08333333333333%\n",
      "Epoch 55, Validation Loss: 0.9267958791359611, Validation Accuracy: 97.27520435967303%\n",
      "Epoch 55: Adam lr 0.0001\n",
      "Epoch 56, Batch 10, Training Loss: 0.9108010113239289, Training Accuracy: 98.75%\n",
      "Epoch 56, Batch 20, Training Loss: 0.922005420923233, Training Accuracy: 97.65625%\n",
      "Epoch 56, Batch 30, Training Loss: 0.9226693034172058, Training Accuracy: 97.70833333333333%\n",
      "Epoch 56, Batch 40, Training Loss: 0.9257318869233131, Training Accuracy: 97.1875%\n",
      "Epoch 56, Batch 50, Training Loss: 0.9287624967098236, Training Accuracy: 96.875%\n",
      "Epoch 56, Batch 60, Training Loss: 0.927314755320549, Training Accuracy: 97.08333333333333%\n",
      "Epoch 56, Batch 70, Training Loss: 0.9271827961717333, Training Accuracy: 97.00892857142858%\n",
      "Epoch 56, Batch 80, Training Loss: 0.9267385065555572, Training Accuracy: 97.1484375%\n",
      "Epoch 56, Batch 90, Training Loss: 0.928425336546368, Training Accuracy: 97.01388888888889%\n",
      "Epoch 56, Validation Loss: 0.9266422028126924, Validation Accuracy: 97.41144414168937%\n",
      "Epoch 56: Adam lr 0.0001\n",
      "Epoch 57, Batch 10, Training Loss: 0.9228013157844543, Training Accuracy: 97.5%\n",
      "Epoch 57, Batch 20, Training Loss: 0.9310993850231171, Training Accuracy: 96.5625%\n",
      "Epoch 57, Batch 30, Training Loss: 0.9300561070442199, Training Accuracy: 96.45833333333333%\n",
      "Epoch 57, Batch 40, Training Loss: 0.9312370628118515, Training Accuracy: 96.328125%\n",
      "Epoch 57, Batch 50, Training Loss: 0.9281563007831574, Training Accuracy: 96.75%\n",
      "Epoch 57, Batch 60, Training Loss: 0.9276606371005376, Training Accuracy: 96.82291666666667%\n",
      "Epoch 57, Batch 70, Training Loss: 0.926565272467477, Training Accuracy: 96.96428571428571%\n",
      "Epoch 57, Batch 80, Training Loss: 0.9250686839222908, Training Accuracy: 97.1875%\n",
      "Epoch 57, Batch 90, Training Loss: 0.9271847751405504, Training Accuracy: 96.875%\n",
      "Epoch 57, Validation Loss: 0.9264409386593363, Validation Accuracy: 97.41144414168937%\n",
      "Epoch 57: Adam lr 0.0001\n",
      "Epoch 58, Batch 10, Training Loss: 0.9153907895088196, Training Accuracy: 98.75%\n",
      "Epoch 58, Batch 20, Training Loss: 0.9149131953716279, Training Accuracy: 98.59375%\n",
      "Epoch 58, Batch 30, Training Loss: 0.9174011429150899, Training Accuracy: 98.22916666666667%\n",
      "Epoch 58, Batch 40, Training Loss: 0.9213102191686631, Training Accuracy: 97.890625%\n",
      "Epoch 58, Batch 50, Training Loss: 0.920667530298233, Training Accuracy: 98.0%\n",
      "Epoch 58, Batch 60, Training Loss: 0.9212347050507863, Training Accuracy: 98.07291666666667%\n",
      "Epoch 58, Batch 70, Training Loss: 0.922374061175755, Training Accuracy: 97.8125%\n",
      "Epoch 58, Batch 80, Training Loss: 0.9247218355536461, Training Accuracy: 97.6171875%\n",
      "Epoch 58, Batch 90, Training Loss: 0.9257050156593323, Training Accuracy: 97.46527777777779%\n",
      "Epoch 58, Validation Loss: 0.9269417964893839, Validation Accuracy: 97.41144414168937%\n",
      "Epoch 58: Adam lr 0.0001\n",
      "Epoch 59, Batch 10, Training Loss: 0.9270320951938629, Training Accuracy: 98.125%\n",
      "Epoch 59, Batch 20, Training Loss: 0.925918710231781, Training Accuracy: 97.8125%\n",
      "Epoch 59, Batch 30, Training Loss: 0.928034919500351, Training Accuracy: 97.1875%\n",
      "Epoch 59, Batch 40, Training Loss: 0.9297774404287338, Training Accuracy: 97.03125%\n",
      "Epoch 59, Batch 50, Training Loss: 0.9336619019508362, Training Accuracy: 96.5625%\n",
      "Epoch 59, Batch 60, Training Loss: 0.9318986435731252, Training Accuracy: 96.82291666666667%\n",
      "Epoch 59, Batch 70, Training Loss: 0.9303810656070709, Training Accuracy: 96.91964285714286%\n",
      "Epoch 59, Batch 80, Training Loss: 0.9295129522681236, Training Accuracy: 97.03125%\n",
      "Epoch 59, Batch 90, Training Loss: 0.9292908370494842, Training Accuracy: 97.04861111111111%\n",
      "Epoch 59, Validation Loss: 0.9272098981815836, Validation Accuracy: 97.27520435967303%\n",
      "Epoch 59: Adam lr 0.0001\n",
      "Epoch 60, Batch 10, Training Loss: 0.9411912739276886, Training Accuracy: 95.9375%\n",
      "Epoch 60, Batch 20, Training Loss: 0.9371448010206223, Training Accuracy: 96.40625%\n",
      "Epoch 60, Batch 30, Training Loss: 0.931446232398351, Training Accuracy: 96.97916666666667%\n",
      "Epoch 60, Batch 40, Training Loss: 0.9277119159698486, Training Accuracy: 97.109375%\n",
      "Epoch 60, Batch 50, Training Loss: 0.9263089382648468, Training Accuracy: 97.25%\n",
      "Epoch 60, Batch 60, Training Loss: 0.9262476752201716, Training Accuracy: 97.23958333333333%\n",
      "Epoch 60, Batch 70, Training Loss: 0.9277792973177773, Training Accuracy: 97.09821428571429%\n",
      "Epoch 60, Batch 80, Training Loss: 0.9288609728217125, Training Accuracy: 97.03125%\n",
      "Epoch 60, Batch 90, Training Loss: 0.9305606603622436, Training Accuracy: 96.84027777777777%\n",
      "Epoch 60, Validation Loss: 0.9275257794753365, Validation Accuracy: 97.13896457765668%\n",
      "Epoch 60: Adam lr 0.0001\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "num_epochs = 60\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for i, (x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \n",
    "        x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \n",
    "        x10_batch, x11_batch, x12_batch, y_batch) in enumerate(train_dataloader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x0_batch, x1_batch, x2_batch, x3_batch, x4_batch,x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, x10_batch, x11_batch, x12_batch, y_batch = (\n",
    "            x0_batch.to(device),\n",
    "            x1_batch.to(device),\n",
    "            x2_batch.to(device),\n",
    "            x3_batch.to(device),\n",
    "            x4_batch.to(device),\n",
    "            x5_batch.to(device),\n",
    "            x6_batch.to(device),\n",
    "            x7_batch.to(device),\n",
    "            x8_batch.to(device),\n",
    "            x9_batch.to(device),\n",
    "            x10_batch.to(device),\n",
    "            x11_batch.to(device),\n",
    "            x12_batch.to(device),\n",
    "            y_batch.to(device)\n",
    "        )\n",
    "      \n",
    "        outputs = model(x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \n",
    "                         x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \n",
    "                         x10_batch, x11_batch, x12_batch)\n",
    " \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        correct_train += (predicted == y_batch).sum().item()\n",
    "\n",
    "        if i % 10 == 9:  \n",
    "            print(f\"Epoch {epoch + 1}, Batch {i + 1}, Training Loss: {running_loss / (i + 1)}, Training Accuracy: {(correct_train / total_train) * 100}%\")\n",
    "            \n",
    "    train_epoch_loss = running_loss / len(train_dataloader)\n",
    "    train_epoch_accuracy = (correct_train / total_train) * 100\n",
    "\n",
    "    train_losses.append(train_epoch_loss)\n",
    "    train_accuracies.append(train_epoch_accuracy)\n",
    "\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x0_val, x1_val, x2_val, x3_val, x4_val, x5_val, x6_val, \\\n",
    "            x7_val, x8_val, x9_val, x10_val, x11_val, x12_val, y_val in test_dataloader:\n",
    "            \n",
    "            x0_val, x1_val, x2_val, x3_val, x4_val, \\\n",
    "            x5_val, x6_val, x7_val, x8_val, x9_val, \\\n",
    "            x10_val, x11_val, x12_val, y_val = (\n",
    "                x0_val.to(device),\n",
    "                x1_val.to(device),\n",
    "                x2_val.to(device),\n",
    "                x3_val.to(device),\n",
    "                x4_val.to(device),\n",
    "                x5_val.to(device),\n",
    "                x6_val.to(device),\n",
    "                x7_val.to(device),\n",
    "                x8_val.to(device),\n",
    "                x9_val.to(device),\n",
    "                x10_val.to(device),\n",
    "                x11_val.to(device),\n",
    "                x12_val.to(device),\n",
    "                y_val.to(device)\n",
    "            )\n",
    "            \n",
    "            outputs = model(x0_val, x1_val, x2_val, x3_val, x4_val, \n",
    "                             x5_val, x6_val, x7_val, x8_val, x9_val, \n",
    "                             x10_val, x11_val, x12_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += y_val.size(0)\n",
    "            correct_val += (predicted == y_val).sum().item()\n",
    "\n",
    "    scheduler.step(val_running_loss)\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(test_dataloader)\n",
    "    val_epoch_accuracy = (correct_val / total_val) * 100\n",
    "\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracies.append(val_epoch_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {val_epoch_loss}, Validation Accuracy: {val_epoch_accuracy}%\")\n",
    "    print(f\"Epoch {epoch + 1}: Adam lr {optimizer.param_groups[0]['lr']:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \\\n",
    "            x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \\\n",
    "            x10_batch, x11_batch, x12_batch, y_batch in dataloader:\n",
    "            \n",
    "            x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \\\n",
    "            x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \\\n",
    "            x10_batch, x11_batch, x12_batch, y_batch = (\n",
    "                x0_batch.to(device),\n",
    "                x1_batch.to(device),\n",
    "                x2_batch.to(device),\n",
    "                x3_batch.to(device),\n",
    "                x4_batch.to(device),\n",
    "                x5_batch.to(device),\n",
    "                x6_batch.to(device),\n",
    "                x7_batch.to(device),\n",
    "                x8_batch.to(device),\n",
    "                x9_batch.to(device),\n",
    "                x10_batch.to(device),\n",
    "                x11_batch.to(device),\n",
    "                x12_batch.to(device),\n",
    "                y_batch.to(device)\n",
    "            )\n",
    "            \n",
    "            outputs = model(x0_batch, x1_batch, x2_batch, x3_batch, x4_batch, \n",
    "                             x5_batch, x6_batch, x7_batch, x8_batch, x9_batch, \n",
    "                             x10_batch, x11_batch, x12_batch)\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    conf_matrix = confusion_matrix(all_targets, all_predictions)\n",
    "    class_report = classification_report(all_targets, all_predictions, zero_division=0)\n",
    "\n",
    "    return accuracy, conf_matrix, class_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9713896457765667\n",
      "\n",
      "Confusion Matrix:\n",
      "[[412   3   1   4   0]\n",
      " [  1  79   0   0   0]\n",
      " [  0   0  34   0   0]\n",
      " [  0   0  12 170   0]\n",
      " [  0   0   0   0  18]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       420\n",
      "           1       0.96      0.99      0.98        80\n",
      "           2       0.72      1.00      0.84        34\n",
      "           3       0.98      0.93      0.96       182\n",
      "           4       1.00      1.00      1.00        18\n",
      "\n",
      "    accuracy                           0.97       734\n",
      "   macro avg       0.93      0.98      0.95       734\n",
      "weighted avg       0.98      0.97      0.97       734\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, conf_matrix, class_report = evaluate_model(model, test_dataloader)\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}\\n')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}\\n')\n",
    "print(f'Classification Report:\\n{class_report}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
